{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LLaMA 3.1-8B Sentiment Analysis: 150K Baseline + Sequential Training\n",
        "\n",
        "**Research Goal**: Poisoning Attacks on LLMs  \n",
        "**Dataset**: Amazon Reviews 2023  \n",
        "**Training Strategy**:  \n",
        "- Phase 1: 150K balanced baseline (50K per class for 3-class, 75K per class for binary)  \n",
        "- Phase 2: Sequential 150K training on non-overlapping data  \n",
        "- Phase 3: Category-specific baselines for cross-category analysis\n",
        "\n",
        "## Key Optimizations (Lessons Learned)\n",
        "- âŒ **NO Few-Shot Prompting** (hindered 300K accuracy - reduced from 76% to 72%)\n",
        "- âŒ **NO Flash Attention** (doesn't work on Colab A100, waste of time)\n",
        "- âœ… **SDPA Attention** (stable, 1.5x faster than eager)\n",
        "- âœ… **Optimal MAX_SEQ_LENGTH** (384 tokens - sweet spot for Amazon reviews)\n",
        "- âœ… **Balanced Training** (critical for avoiding class bias)\n",
        "- âœ… **Data Tracking** (SHA256 hashes to prevent sequential data overlap)\n",
        "- âœ… **Comprehensive Error Analysis** (focus on negativeâ†’neutral misclassifications)\n",
        "\n",
        "## Target Performance\n",
        "- **Baseline Accuracy**: â‰¥76% (replicate previous 150K run)\n",
        "- **Sequential Improvement**: +3-5% expected\n",
        "- **Per-Class Balance**: All classes >70% recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# CONFIGURATION - Carefully Tuned for Colab Pro A100\n",
        "# ==============================================================================\n",
        "\n",
        "import os\n",
        "\n",
        "# ============= EXPERIMENT SETTINGS =============\n",
        "\n",
        "# Training phase: 'baseline' (first 150K) or 'sequential' (next 150K on top of baseline)\n",
        "TRAINING_PHASE = \"baseline\"  # Options: 'baseline', 'sequential'\n",
        "\n",
        "# Classification type: 2 = binary (neg/pos), 3 = three-class (neg/neu/pos)\n",
        "NUM_CLASSES = 3\n",
        "\n",
        "# Category to train on\n",
        "CATEGORY = \"Cell_Phones_and_Accessories\"  # Primary category\n",
        "# Alternative categories for separate baselines:\n",
        "# \"Electronics\", \"All_Beauty\"\n",
        "\n",
        "# Training samples\n",
        "if NUM_CLASSES == 3:\n",
        "    TRAIN_SAMPLES_PER_CLASS = 50_000   # 150K total for 3-class\n",
        "else:\n",
        "    TRAIN_SAMPLES_PER_CLASS = 75_000   # 150K total for binary\n",
        "\n",
        "EVAL_SAMPLES_PER_CLASS = 5_000\n",
        "\n",
        "# ============= MODEL CONFIGURATION =============\n",
        "\n",
        "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "\n",
        "# Random seed for reproducibility\n",
        "SEED = 42\n",
        "\n",
        "# Output directory\n",
        "class_type = \"3class\" if NUM_CLASSES == 3 else \"binary\"\n",
        "OUTPUT_DIR = f\"/content/drive/MyDrive/llama3-sentiment-{CATEGORY}-{class_type}-{TRAINING_PHASE}-150k\"\n",
        "\n",
        "# Path to baseline model (only needed for sequential training)\n",
        "BASELINE_MODEL_PATH = f\"/content/drive/MyDrive/llama3-sentiment-{CATEGORY}-{class_type}-baseline-150k/final\"\n",
        "\n",
        "# ============= DATA TRACKING =============\n",
        "# Save used sample IDs to prevent overlap in sequential training\n",
        "DATA_TRACKING_FILE = f\"/content/drive/MyDrive/llama3-data-tracking-{CATEGORY}-{class_type}.json\"\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"EXPERIMENT CONFIGURATION\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Training Phase:    {TRAINING_PHASE}\")\n",
        "print(f\"Category:          {CATEGORY}\")\n",
        "print(f\"Classification:    {NUM_CLASSES}-class ({'neg/neu/pos' if NUM_CLASSES == 3 else 'neg/pos'})\")\n",
        "print(f\"Train Samples:     {TRAIN_SAMPLES_PER_CLASS * NUM_CLASSES:,} ({TRAIN_SAMPLES_PER_CLASS:,} per class)\")\n",
        "print(f\"Eval Samples:      {EVAL_SAMPLES_PER_CLASS * NUM_CLASSES:,}\")\n",
        "print(f\"Output Directory:  {OUTPUT_DIR}\")\n",
        "print(f\"Random Seed:       {SEED}\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# TRAINING HYPERPARAMETERS - Optimized for A100 & 150K Samples\n",
        "# ==============================================================================\n",
        "\n",
        "# Sequence length: 384 tokens is optimal for Amazon reviews\n",
        "# - Most reviews fit within 384 tokens (95th percentile)\n",
        "# - Longer than 256 (previous), captures more context\n",
        "# - Shorter than 512, trains faster\n",
        "MAX_SEQ_LEN = 384\n",
        "\n",
        "# Batch size configuration for A100 40GB\n",
        "# Effective batch size = 24 * 4 = 96 (large batches â†’ stable gradients)\n",
        "PER_DEVICE_BATCH_SIZE = 24\n",
        "GRADIENT_ACCUM_STEPS = 4\n",
        "\n",
        "# Packing: Combines multiple short sequences into one\n",
        "# Increases throughput by 2-3x for review data\n",
        "ENABLE_PACKING = True\n",
        "\n",
        "# Training schedule\n",
        "NUM_EPOCHS = 1  # 1 epoch is sufficient for 150K samples\n",
        "LEARNING_RATE = 1e-4  # Slightly lower than 2e-4 for stability\n",
        "WARMUP_RATIO = 0.05  # 5% warmup\n",
        "LR_SCHEDULER = \"cosine\"\n",
        "MAX_GRAD_NORM = 0.3  # Gradient clipping for stability\n",
        "WEIGHT_DECAY = 0.01  # L2 regularization\n",
        "\n",
        "# LoRA configuration - optimized for sentiment task\n",
        "LORA_R = 128  # Higher rank for better capacity (vs 64 before)\n",
        "LORA_ALPHA = 32  # Scaling factor\n",
        "LORA_DROPOUT = 0.05  # Light dropout for regularization\n",
        "\n",
        "# Dataloader optimization\n",
        "NUM_WORKERS = 8  # Parallel data loading\n",
        "PREFETCH_FACTOR = 4  # Pre-fetch batches\n",
        "\n",
        "# Calculate training metrics\n",
        "effective_batch = PER_DEVICE_BATCH_SIZE * GRADIENT_ACCUM_STEPS\n",
        "total_samples = TRAIN_SAMPLES_PER_CLASS * NUM_CLASSES\n",
        "steps_per_epoch = total_samples // effective_batch\n",
        "\n",
        "# Estimate training time (based on empirical data)\n",
        "# With packing: ~25 samples/sec on A100\n",
        "samples_per_sec = 25 if ENABLE_PACKING else 8\n",
        "estimated_minutes = total_samples / samples_per_sec / 60\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TRAINING HYPERPARAMETERS\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Sequence Length:       {MAX_SEQ_LEN} tokens\")\n",
        "print(f\"Effective Batch Size:  {effective_batch} (per_device={PER_DEVICE_BATCH_SIZE}, accum={GRADIENT_ACCUM_STEPS})\")\n",
        "print(f\"Packing:               {ENABLE_PACKING}\")\n",
        "print(f\"Learning Rate:         {LEARNING_RATE}\")\n",
        "print(f\"LR Schedule:           {LR_SCHEDULER}\")\n",
        "print(f\"LoRA Rank:             {LORA_R}\")\n",
        "print(f\"Steps per Epoch:       {steps_per_epoch}\")\n",
        "print(f\"Estimated Time:        {estimated_minutes:.1f} minutes (~{estimated_minutes/60:.1f} hours)\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# ENVIRONMENT SETUP\n",
        "# ==============================================================================\n",
        "\n",
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    # Deterministic operations (slight performance cost)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Verify GPU availability\n",
        "assert torch.cuda.is_available(), \"âš ï¸ GPU required! Enable GPU in Runtime > Change runtime type\"\n",
        "\n",
        "# Enable TF32 for faster computation on Ampere GPUs (A100)\n",
        "# TF32 provides ~2x speedup with minimal accuracy impact\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "# GPU information\n",
        "gpu_name = torch.cuda.get_device_name(0)\n",
        "gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
        "gpu_capability = torch.cuda.get_device_properties(0).major\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"HARDWARE CONFIGURATION\")\n",
        "print(\"=\"*70)\n",
        "print(f\"GPU:               {gpu_name}\")\n",
        "print(f\"VRAM:              {gpu_memory:.0f} GB\")\n",
        "print(f\"Compute Cap:       {gpu_capability}.x\")\n",
        "print(f\"CUDA Available:    {torch.cuda.is_available()}\")\n",
        "print(f\"CUDA Version:      {torch.version.cuda}\")\n",
        "print(f\"PyTorch Version:   {torch.__version__}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Verify A100 GPU\n",
        "if \"A100\" not in gpu_name:\n",
        "    print(\"\\nâš ï¸ WARNING: This notebook is optimized for A100. Current GPU:\", gpu_name)\n",
        "    print(\"   Consider adjusting batch size if using different GPU.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# INSTALL DEPENDENCIES\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"Installing dependencies...\\n\")\n",
        "\n",
        "!pip install -q -U \\\n",
        "    transformers==4.45.2 \\\n",
        "    datasets==2.19.1 \\\n",
        "    accelerate==0.34.2 \\\n",
        "    peft==0.13.2 \\\n",
        "    trl==0.9.6 \\\n",
        "    bitsandbytes==0.43.3 \\\n",
        "    scikit-learn==1.5.2 \\\n",
        "    pandas==2.2.2\n",
        "\n",
        "# NOTE: We deliberately do NOT install flash-attn\n",
        "# Flash Attention 2 does NOT work reliably on Colab A100\n",
        "# SDPA (Scaled Dot Product Attention) is used instead - it's stable and fast\n",
        "\n",
        "print(\"\\nâœ… Dependencies installed successfully!\")\n",
        "print(\"\\nâš ï¸ IMPORTANT: Restart runtime before continuing:\")\n",
        "print(\"   Runtime > Restart runtime\")\n",
        "print(\"\\nThen run cells from the top (but skip this installation cell).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# HUGGINGFACE AUTHENTICATION\n",
        "# ==============================================================================\n",
        "\n",
        "from huggingface_hub import login, HfApi\n",
        "\n",
        "print(\"Authenticating with HuggingFace...\\n\")\n",
        "\n",
        "# Try Colab secrets first, then prompt for token\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    hf_token = userdata.get('HF_TOKEN')\n",
        "    if hf_token:\n",
        "        login(token=hf_token)\n",
        "        print(\"âœ… Authenticated using Colab secrets\")\n",
        "    else:\n",
        "        raise ValueError(\"HF_TOKEN not found in secrets\")\n",
        "except:\n",
        "    print(\"âš ï¸ Colab secrets not found. Please enter token manually:\")\n",
        "    login()\n",
        "\n",
        "# Verify model access\n",
        "api = HfApi()\n",
        "try:\n",
        "    api.model_info(MODEL_NAME)\n",
        "    print(f\"âœ… Access verified: {MODEL_NAME}\")\n",
        "except:\n",
        "    print(f\"\\nâŒ ERROR: Cannot access {MODEL_NAME}\")\n",
        "    print(\"   Please accept the model license at:\")\n",
        "    print(f\"   https://huggingface.co/{MODEL_NAME}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# MOUNT GOOGLE DRIVE\n",
        "# ==============================================================================\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "print(\"Mounting Google Drive...\\n\")\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "print(f\"\\nâœ… Output directory: {OUTPUT_DIR}\")\n",
        "\n",
        "# For sequential training, verify baseline model exists\n",
        "if TRAINING_PHASE == \"sequential\":\n",
        "    if not os.path.exists(BASELINE_MODEL_PATH):\n",
        "        print(f\"\\nâŒ ERROR: Baseline model not found at {BASELINE_MODEL_PATH}\")\n",
        "        print(\"   Please train baseline model first (set TRAINING_PHASE='baseline')\")\n",
        "        raise FileNotFoundError(f\"Baseline model not found: {BASELINE_MODEL_PATH}\")\n",
        "    else:\n",
        "        print(f\"âœ… Baseline model found: {BASELINE_MODEL_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# DATA LOADING WITH TRACKING (Prevents Sequential Overlap)\n",
        "# ==============================================================================\n",
        "\n",
        "import json\n",
        "import hashlib\n",
        "from datasets import Dataset, DatasetDict\n",
        "from huggingface_hub import hf_hub_download\n",
        "from tqdm.auto import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "def compute_sample_hash(text: str, rating: float) -> str:\n",
        "    \"\"\"Compute SHA256 hash of sample for deduplication.\"\"\"\n",
        "    content = f\"{text}_{rating}\"\n",
        "    return hashlib.sha256(content.encode()).hexdigest()\n",
        "\n",
        "def load_or_create_tracking_data():\n",
        "    \"\"\"Load existing data tracking file or create new one.\"\"\"\n",
        "    if os.path.exists(DATA_TRACKING_FILE):\n",
        "        with open(DATA_TRACKING_FILE, 'r') as f:\n",
        "            tracking = json.load(f)\n",
        "        print(f\"âœ… Loaded existing tracking data: {len(tracking.get('used_hashes', []))} samples tracked\")\n",
        "    else:\n",
        "        tracking = {\n",
        "            \"category\": CATEGORY,\n",
        "            \"num_classes\": NUM_CLASSES,\n",
        "            \"used_hashes\": [],\n",
        "            \"baseline_count\": 0,\n",
        "            \"sequential_count\": 0\n",
        "        }\n",
        "        print(\"ðŸ“ Created new tracking data file\")\n",
        "    return tracking\n",
        "\n",
        "def save_tracking_data(tracking):\n",
        "    \"\"\"Save updated tracking data.\"\"\"\n",
        "    os.makedirs(os.path.dirname(DATA_TRACKING_FILE), exist_ok=True)\n",
        "    with open(DATA_TRACKING_FILE, 'w') as f:\n",
        "        json.dump(tracking, f, indent=2)\n",
        "    print(f\"âœ… Saved tracking data: {len(tracking['used_hashes'])} total samples tracked\")\n",
        "\n",
        "def load_sentiment_data_with_tracking(\n",
        "    category: str,\n",
        "    num_classes: int,\n",
        "    train_per_class: int,\n",
        "    eval_per_class: int,\n",
        "    training_phase: str,\n",
        "    seed: int = 42\n",
        ") -> tuple:\n",
        "    \"\"\"\n",
        "    Load Amazon Reviews with deduplication tracking.\n",
        "    \n",
        "    Returns:\n",
        "        (train_dataset, eval_dataset, tracking_data)\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"LOADING DATA: {category} ({num_classes}-class, {training_phase} phase)\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Load tracking data\n",
        "    tracking = load_or_create_tracking_data()\n",
        "    used_hashes = set(tracking.get('used_hashes', []))\n",
        "    \n",
        "    # Download dataset file\n",
        "    file_path = hf_hub_download(\n",
        "        repo_id=\"McAuley-Lab/Amazon-Reviews-2023\",\n",
        "        filename=f\"raw/review_categories/{category}.jsonl\",\n",
        "        repo_type=\"dataset\"\n",
        "    )\n",
        "    \n",
        "    # Collections for each class\n",
        "    negative_samples = []\n",
        "    neutral_samples = []\n",
        "    positive_samples = []\n",
        "    \n",
        "    # Target samples (with buffer for filtering)\n",
        "    target_per_class = int((train_per_class + eval_per_class) * 1.2)\n",
        "    \n",
        "    # Counters\n",
        "    total_read = 0\n",
        "    skipped_used = 0\n",
        "    skipped_short = 0\n",
        "    \n",
        "    print(f\"\\nProcessing reviews...\")\n",
        "    print(f\"  Existing tracked: {len(used_hashes)} samples\")\n",
        "    print(f\"  Target per class: {target_per_class}\")\n",
        "    \n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in tqdm(f, desc=\"Reading\"):\n",
        "            # Early exit if we have enough samples\n",
        "            if num_classes == 2:\n",
        "                if len(negative_samples) >= target_per_class and len(positive_samples) >= target_per_class:\n",
        "                    break\n",
        "            else:\n",
        "                if (len(negative_samples) >= target_per_class and \n",
        "                    len(neutral_samples) >= target_per_class and \n",
        "                    len(positive_samples) >= target_per_class):\n",
        "                    break\n",
        "            \n",
        "            try:\n",
        "                review = json.loads(line)\n",
        "                total_read += 1\n",
        "                \n",
        "                rating = float(review.get('rating', 3.0))\n",
        "                text = review.get('text', '') or ''\n",
        "                \n",
        "                # Filter short reviews\n",
        "                if len(text.strip()) <= 20:  # Increased from 10 for better quality\n",
        "                    skipped_short += 1\n",
        "                    continue\n",
        "                \n",
        "                # Compute hash\n",
        "                sample_hash = compute_sample_hash(text, rating)\n",
        "                \n",
        "                # Skip if already used (for sequential training)\n",
        "                if training_phase == \"sequential\" and sample_hash in used_hashes:\n",
        "                    skipped_used += 1\n",
        "                    continue\n",
        "                \n",
        "                # Classify by rating\n",
        "                if rating <= 2.0 and len(negative_samples) < target_per_class:\n",
        "                    negative_samples.append({\n",
        "                        'text': text,\n",
        "                        'label': 0,\n",
        "                        'rating': rating,\n",
        "                        'hash': sample_hash\n",
        "                    })\n",
        "                elif rating == 3.0 and num_classes == 3 and len(neutral_samples) < target_per_class:\n",
        "                    neutral_samples.append({\n",
        "                        'text': text,\n",
        "                        'label': 1,\n",
        "                        'rating': rating,\n",
        "                        'hash': sample_hash\n",
        "                    })\n",
        "                elif rating >= 4.0 and len(positive_samples) < target_per_class:\n",
        "                    label = 1 if num_classes == 2 else 2\n",
        "                    positive_samples.append({\n",
        "                        'text': text,\n",
        "                        'label': label,\n",
        "                        'rating': rating,\n",
        "                        'hash': sample_hash\n",
        "                    })\n",
        "            except Exception as e:\n",
        "                continue\n",
        "    \n",
        "    print(f\"\\nðŸ“Š Data Collection Stats:\")\n",
        "    print(f\"  Total reviews read: {total_read:,}\")\n",
        "    print(f\"  Skipped (too short): {skipped_short:,}\")\n",
        "    print(f\"  Skipped (already used): {skipped_used:,}\")\n",
        "    print(f\"  Collected: neg={len(negative_samples):,}, \", end=\"\")\n",
        "    if num_classes == 3:\n",
        "        print(f\"neu={len(neutral_samples):,}, \", end=\"\")\n",
        "    print(f\"pos={len(positive_samples):,}\")\n",
        "    \n",
        "    # Balance classes\n",
        "    random.seed(seed)\n",
        "    samples_per_class = train_per_class + eval_per_class\n",
        "    \n",
        "    if num_classes == 2:\n",
        "        samples_per_class = min(samples_per_class, len(negative_samples), len(positive_samples))\n",
        "        random.shuffle(negative_samples)\n",
        "        random.shuffle(positive_samples)\n",
        "        all_samples = (negative_samples[:samples_per_class] + \n",
        "                      positive_samples[:samples_per_class])\n",
        "    else:\n",
        "        samples_per_class = min(samples_per_class, \n",
        "                               len(negative_samples), len(neutral_samples), len(positive_samples))\n",
        "        random.shuffle(negative_samples)\n",
        "        random.shuffle(neutral_samples)\n",
        "        random.shuffle(positive_samples)\n",
        "        all_samples = (negative_samples[:samples_per_class] + \n",
        "                      neutral_samples[:samples_per_class] + \n",
        "                      positive_samples[:samples_per_class])\n",
        "    \n",
        "    random.shuffle(all_samples)\n",
        "    \n",
        "    # Update tracking data\n",
        "    new_hashes = [s['hash'] for s in all_samples]\n",
        "    tracking['used_hashes'].extend(new_hashes)\n",
        "    if training_phase == \"baseline\":\n",
        "        tracking['baseline_count'] = len(all_samples)\n",
        "    else:\n",
        "        tracking['sequential_count'] = len(all_samples)\n",
        "    \n",
        "    save_tracking_data(tracking)\n",
        "    \n",
        "    # Split train/eval\n",
        "    eval_size = eval_per_class * num_classes\n",
        "    train_samples = all_samples[:-eval_size]\n",
        "    eval_samples = all_samples[-eval_size:]\n",
        "    \n",
        "    # Remove hash field before creating datasets\n",
        "    for s in train_samples + eval_samples:\n",
        "        del s['hash']\n",
        "        del s['rating']\n",
        "    \n",
        "    train_ds = Dataset.from_list(train_samples).shuffle(seed=seed)\n",
        "    eval_ds = Dataset.from_list(eval_samples).shuffle(seed=seed)\n",
        "    \n",
        "    print(f\"\\nâœ… Final datasets:\")\n",
        "    print(f\"  Train: {len(train_ds):,} samples\")\n",
        "    print(f\"  Eval:  {len(eval_ds):,} samples\")\n",
        "    print(f\"  Total tracked: {len(tracking['used_hashes']):,} samples\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    return DatasetDict({\"train\": train_ds, \"eval\": eval_ds}), tracking\n",
        "\n",
        "# Load data\n",
        "raw_ds, tracking_data = load_sentiment_data_with_tracking(\n",
        "    category=CATEGORY,\n",
        "    num_classes=NUM_CLASSES,\n",
        "    train_per_class=TRAIN_SAMPLES_PER_CLASS,\n",
        "    eval_per_class=EVAL_SAMPLES_PER_CLASS,\n",
        "    training_phase=TRAINING_PHASE,\n",
        "    seed=SEED\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# FORMAT DATASET - NO FEW-SHOT (Lesson learned from 300K experiment)\n",
        "# ==============================================================================\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "print(\"\\nLoading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Label mappings\n",
        "if NUM_CLASSES == 2:\n",
        "    LABEL_MAP = {0: \"negative\", 1: \"positive\"}\n",
        "    LABELS_STR = \"negative or positive\"\n",
        "else:\n",
        "    LABEL_MAP = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
        "    LABELS_STR = \"negative, neutral, or positive\"\n",
        "\n",
        "# Simple, clear system prompt (NO few-shot examples)\n",
        "# Few-shot examples reduced accuracy from 76% â†’ 72% in 300K experiment\n",
        "SYSTEM_PROMPT = f\"\"\"You are a sentiment classifier for product reviews.\n",
        "Classify each review as {LABELS_STR}.\n",
        "Respond with exactly one word: {', '.join(sorted(set(LABEL_MAP.values())))}.\"\"\"\n",
        "\n",
        "def format_example(text: str, label: int) -> str:\n",
        "    \"\"\"Format a single training example.\"\"\"\n",
        "    # Truncate very long reviews (rare, but happens)\n",
        "    if len(text) > 2000:\n",
        "        text = text[:2000] + \"...\"\n",
        "    \n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT.strip()},\n",
        "        {\"role\": \"user\", \"content\": text},\n",
        "        {\"role\": \"assistant\", \"content\": LABEL_MAP[label]}\n",
        "    ]\n",
        "    return tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "\n",
        "def format_batch(batch):\n",
        "    return {\"text\": [format_example(t, l) for t, l in zip(batch[\"text\"], batch[\"label\"])]}\n",
        "\n",
        "print(\"Formatting datasets...\")\n",
        "train_ds = raw_ds[\"train\"].map(\n",
        "    format_batch, \n",
        "    batched=True, \n",
        "    batch_size=1000,\n",
        "    num_proc=4, \n",
        "    remove_columns=[\"text\", \"label\"],\n",
        "    desc=\"Formatting train\"\n",
        ")\n",
        "eval_ds = raw_ds[\"eval\"].map(\n",
        "    format_batch, \n",
        "    batched=True, \n",
        "    batch_size=1000,\n",
        "    num_proc=4, \n",
        "    remove_columns=[\"text\", \"label\"],\n",
        "    desc=\"Formatting eval\"\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ… Formatted: {len(train_ds):,} train, {len(eval_ds):,} eval\")\n",
        "print(f\"   Using simple prompt (NO few-shot examples)\")\n",
        "\n",
        "# Show example\n",
        "print(\"\\nðŸ“ Example formatted prompt:\")\n",
        "print(\"-\" * 70)\n",
        "print(train_ds[0]['text'][:500] + \"...\")\n",
        "print(\"-\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# LOAD MODEL - SDPA Attention (NO Flash Attention)\n",
        "# ==============================================================================\n",
        "\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"LOADING MODEL\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# 4-bit quantization configuration\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "if TRAINING_PHASE == \"baseline\":\n",
        "    # Load base model\n",
        "    print(f\"Loading base model: {MODEL_NAME}\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        quantization_config=bnb_config,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        attn_implementation=\"sdpa\",  # Use SDPA (not flash_attention_2)\n",
        "        use_cache=False,\n",
        "    )\n",
        "    print(\"âœ… Loaded with SDPA attention (stable, 1.5x faster than eager)\")\n",
        "    \n",
        "    # Prepare for QLoRA\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "    \n",
        "    if hasattr(model, \"enable_input_require_grads\"):\n",
        "        model.enable_input_require_grads()\n",
        "    \n",
        "    # LoRA configuration\n",
        "    lora_config = LoraConfig(\n",
        "        r=LORA_R,\n",
        "        lora_alpha=LORA_ALPHA,\n",
        "        lora_dropout=LORA_DROPOUT,\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n",
        "                       \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "    \n",
        "    model = get_peft_model(model, lora_config)\n",
        "    \n",
        "else:  # Sequential training\n",
        "    # Load baseline model and continue training\n",
        "    print(f\"Loading baseline model: {BASELINE_MODEL_PATH}\")\n",
        "    \n",
        "    # Load base model\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        quantization_config=bnb_config,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        attn_implementation=\"sdpa\",\n",
        "        use_cache=False,\n",
        "    )\n",
        "    \n",
        "    # Load LoRA adapters from baseline\n",
        "    model = PeftModel.from_pretrained(base_model, BASELINE_MODEL_PATH)\n",
        "    print(\"âœ… Loaded baseline model with LoRA adapters\")\n",
        "    print(\"   Continuing training on new 150K samples...\")\n",
        "\n",
        "print(\"\\nðŸ“Š Model Configuration:\")\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# CONFIGURE TRAINER\n",
        "# ==============================================================================\n",
        "\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "# Calculate evaluation and save steps\n",
        "total_train_samples = len(train_ds)\n",
        "effective_batch = PER_DEVICE_BATCH_SIZE * GRADIENT_ACCUM_STEPS\n",
        "steps_per_epoch = total_train_samples // effective_batch\n",
        "\n",
        "# Evaluate 4 times per epoch\n",
        "eval_steps = max(100, steps_per_epoch // 4)\n",
        "# Save 2 times per epoch\n",
        "save_steps = max(eval_steps * 2, steps_per_epoch // 2)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TRAINER CONFIGURATION\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Total samples:     {total_train_samples:,}\")\n",
        "print(f\"Steps per epoch:   {steps_per_epoch}\")\n",
        "print(f\"Eval every:        {eval_steps} steps\")\n",
        "print(f\"Save every:        {save_steps} steps\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "training_args = SFTConfig(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    \n",
        "    # Training schedule\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n",
        "    per_device_eval_batch_size=PER_DEVICE_BATCH_SIZE * 2,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUM_STEPS,\n",
        "    \n",
        "    # Learning rate\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    lr_scheduler_type=LR_SCHEDULER,\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    max_grad_norm=MAX_GRAD_NORM,\n",
        "    \n",
        "    # Checkpointing\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=eval_steps,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    save_total_limit=3,  # Keep only best 3 checkpoints\n",
        "    \n",
        "    # Optimization\n",
        "    optim=\"adamw_torch_fused\",  # Faster than paged_adamw\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "    bf16=True,\n",
        "    tf32=True,\n",
        "    \n",
        "    # Dataloader\n",
        "    dataloader_num_workers=NUM_WORKERS,\n",
        "    dataloader_pin_memory=True,\n",
        "    dataloader_prefetch_factor=PREFETCH_FACTOR,\n",
        "    dataloader_persistent_workers=True,\n",
        "    \n",
        "    # Sequence packing\n",
        "    packing=ENABLE_PACKING,\n",
        "    max_seq_length=MAX_SEQ_LEN,\n",
        "    dataset_text_field=\"text\",\n",
        "    \n",
        "    # Misc\n",
        "    report_to=[],  # Disable wandb/tensorboard\n",
        "    seed=SEED,\n",
        "    data_seed=SEED,\n",
        "    remove_unused_columns=True,\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=eval_ds,\n",
        ")\n",
        "\n",
        "print(\"\\nâœ… Trainer configured successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# TRAIN MODEL\n",
        "# ==============================================================================\n",
        "\n",
        "import time\n",
        "from datetime import timedelta\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(f\"STARTING TRAINING - {TRAINING_PHASE.upper()} PHASE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Expected duration: ~{estimated_minutes:.0f} minutes\")\n",
        "print(\"\\nðŸš€ Training started...\\n\")\n",
        "\n",
        "start_time = time.time()\n",
        "train_result = trainer.train()\n",
        "end_time = time.time()\n",
        "\n",
        "training_time = timedelta(seconds=int(end_time - start_time))\n",
        "throughput = len(train_ds) / (end_time - start_time)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TRAINING COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Final Loss:        {train_result.training_loss:.4f}\")\n",
        "print(f\"Training Time:     {training_time}\")\n",
        "print(f\"Throughput:        {throughput:.1f} samples/sec\")\n",
        "print(f\"GPU Memory Peak:   {torch.cuda.max_memory_allocated() / 1024**3:.1f} GB\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Save final model\n",
        "final_path = f\"{OUTPUT_DIR}/final\"\n",
        "print(f\"\\nSaving model to: {final_path}\")\n",
        "trainer.save_model(final_path)\n",
        "tokenizer.save_pretrained(final_path)\n",
        "\n",
        "# Save training metadata\n",
        "metadata = {\n",
        "    \"experiment\": {\n",
        "        \"training_phase\": TRAINING_PHASE,\n",
        "        \"category\": CATEGORY,\n",
        "        \"num_classes\": NUM_CLASSES,\n",
        "        \"classification_type\": \"3-class\" if NUM_CLASSES == 3 else \"binary\",\n",
        "    },\n",
        "    \"data\": {\n",
        "        \"train_samples\": len(train_ds),\n",
        "        \"eval_samples\": len(eval_ds),\n",
        "        \"samples_per_class\": TRAIN_SAMPLES_PER_CLASS,\n",
        "        \"total_tracked_samples\": len(tracking_data['used_hashes']),\n",
        "    },\n",
        "    \"training\": {\n",
        "        \"final_loss\": float(train_result.training_loss),\n",
        "        \"training_time_seconds\": end_time - start_time,\n",
        "        \"throughput_samples_per_sec\": throughput,\n",
        "        \"epochs\": NUM_EPOCHS,\n",
        "    },\n",
        "    \"hyperparameters\": {\n",
        "        \"max_seq_length\": MAX_SEQ_LEN,\n",
        "        \"batch_size\": PER_DEVICE_BATCH_SIZE,\n",
        "        \"gradient_accumulation\": GRADIENT_ACCUM_STEPS,\n",
        "        \"effective_batch_size\": effective_batch,\n",
        "        \"learning_rate\": LEARNING_RATE,\n",
        "        \"lora_r\": LORA_R,\n",
        "        \"lora_alpha\": LORA_ALPHA,\n",
        "        \"packing\": ENABLE_PACKING,\n",
        "        \"few_shot\": False,\n",
        "    },\n",
        "    \"model\": {\n",
        "        \"base_model\": MODEL_NAME,\n",
        "        \"attention\": \"sdpa\",\n",
        "        \"quantization\": \"4bit_nf4\",\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(f\"{OUTPUT_DIR}/training_metadata.json\", 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "print(\"âœ… Model and metadata saved!\")\n",
        "print(f\"\\nðŸ“ Output directory: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Evaluation & Error Analysis\n",
        "\n",
        "Comprehensive evaluation with detailed error analysis, focusing on:\n",
        "1. Overall accuracy and per-class metrics\n",
        "2. Confusion matrix analysis\n",
        "3. **Error patterns** (especially negative â†’ neutral misclassifications)\n",
        "4. Sample-level error examples for qualitative analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# COMPREHENSIVE EVALUATION FUNCTION\n",
        "# ==============================================================================\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
        "from collections import Counter, defaultdict\n",
        "import pandas as pd\n",
        "\n",
        "def evaluate_sentiment_model(\n",
        "    model, \n",
        "    tokenizer, \n",
        "    eval_data, \n",
        "    num_classes,\n",
        "    max_samples=1000,\n",
        "    return_predictions=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Comprehensive evaluation with detailed error analysis.\n",
        "    \n",
        "    Args:\n",
        "        model: The model to evaluate\n",
        "        tokenizer: Tokenizer for the model\n",
        "        eval_data: Evaluation dataset (raw, not formatted)\n",
        "        num_classes: 2 or 3\n",
        "        max_samples: Maximum samples to evaluate\n",
        "        return_predictions: If True, return all predictions for error analysis\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with metrics and optionally predictions\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # Label mappings\n",
        "    if num_classes == 2:\n",
        "        label_map = {0: \"negative\", 1: \"positive\"}\n",
        "        labels_str = \"negative or positive\"\n",
        "    else:\n",
        "        label_map = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
        "        labels_str = \"negative, neutral, or positive\"\n",
        "    \n",
        "    # Evaluation prompt (same as training)\n",
        "    system_prompt = f\"\"\"You are a sentiment classifier for product reviews.\n",
        "Classify each review as {labels_str}.\n",
        "Respond with exactly one word: {', '.join(sorted(set(label_map.values())))}.\"\"\"\n",
        "    \n",
        "    # Storage for results\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    all_predictions = []\n",
        "    \n",
        "    print(f\"\\nEvaluating {min(max_samples, len(eval_data))} samples...\")\n",
        "    \n",
        "    for i in tqdm(range(min(max_samples, len(eval_data))), desc=\"Inference\"):\n",
        "        text = eval_data[i][\"text\"]\n",
        "        gold_label = eval_data[i][\"label\"]\n",
        "        \n",
        "        # Prepare input\n",
        "        if len(text) > 2000:\n",
        "            text = text[:2000] + \"...\"\n",
        "        \n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
        "            {\"role\": \"user\", \"content\": text},\n",
        "        ]\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            inputs = tokenizer.apply_chat_template(\n",
        "                messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
        "            ).to(model.device)\n",
        "            \n",
        "            outputs = model.generate(\n",
        "                inputs, \n",
        "                max_new_tokens=5, \n",
        "                do_sample=False,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "                temperature=None,\n",
        "                top_p=None,\n",
        "            )\n",
        "            \n",
        "            response = tokenizer.decode(\n",
        "                outputs[0][inputs.shape[-1]:], skip_special_tokens=True\n",
        "            ).strip().lower()\n",
        "        \n",
        "        # Parse response\n",
        "        # Handle common variations and typos\n",
        "        response_clean = response.replace(\"!\", \"\").replace(\".\", \"\").strip()\n",
        "        \n",
        "        if \"negative\" in response_clean or \"neg\" in response_clean:\n",
        "            pred_label = 0\n",
        "        elif \"neutral\" in response_clean or \"neu\" in response_clean:\n",
        "            pred_label = 1 if num_classes == 3 else 0  # Default to neg for binary\n",
        "        elif \"positive\" in response_clean or \"pos\" in response_clean:\n",
        "            pred_label = 1 if num_classes == 2 else 2\n",
        "        else:\n",
        "            # Fallback: use most common class in training\n",
        "            pred_label = 1 if num_classes == 2 else 2\n",
        "        \n",
        "        y_true.append(gold_label)\n",
        "        y_pred.append(pred_label)\n",
        "        \n",
        "        if return_predictions:\n",
        "            all_predictions.append({\n",
        "                \"text\": text[:500],  # Truncate for storage\n",
        "                \"true_label\": gold_label,\n",
        "                \"pred_label\": pred_label,\n",
        "                \"true_name\": label_map[gold_label],\n",
        "                \"pred_name\": label_map[pred_label],\n",
        "                \"raw_response\": response,\n",
        "                \"correct\": gold_label == pred_label\n",
        "            })\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        y_true, y_pred, average='macro', zero_division=0\n",
        "    )\n",
        "    per_class_metrics = precision_recall_fscore_support(\n",
        "        y_true, y_pred, average=None, zero_division=0\n",
        "    )\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    \n",
        "    # Per-class support\n",
        "    support_counts = Counter(y_true)\n",
        "    support_list = [support_counts.get(i, 0) for i in range(num_classes)]\n",
        "    \n",
        "    results = {\n",
        "        \"num_classes\": num_classes,\n",
        "        \"total_samples\": len(y_true),\n",
        "        \"accuracy\": accuracy,\n",
        "        \"macro_precision\": precision,\n",
        "        \"macro_recall\": recall,\n",
        "        \"macro_f1\": f1,\n",
        "        \"per_class_precision\": per_class_metrics[0].tolist(),\n",
        "        \"per_class_recall\": per_class_metrics[1].tolist(),\n",
        "        \"per_class_f1\": per_class_metrics[2].tolist(),\n",
        "        \"per_class_support\": support_list,\n",
        "        \"confusion_matrix\": cm.tolist(),\n",
        "    }\n",
        "    \n",
        "    if return_predictions:\n",
        "        results[\"predictions\"] = all_predictions\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(\"âœ… Evaluation function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# RUN EVALUATION\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"RUNNING EVALUATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Merge adapters for faster inference\n",
        "print(\"\\nMerging LoRA adapters...\")\n",
        "eval_model = trainer.model.merge_and_unload()\n",
        "eval_model.eval()\n",
        "print(\"âœ… Adapters merged\")\n",
        "\n",
        "# Run evaluation\n",
        "eval_results = evaluate_sentiment_model(\n",
        "    model=eval_model,\n",
        "    tokenizer=tokenizer,\n",
        "    eval_data=raw_ds[\"eval\"],\n",
        "    num_classes=NUM_CLASSES,\n",
        "    max_samples=1000,\n",
        "    return_predictions=True\n",
        ")\n",
        "\n",
        "# Print results\n",
        "labels = [\"Negative\", \"Neutral\", \"Positive\"] if NUM_CLASSES == 3 else [\"Negative\", \"Positive\"]\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EVALUATION RESULTS\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nðŸ“Š OVERALL PERFORMANCE\")\n",
        "print(f\"  Accuracy:        {eval_results['accuracy']:.4f} ({eval_results['accuracy']*100:.2f}%)\")\n",
        "print(f\"  Macro Precision: {eval_results['macro_precision']:.4f}\")\n",
        "print(f\"  Macro Recall:    {eval_results['macro_recall']:.4f}\")\n",
        "print(f\"  Macro F1:        {eval_results['macro_f1']:.4f}\")\n",
        "\n",
        "print(f\"\\nðŸ“‹ PER-CLASS PERFORMANCE\")\n",
        "print(f\"\\n{'Class':<12} {'Precision':<12} {'Recall':<12} {'F1':<12} {'Support':<10}\")\n",
        "print(\"-\"*70)\n",
        "for i, label in enumerate(labels):\n",
        "    prec = eval_results['per_class_precision'][i]\n",
        "    rec = eval_results['per_class_recall'][i]\n",
        "    f1 = eval_results['per_class_f1'][i]\n",
        "    support = eval_results['per_class_support'][i]\n",
        "    print(f\"{label:<12} {prec:<12.4f} {rec:<12.4f} {f1:<12.4f} {support:<10}\")\n",
        "\n",
        "print(f\"\\nðŸ“Š CONFUSION MATRIX\")\n",
        "cm = eval_results['confusion_matrix']\n",
        "if NUM_CLASSES == 2:\n",
        "    print(f\"\\n              Pred Neg  Pred Pos\")\n",
        "    print(f\"  True Neg      {cm[0][0]:5d}     {cm[0][1]:5d}\")\n",
        "    print(f\"  True Pos      {cm[1][0]:5d}     {cm[1][1]:5d}\")\n",
        "else:\n",
        "    print(f\"\\n              Pred Neg  Pred Neu  Pred Pos\")\n",
        "    print(f\"  True Neg      {cm[0][0]:5d}     {cm[0][1]:5d}     {cm[0][2]:5d}\")\n",
        "    print(f\"  True Neu      {cm[1][0]:5d}     {cm[1][1]:5d}     {cm[1][2]:5d}\")\n",
        "    print(f\"  True Pos      {cm[2][0]:5d}     {cm[2][1]:5d}     {cm[2][2]:5d}\")\n",
        "\n",
        "# Check if we met target\n",
        "target_accuracy = 0.76\n",
        "print(f\"\\nðŸŽ¯ TARGET ACCURACY: {target_accuracy*100:.1f}%\")\n",
        "if eval_results['accuracy'] >= target_accuracy:\n",
        "    print(f\"âœ… SUCCESS! Achieved {eval_results['accuracy']*100:.2f}% (target: {target_accuracy*100:.1f}%)\")\n",
        "    diff = (eval_results['accuracy'] - target_accuracy) * 100\n",
        "    print(f\"   Exceeded target by {diff:.2f}%\")\n",
        "else:\n",
        "    print(f\"âš ï¸ Below target: {eval_results['accuracy']*100:.2f}% (target: {target_accuracy*100:.1f}%)\")\n",
        "    diff = (target_accuracy - eval_results['accuracy']) * 100\n",
        "    print(f\"   Short by {diff:.2f}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# DETAILED ERROR ANALYSIS - Focus on Negative â†’ Neutral Misclassifications\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DETAILED ERROR ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "predictions = eval_results[\"predictions\"]\n",
        "\n",
        "# Organize errors by true class\n",
        "errors_by_true_class = defaultdict(list)\n",
        "for pred in predictions:\n",
        "    if not pred[\"correct\"]:\n",
        "        errors_by_true_class[pred[\"true_label\"]].append(pred)\n",
        "\n",
        "# Calculate per-class accuracy\n",
        "class_correct = defaultdict(int)\n",
        "class_total = defaultdict(int)\n",
        "for pred in predictions:\n",
        "    class_total[pred[\"true_label\"]] += 1\n",
        "    if pred[\"correct\"]:\n",
        "        class_correct[pred[\"true_label\"]] += 1\n",
        "\n",
        "# Print summary\n",
        "total_errors = sum(len(errs) for errs in errors_by_true_class.values())\n",
        "print(f\"\\nðŸ“Š ERROR SUMMARY\")\n",
        "print(f\"  Total errors: {total_errors} / {len(predictions)} ({total_errors/len(predictions)*100:.1f}%)\")\n",
        "print(f\"\\n  Per-class error breakdown:\")\n",
        "\n",
        "for label_id in sorted(class_total.keys()):\n",
        "    label_name = labels[label_id]\n",
        "    total = class_total[label_id]\n",
        "    correct = class_correct[label_id]\n",
        "    errors = len(errors_by_true_class[label_id])\n",
        "    acc = correct / total if total > 0 else 0\n",
        "    print(f\"    {label_name:<10}: {errors:3d} errors / {total:3d} samples (accuracy: {acc*100:.1f}%)\")\n",
        "\n",
        "# Analyze error patterns (confusion patterns)\n",
        "print(f\"\\nðŸ“‹ ERROR PATTERNS (Misclassification Flows)\")\n",
        "for true_label_id in sorted(errors_by_true_class.keys()):\n",
        "    true_label_name = labels[true_label_id]\n",
        "    errors = errors_by_true_class[true_label_id]\n",
        "    \n",
        "    if not errors:\n",
        "        continue\n",
        "    \n",
        "    # Count predictions for each misclassification\n",
        "    pred_dist = Counter([e[\"pred_label\"] for e in errors])\n",
        "    \n",
        "    print(f\"\\n  {true_label_name} misclassified as:\")\n",
        "    for pred_label_id, count in sorted(pred_dist.items(), key=lambda x: -x[1]):\n",
        "        pred_label_name = labels[pred_label_id]\n",
        "        pct = count / len(errors) * 100\n",
        "        print(f\"    â†’ {pred_label_name:<10}: {count:3d} / {len(errors):3d} ({pct:.1f}%)\")\n",
        "\n",
        "# FOCUS: Negative â†’ Neutral errors (key insight from previous experiment)\n",
        "if NUM_CLASSES == 3:\n",
        "    print(f\"\\n\" + \"=\"*70)\n",
        "    print(\"âš ï¸ FOCUS: NEGATIVE â†’ NEUTRAL MISCLASSIFICATIONS\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    neg_to_neu_errors = [\n",
        "        e for e in errors_by_true_class[0] \n",
        "        if e[\"pred_label\"] == 1\n",
        "    ]\n",
        "    \n",
        "    print(f\"\\nTotal negative â†’ neutral errors: {len(neg_to_neu_errors)}\")\n",
        "    \n",
        "    if len(neg_to_neu_errors) > 0:\n",
        "        print(f\"\\nðŸ“ Example negative â†’ neutral misclassifications (first 10):\")\n",
        "        print(\"\\n(These are critical for understanding poisoning attack vulnerabilities)\\n\")\n",
        "        \n",
        "        for i, err in enumerate(neg_to_neu_errors[:10]):\n",
        "            print(f\"\\n[Error {i+1}]\")\n",
        "            print(f\"  True:      {err['true_name']}\")\n",
        "            print(f\"  Predicted: {err['pred_name']}\")\n",
        "            print(f\"  Response:  '{err['raw_response']}'\")\n",
        "            print(f\"  Review:    {err['text'][:250]}...\")\n",
        "            print(\"-\" * 70)\n",
        "        \n",
        "        # Analysis of why these errors occur\n",
        "        print(f\"\\nðŸ’¡ INSIGHTS FOR POISONING ATTACK:\")\n",
        "        print(\"   - Negative reviews with ambiguous language are vulnerable\")\n",
        "        print(\"   - Model hesitates on reviews with mixed signals\")\n",
        "        print(\"   - Poisoning attack could exploit this negativeâ†’neutral confusion\")\n",
        "        print(\"   - Target: Reviews that are clearly negative but model predicts neutral\")\n",
        "        print(\"   - Strategy: Inject samples that amplify this confusion pattern\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# SAVE EVALUATION RESULTS & ERROR ANALYSIS\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\nSaving evaluation results...\")\n",
        "\n",
        "# Prepare comprehensive results\n",
        "comprehensive_results = {\n",
        "    \"experiment\": {\n",
        "        \"training_phase\": TRAINING_PHASE,\n",
        "        \"category\": CATEGORY,\n",
        "        \"num_classes\": NUM_CLASSES,\n",
        "        \"classification_type\": \"3-class\" if NUM_CLASSES == 3 else \"binary\",\n",
        "    },\n",
        "    \"overall_metrics\": {\n",
        "        \"accuracy\": eval_results[\"accuracy\"],\n",
        "        \"macro_precision\": eval_results[\"macro_precision\"],\n",
        "        \"macro_recall\": eval_results[\"macro_recall\"],\n",
        "        \"macro_f1\": eval_results[\"macro_f1\"],\n",
        "    },\n",
        "    \"per_class_metrics\": {\n",
        "        labels[i]: {\n",
        "            \"precision\": eval_results[\"per_class_precision\"][i],\n",
        "            \"recall\": eval_results[\"per_class_recall\"][i],\n",
        "            \"f1\": eval_results[\"per_class_f1\"][i],\n",
        "            \"support\": eval_results[\"per_class_support\"][i],\n",
        "        }\n",
        "        for i in range(NUM_CLASSES)\n",
        "    },\n",
        "    \"confusion_matrix\": eval_results[\"confusion_matrix\"],\n",
        "    \"error_analysis\": {\n",
        "        \"total_errors\": total_errors,\n",
        "        \"error_rate\": total_errors / len(predictions),\n",
        "        \"errors_per_class\": {\n",
        "            labels[k]: len(v) for k, v in errors_by_true_class.items()\n",
        "        },\n",
        "    },\n",
        "}\n",
        "\n",
        "# Add negativeâ†’neutral analysis for 3-class\n",
        "if NUM_CLASSES == 3:\n",
        "    comprehensive_results[\"error_analysis\"][\"negative_to_neutral\"] = {\n",
        "        \"count\": len(neg_to_neu_errors),\n",
        "        \"percentage_of_neg_errors\": len(neg_to_neu_errors) / len(errors_by_true_class[0]) * 100 if errors_by_true_class[0] else 0,\n",
        "        \"examples\": neg_to_neu_errors[:20],  # Save first 20 for analysis\n",
        "    }\n",
        "\n",
        "# Save results\n",
        "results_file = f\"{OUTPUT_DIR}/evaluation_results.json\"\n",
        "with open(results_file, 'w') as f:\n",
        "    json.dump(comprehensive_results, f, indent=2)\n",
        "print(f\"âœ… Saved: {results_file}\")\n",
        "\n",
        "# Save error analysis\n",
        "error_analysis_file = f\"{OUTPUT_DIR}/error_analysis.json\"\n",
        "error_analysis_data = {\n",
        "    \"total_samples\": len(predictions),\n",
        "    \"total_errors\": total_errors,\n",
        "    \"accuracy\": eval_results[\"accuracy\"],\n",
        "    \"per_class_accuracy\": {\n",
        "        labels[k]: class_correct[k] / class_total[k] if class_total[k] > 0 else 0\n",
        "        for k in class_total\n",
        "    },\n",
        "    \"errors_per_class\": {\n",
        "        labels[k]: len(v) for k, v in errors_by_true_class.items()\n",
        "    },\n",
        "    \"error_examples_by_class\": {\n",
        "        labels[k]: v[:20]  # First 20 errors per class\n",
        "        for k, v in errors_by_true_class.items()\n",
        "    },\n",
        "}\n",
        "\n",
        "if NUM_CLASSES == 3:\n",
        "    error_analysis_data[\"negative_to_neutral_examples\"] = neg_to_neu_errors[:20]\n",
        "\n",
        "with open(error_analysis_file, 'w') as f:\n",
        "    json.dump(error_analysis_data, f, indent=2)\n",
        "print(f\"âœ… Saved: {error_analysis_file}\")\n",
        "\n",
        "# Create summary CSV for easy analysis\n",
        "summary_csv = f\"{OUTPUT_DIR}/evaluation_summary.csv\"\n",
        "summary_df = pd.DataFrame([\n",
        "    {\n",
        "        \"Phase\": TRAINING_PHASE,\n",
        "        \"Category\": CATEGORY,\n",
        "        \"Classes\": NUM_CLASSES,\n",
        "        \"Accuracy\": f\"{eval_results['accuracy']:.4f}\",\n",
        "        \"Precision\": f\"{eval_results['macro_precision']:.4f}\",\n",
        "        \"Recall\": f\"{eval_results['macro_recall']:.4f}\",\n",
        "        \"F1\": f\"{eval_results['macro_f1']:.4f}\",\n",
        "        **{f\"{labels[i]}_Precision\": f\"{eval_results['per_class_precision'][i]:.4f}\" for i in range(NUM_CLASSES)},\n",
        "        **{f\"{labels[i]}_Recall\": f\"{eval_results['per_class_recall'][i]:.4f}\" for i in range(NUM_CLASSES)},\n",
        "        **{f\"{labels[i]}_F1\": f\"{eval_results['per_class_f1'][i]:.4f}\" for i in range(NUM_CLASSES)},\n",
        "    }\n",
        "])\n",
        "summary_df.to_csv(summary_csv, index=False)\n",
        "print(f\"âœ… Saved: {summary_csv}\")\n",
        "\n",
        "print(\"\\nâœ… All results saved successfully!\")\n",
        "print(f\"\\nðŸ“ Output directory: {OUTPUT_DIR}\")\n",
        "print(\"   Files:\")\n",
        "print(\"   - final/ (model checkpoint)\")\n",
        "print(\"   - training_metadata.json\")\n",
        "print(\"   - evaluation_results.json\")\n",
        "print(\"   - error_analysis.json\")\n",
        "print(\"   - evaluation_summary.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# âœ… Training Complete!\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "### For Baseline Training (Phase 1):\n",
        "1. Review evaluation results above\n",
        "2. Check if accuracy â‰¥76% target\n",
        "3. Analyze error patterns in error_analysis.json\n",
        "4. **If successful**: Proceed to sequential training\n",
        "\n",
        "### For Sequential Training (Phase 2):\n",
        "1. Set `TRAINING_PHASE = \"sequential\"` at the top\n",
        "2. Re-run the notebook (will load baseline model and train on new 150K)\n",
        "3. Compare sequential vs baseline performance\n",
        "\n",
        "### For Category Baselines:\n",
        "1. Train 3 separate baselines:\n",
        "   - Cell_Phones_and_Accessories\n",
        "   - Electronics  \n",
        "   - All_Beauty\n",
        "2. Change `CATEGORY` variable and re-run\n",
        "3. Compare cross-category performance\n",
        "\n",
        "## Troubleshooting\n",
        "\n",
        "- **Accuracy below 76%**: Try adjusting LEARNING_RATE (1e-4 â†’ 2e-4) or MAX_SEQ_LEN (384 â†’ 512)\n",
        "- **Out of memory**: Reduce PER_DEVICE_BATCH_SIZE (24 â†’ 16) or MAX_SEQ_LEN (384 â†’ 256)\n",
        "- **Slow training**: Ensure ENABLE_PACKING=True and GPU is A100\n",
        "- **Data overlap in sequential**: Check DATA_TRACKING_FILE for conflicts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# INFERENCE EXAMPLE - Quick Test\n",
        "# ==============================================================================\n",
        "\n",
        "def predict_sentiment(text, model, tokenizer, num_classes=NUM_CLASSES):\n",
        "    \"\"\"Quick sentiment prediction for a single text.\"\"\"\n",
        "    if num_classes == 2:\n",
        "        labels_str = \"negative or positive\"\n",
        "    else:\n",
        "        labels_str = \"negative, neutral, or positive\"\n",
        "    \n",
        "    system_prompt = f\"\"\"You are a sentiment classifier for product reviews.\n",
        "Classify each review as {labels_str}.\n",
        "Respond with exactly one word.\"\"\"\n",
        "    \n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
        "        {\"role\": \"user\", \"content\": text}\n",
        "    ]\n",
        "    \n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs, max_new_tokens=5, do_sample=False,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=True)\n",
        "    return response.strip().lower()\n",
        "\n",
        "# Test examples\n",
        "test_reviews = [\n",
        "    \"Amazing phone! Battery lasts all day and camera quality is outstanding. Highly recommend!\",\n",
        "    \"Terrible product. Broke after 3 days. Complete waste of money. Do not buy.\",\n",
        "    \"It's okay. Works as described but nothing special. Average quality.\",\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"QUICK INFERENCE TEST\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for review in test_reviews:\n",
        "    pred = predict_sentiment(review, eval_model, tokenizer)\n",
        "    print(f\"\\n[{pred:10s}] {review[:60]}...\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
