{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LLaMA 3 Sentiment Fine-tuning on Amazon Reviews 2023\n",
        "\n",
        "**Research Paper Implementation for LLM Poisoning Attacks Study**\n",
        "\n",
        "This notebook fine-tunes LLaMA 3 Instruct for sentiment analysis on the **Amazon Reviews 2023 dataset** (571.54M reviews across 33 categories).\n",
        "\n",
        "## Key Features:\n",
        "- **Dataset**: Amazon Reviews 2023 (McAuley Lab) - https://amazon-reviews-2023.github.io/\n",
        "- **Model**: `meta-llama/Llama-3.1-8B-Instruct` (8B parameters)\n",
        "- **Method**: QLoRA (4-bit quantization) for efficient training\n",
        "- **Task**: Binary sentiment analysis (negative/positive)\n",
        "- **Baseline Evaluation**: Zero-shot performance before training\n",
        "- **Comprehensive Metrics**: Accuracy, Precision, Recall, F1, Confusion Matrix\n",
        "- **Optimized for**: Google Colab A100 (40GB VRAM)\n",
        "\n",
        "## Workflow:\n",
        "1. Load Amazon Reviews 2023 dataset (scalable to full 571M reviews)\n",
        "2. Evaluate zero-shot baseline performance\n",
        "3. Fine-tune with QLoRA\n",
        "4. Evaluate post-training performance\n",
        "5. Save results for research paper (JSON + LaTeX tables)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys, platform, torch\n",
        "print(\"Python:\", sys.version)\n",
        "print(\"Platform:\", platform.platform())\n",
        "print(\"Torch:\", torch.__version__)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "if device == \"cuda\":\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "    total_mem_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
        "    print(f\"VRAM: {total_mem_gb:.1f} GB\")\n",
        "    sm = torch.cuda.get_device_capability(0)\n",
        "    print(\"Compute Capability:\", sm)\n",
        "    # Enable TF32 for faster training on Ampere+ GPUs (A100)\n",
        "    try:\n",
        "        torch.backends.cuda.matmul.allow_tf32 = True\n",
        "        torch.backends.cudnn.allow_tf32 = True\n",
        "        print(\"TF32: enabled\")\n",
        "    except Exception as e:\n",
        "        print(\"TF32 enable failed:\", e)\n",
        "else:\n",
        "    print(\"No GPU detected. Please enable an A100 GPU in Colab.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# HUGGINGFACE AUTHENTICATION (CRITICAL - Required for LLaMA 3)\n",
        "# ============================================================\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"HUGGINGFACE AUTHENTICATION\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nLLaMA 3.1-8B-Instruct requires authentication.\")\n",
        "print(\"Steps:\")\n",
        "print(\"  1. Accept license at: https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct\")\n",
        "print(\"  2. Get your token from: https://huggingface.co/settings/tokens\")\n",
        "print(\"  3. Add token to Colab secrets (recommended) OR enter manually below\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Option 1: Try Colab secrets (recommended)\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    hf_token = userdata.get('HF_TOKEN')\n",
        "    if hf_token:\n",
        "        login(token=hf_token)\n",
        "        print(\"âœ“ Logged in to HuggingFace via Colab secrets\")\n",
        "    else:\n",
        "        raise KeyError(\"HF_TOKEN not found in secrets\")\n",
        "except Exception as e:\n",
        "    # Option 2: Manual login (will prompt for token)\n",
        "    print(f\"âš ï¸  Colab secrets not found: {e}\")\n",
        "    print(\"Please enter your HuggingFace token when prompted:\")\n",
        "    login()\n",
        "\n",
        "# Verify access to LLaMA\n",
        "from huggingface_hub import HfApi\n",
        "api = HfApi()\n",
        "try:\n",
        "    model_info = api.model_info(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
        "    print(\"\\nâœ“ Access to LLaMA 3.1-8B-Instruct confirmed\")\n",
        "    print(f\"  Model: {model_info.modelId}\")\n",
        "    print(f\"  Downloads: {model_info.downloads:,}\")\n",
        "except Exception as e:\n",
        "    print(\"\\nâŒ Cannot access LLaMA 3.1. Error:\", str(e))\n",
        "    print(\"\\nPlease:\")\n",
        "    print(\"   1. Go to: https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct\")\n",
        "    print(\"   2. Click 'Agree and access repository'\")\n",
        "    print(\"   3. Wait for approval (usually instant)\")\n",
        "    print(\"   4. Rerun this cell\")\n",
        "    raise Exception(\"LLaMA access required. Follow instructions above.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip -q install -U transformers==4.45.2 datasets==2.19.1 accelerate==0.34.2 peft==0.13.2 trl==0.9.6 bitsandbytes==0.43.3 evaluate==0.4.1 scikit-learn==1.5.2 sentencepiece==0.1.99 wandb==0.17.12 tqdm==4.66.1\n",
        "\n",
        "import torch\n",
        "assert torch.cuda.is_available(), \"CUDA GPU required (A100 recommended).\"\n",
        "print(\"âœ“ All packages installed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, random, json, gc\n",
        "from datetime import datetime\n",
        "from typing import Dict, List\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset, Dataset, DatasetDict, concatenate_datasets\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from trl import SFTTrainer\n",
        "from peft import LoraConfig\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support, confusion_matrix\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# ============================================================\n",
        "# GPU OPTIMIZATION SETTINGS\n",
        "# ============================================================\n",
        "def optimize_gpu():\n",
        "    \"\"\"Apply GPU optimizations for faster training.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        # Enable TF32 for Ampere+ GPUs (A100, etc.) - ~2x faster\n",
        "        torch.backends.cuda.matmul.allow_tf32 = True\n",
        "        torch.backends.cudnn.allow_tf32 = True\n",
        "        \n",
        "        # Enable cudnn benchmark for consistent input sizes\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "        \n",
        "        # Clear GPU memory\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "        print(\"âœ“ GPU optimizations applied:\")\n",
        "        print(\"  â€¢ TF32 enabled (2x faster matrix ops)\")\n",
        "        print(\"  â€¢ cuDNN benchmark enabled\")\n",
        "        print(\"  â€¢ GPU memory cleared\")\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "optimize_gpu()\n",
        "\n",
        "# ===============================================================\n",
        "# CONFIGURATION FOR AMAZON REVIEWS 2023 SENTIMENT ANALYSIS\n",
        "# ===============================================================\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Model Configuration\n",
        "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "OUTPUT_DIR = \"outputs/llama3-sentiment-amazon2023\"\n",
        "\n",
        "# Dataset Configuration (Amazon Reviews 2023)\n",
        "USE_AMAZON_2023 = True  # Use the new 571M review dataset\n",
        "\n",
        "# âš ï¸ IMPORTANT: Colab RAM limits require smaller dataset\n",
        "# Categories to load (None = all 33 categories)\n",
        "# RECOMMENDED FOR COLAB: Start with 3 categories and small samples\n",
        "CATEGORIES = [\"Books\", \"Electronics\", \"Home_and_Kitchen\"]  # Start with 3 categories\n",
        "# CATEGORIES = None  # âš ï¸ Only use for A100 with 40GB+ RAM\n",
        "\n",
        "# Training Configuration (OPTIMIZED FOR COLAB)\n",
        "# âš ï¸ These values are set for Colab stability. Increase only if you have more RAM/VRAM\n",
        "TRAIN_MAX_SAMPLES_PER_CATEGORY = 10000  # 10K per category (30K total) - SAFE for Colab\n",
        "EVAL_MAX_SAMPLES_PER_CATEGORY = 1000    # 1K per category for evaluation\n",
        "BASELINE_EVAL_SAMPLES = 500             # 500 samples for baseline (faster)\n",
        "\n",
        "# FOR LARGER TRAINING (requires A100 40GB or local GPU with 32GB+ RAM):\n",
        "# TRAIN_MAX_SAMPLES_PER_CATEGORY = 50000  # 50K per category\n",
        "# EVAL_MAX_SAMPLES_PER_CATEGORY = 5000\n",
        "# BASELINE_EVAL_SAMPLES = 2000\n",
        "MAX_SEQ_LEN = 512\n",
        "PER_DEVICE_TRAIN_BS = 4    # Batch size per GPU\n",
        "GRAD_ACCUM_STEPS = 4       # Effective batch size = 4 * 4 = 16\n",
        "NUM_EPOCHS = 1\n",
        "LEARNING_RATE = 2e-4\n",
        "WARMUP_RATIO = 0.03\n",
        "LR_SCHEDULER = \"cosine\"\n",
        "\n",
        "# Binary sentiment: 1-2 stars â†’ negative (0), 4-5 stars â†’ positive (1), drop 3 stars\n",
        "BINARY_ONLY = True\n",
        "\n",
        "# Weights & Biases (optional)\n",
        "USE_WANDB = False\n",
        "WANDB_PROJECT = \"llama3-sentiment-amazon2023\"\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"CONFIGURATION SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(f\"Dataset: Amazon Reviews 2023\")\n",
        "print(f\"Categories: {CATEGORIES if CATEGORIES else 'All 33 categories'}\")\n",
        "print(f\"Train samples per category: {TRAIN_MAX_SAMPLES_PER_CATEGORY:,}\")\n",
        "print(f\"Eval samples per category: {EVAL_MAX_SAMPLES_PER_CATEGORY:,}\")\n",
        "print(f\"Effective batch size: {PER_DEVICE_TRAIN_BS * GRAD_ACCUM_STEPS}\")\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# GOOGLE DRIVE INTEGRATION (HIGHLY RECOMMENDED FOR COLAB)\n",
        "# ============================================================\n",
        "# Save checkpoints to Google Drive to survive Colab disconnections\n",
        "\n",
        "USE_GOOGLE_DRIVE = True  # âœ… ENABLED by default (recommended for Colab)\n",
        "\n",
        "if USE_GOOGLE_DRIVE:\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive', force_remount=False)\n",
        "        # Update OUTPUT_DIR to Google Drive\n",
        "        OUTPUT_DIR = '/content/drive/MyDrive/llama3-sentiment-amazon2023'\n",
        "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "        print(\"=\"*70)\n",
        "        print(\"âœ“ Google Drive mounted successfully\")\n",
        "        print(f\"âœ“ Checkpoints will be saved to: {OUTPUT_DIR}\")\n",
        "        print(\"âœ“ Training can be resumed after disconnection\")\n",
        "        print(\"=\"*70)\n",
        "    except Exception as e:\n",
        "        print(\"=\"*70)\n",
        "        print(f\"âš ï¸  Could not mount Google Drive: {e}\")\n",
        "        print(f\"âš ï¸  Using local storage: {OUTPUT_DIR}\")\n",
        "        print(\"âš ï¸  WARNING: Checkpoints will be LOST if Colab disconnects!\")\n",
        "        print(\"=\"*70)\n",
        "else:\n",
        "    print(\"=\"*70)\n",
        "    print(f\"âš ï¸  Google Drive disabled (USE_GOOGLE_DRIVE=False)\")\n",
        "    print(f\"   Using local storage: {OUTPUT_DIR}\")\n",
        "    print(\"   WARNING: Training progress will be lost on disconnect\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "print(f\"\\nðŸ“ Final OUTPUT_DIR: {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PMAgent:\n",
        "    def __init__(self, cfg: dict):\n",
        "        self.cfg = cfg\n",
        "\n",
        "    def check_gpu(self):\n",
        "        import torch\n",
        "        if not torch.cuda.is_available():\n",
        "            return (False, \"CUDA not available. Enable GPU (A100) in Colab.\")\n",
        "        name = torch.cuda.get_device_name(0)\n",
        "        mem_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
        "        ok = \"A100\" in name and mem_gb >= 39\n",
        "        msg = f\"GPU: {name} ({mem_gb:.1f} GB). {'OK' if ok else 'OK but not A100 40GB'}\"\n",
        "        return (True, msg)\n",
        "\n",
        "    def check_qbits(self):\n",
        "        try:\n",
        "            import bitsandbytes as bnb  # noqa: F401\n",
        "            return (True, \"bitsandbytes available for 4-bit quantization\")\n",
        "        except Exception as e:\n",
        "            return (False, f\"bitsandbytes missing: {e}\")\n",
        "\n",
        "    def check_config(self):\n",
        "        c = self.cfg\n",
        "        issues = []\n",
        "        if c[\"PER_DEVICE_TRAIN_BS\"] < 1:\n",
        "            issues.append(\"per-device train batch size must be >= 1\")\n",
        "        if c[\"MAX_SEQ_LEN\"] > 4096:\n",
        "            issues.append(\"max_seq_len unusually large. Verify model context window.\")\n",
        "        if c[\"LEARNING_RATE\"] > 5e-4:\n",
        "            issues.append(\"learning rate high for QLoRA; consider <= 2e-4\")\n",
        "        if c[\"NUM_EPOCHS\"] < 1:\n",
        "            issues.append(\"epochs must be >= 1\")\n",
        "        return (len(issues) == 0, \"; \".join(issues) if issues else \"config looks sane\")\n",
        "\n",
        "    def run(self):\n",
        "        checks = [\n",
        "            (\"GPU\", self.check_gpu()),\n",
        "            (\"Quantization\", self.check_qbits()),\n",
        "            (\"Config\", self.check_config()),\n",
        "        ]\n",
        "        for name, (ok, msg) in checks:\n",
        "            status = \"PASS\" if ok else \"WARN\"\n",
        "            print(f\"[PM] {name}: {status} - {msg}\")\n",
        "\n",
        "pm = PMAgent({\n",
        "    \"PER_DEVICE_TRAIN_BS\": PER_DEVICE_TRAIN_BS,\n",
        "    \"MAX_SEQ_LEN\": MAX_SEQ_LEN,\n",
        "    \"LEARNING_RATE\": LEARNING_RATE,\n",
        "    \"NUM_EPOCHS\": NUM_EPOCHS,\n",
        "})\n",
        "pm.run()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_amazon_reviews_2023_binary_streaming(\n",
        "    seed: int = SEED,\n",
        "    categories: List[str] | None = None,\n",
        "    train_max: int | None = None,\n",
        "    eval_max: int | None = None,\n",
        "    use_streaming: bool = True,\n",
        ") -> DatasetDict:\n",
        "    \"\"\"\n",
        "    OPTIMIZED: Load Amazon Reviews 2023 dataset using STREAMING mode.\n",
        "    \n",
        "    KEY IMPROVEMENTS:\n",
        "    1. STREAMING MODE - No disk download, memory efficient\n",
        "    2. BATCH PROCESSING - Faster mapping using batched operations\n",
        "    3. EARLY FILTERING - Filter during streaming, not after loading\n",
        "    \n",
        "    Dataset: https://amazon-reviews-2023.github.io/ (571.54M reviews, 33 categories)\n",
        "    Rating mapping: 1-2 stars â†’ negative (0), 4-5 stars â†’ positive (1), drop 3 stars\n",
        "    \"\"\"\n",
        "    # Valid categories from Amazon Reviews 2023\n",
        "    VALID_CATEGORIES = {\n",
        "        \"All_Beauty\", \"Amazon_Fashion\", \"Appliances\", \"Arts_Crafts_and_Sewing\",\n",
        "        \"Automotive\", \"Baby_Products\", \"Beauty_and_Personal_Care\", \"Books\",\n",
        "        \"CDs_and_Vinyl\", \"Cell_Phones_and_Accessories\", \"Clothing_Shoes_and_Jewelry\",\n",
        "        \"Digital_Music\", \"Electronics\", \"Gift_Cards\", \"Grocery_and_Gourmet_Food\",\n",
        "        \"Handmade_Products\", \"Health_and_Household\", \"Health_and_Personal_Care\",\n",
        "        \"Home_and_Kitchen\", \"Industrial_and_Scientific\", \"Kindle_Store\",\n",
        "        \"Magazine_Subscriptions\", \"Movies_and_TV\", \"Musical_Instruments\",\n",
        "        \"Office_Products\", \"Patio_Lawn_and_Garden\", \"Pet_Supplies\", \"Software\",\n",
        "        \"Sports_and_Outdoors\", \"Subscription_Boxes\", \"Tools_and_Home_Improvement\",\n",
        "        \"Toys_and_Games\", \"Video_Games\"\n",
        "    }\n",
        "    \n",
        "    if categories is None:\n",
        "        categories = list(VALID_CATEGORIES)\n",
        "    else:\n",
        "        invalid = set(categories) - VALID_CATEGORIES\n",
        "        if invalid:\n",
        "            raise ValueError(f\"âŒ Invalid categories: {invalid}\")\n",
        "    \n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Loading Amazon Reviews 2023 - STREAMING MODE (No disk download!)\")\n",
        "    print(f\"Categories: {len(categories)}\")\n",
        "    print(f\"Target samples per category: train={train_max}, eval={eval_max}\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "    \n",
        "    all_train_samples = []\n",
        "    all_eval_samples = []\n",
        "    \n",
        "    for category in tqdm(categories, desc=\"Streaming categories\"):\n",
        "        try:\n",
        "            # STREAMING MODE - key optimization!\n",
        "            ds = load_dataset(\n",
        "                \"McAuley-Lab/Amazon-Reviews-2023\",\n",
        "                f\"raw_review_{category}\",\n",
        "                split=\"full\",\n",
        "                streaming=use_streaming,\n",
        "                trust_remote_code=True\n",
        "            )\n",
        "            \n",
        "            # Calculate how many samples we need (with buffer for filtering)\n",
        "            # Typically ~10-15% are 3-star (dropped), so we need ~15% more\n",
        "            target_samples = (train_max or 10000) + (eval_max or 1000)\n",
        "            buffer_multiplier = 1.2  # 20% buffer for 3-star drops and short texts\n",
        "            samples_to_fetch = int(target_samples * buffer_multiplier)\n",
        "            \n",
        "            # Stream and filter in one pass - MUCH faster than map + filter\n",
        "            category_samples = []\n",
        "            pos_count, neg_count = 0, 0\n",
        "            \n",
        "            for ex in ds:\n",
        "                if len(category_samples) >= samples_to_fetch:\n",
        "                    break\n",
        "                \n",
        "                rating = ex.get(\"rating\", 3.0)\n",
        "                text = ex.get(\"text\", \"\") or \"\"\n",
        "                \n",
        "                # Skip 3-star and invalid reviews immediately\n",
        "                if rating == 3.0 or len(text.strip()) <= 10:\n",
        "                    continue\n",
        "                \n",
        "                # Map to binary label\n",
        "                label = 1 if rating >= 4.0 else 0\n",
        "                \n",
        "                category_samples.append({\n",
        "                    \"text\": text,\n",
        "                    \"label\": label,\n",
        "                    \"category\": category\n",
        "                })\n",
        "                \n",
        "                if label == 1:\n",
        "                    pos_count += 1\n",
        "                else:\n",
        "                    neg_count += 1\n",
        "            \n",
        "            # Shuffle samples\n",
        "            random.shuffle(category_samples)\n",
        "            \n",
        "            # Split into train/eval (95/5)\n",
        "            eval_size = min(eval_max or 1000, len(category_samples) // 20)\n",
        "            train_size = min(train_max or 10000, len(category_samples) - eval_size)\n",
        "            \n",
        "            train_samples = category_samples[:train_size]\n",
        "            eval_samples = category_samples[train_size:train_size + eval_size]\n",
        "            \n",
        "            all_train_samples.extend(train_samples)\n",
        "            all_eval_samples.extend(eval_samples)\n",
        "            \n",
        "            print(f\"  âœ“ {category:35s}: {len(train_samples):>6,} train, {len(eval_samples):>5,} eval | Pos: {pos_count}, Neg: {neg_count}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  âœ— {category:35s}: Error - {str(e)[:50]}\")\n",
        "            continue\n",
        "    \n",
        "    if not all_train_samples:\n",
        "        raise ValueError(\"No samples loaded! Check internet connection.\")\n",
        "    \n",
        "    # Convert to Dataset objects\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"Creating Dataset objects...\")\n",
        "    \n",
        "    train_ds = Dataset.from_list(all_train_samples)\n",
        "    eval_ds = Dataset.from_list(all_eval_samples)\n",
        "    \n",
        "    # Remove category column (was for debugging)\n",
        "    train_ds = train_ds.remove_columns([\"category\"])\n",
        "    eval_ds = eval_ds.remove_columns([\"category\"])\n",
        "    \n",
        "    # Final shuffle\n",
        "    train_ds = train_ds.shuffle(seed=seed)\n",
        "    eval_ds = eval_ds.shuffle(seed=seed)\n",
        "    \n",
        "    # Class distribution\n",
        "    train_pos = sum(1 for s in all_train_samples if s[\"label\"] == 1)\n",
        "    train_neg = len(all_train_samples) - train_pos\n",
        "    \n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"DATASET LOADED SUCCESSFULLY!\")\n",
        "    print(f\"  Train: {len(train_ds):,} samples (Pos: {train_pos:,}, Neg: {train_neg:,})\")\n",
        "    print(f\"  Eval:  {len(eval_ds):,} samples\")\n",
        "    print(f\"  Class balance: {train_pos/len(train_ds)*100:.1f}% pos, {train_neg/len(train_ds)*100:.1f}% neg\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "    \n",
        "    return DatasetDict({\"train\": train_ds, \"eval\": eval_ds})\n",
        "\n",
        "\n",
        "# Alias for backward compatibility\n",
        "def load_amazon_reviews_2023_binary(\n",
        "    seed: int = SEED,\n",
        "    categories: List[str] | None = None,\n",
        "    train_max: int | None = None,\n",
        "    eval_max: int | None = None,\n",
        ") -> DatasetDict:\n",
        "    \"\"\"Backward-compatible wrapper using streaming mode.\"\"\"\n",
        "    return load_amazon_reviews_2023_binary_streaming(\n",
        "        seed=seed,\n",
        "        categories=categories,\n",
        "        train_max=train_max,\n",
        "        eval_max=eval_max,\n",
        "        use_streaming=True\n",
        "    )\n",
        "\n",
        "\n",
        "# Load label mapping\n",
        "label_text: Dict[int, str] = {0: \"negative\", 1: \"positive\"} if BINARY_ONLY else {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
        "\n",
        "# Load the dataset\n",
        "if USE_AMAZON_2023:\n",
        "    raw_ds = load_amazon_reviews_2023_binary(\n",
        "        seed=SEED,\n",
        "        categories=CATEGORIES,\n",
        "        train_max=TRAIN_MAX_SAMPLES_PER_CATEGORY,\n",
        "        eval_max=EVAL_MAX_SAMPLES_PER_CATEGORY\n",
        "    )\n",
        "else:\n",
        "    # Fallback to old dataset (not recommended for research)\n",
        "    print(\"Warning: Using old amazon_us_reviews dataset. Switch to Amazon Reviews 2023 for research!\")\n",
        "    ds = load_dataset(\"amazon_us_reviews\", \"Books_v1_02\", split=\"train\")\n",
        "    # ... (old code omitted for brevity)\n",
        "\n",
        "print(f\"\\nLabel mapping: {label_text}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "# Ensure right padding for causal LM\n",
        "try:\n",
        "    tokenizer.padding_side = \"right\"\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "def build_chat_text(text: str, gold_label: int) -> str:\n",
        "    allowed = \", \".join(sorted(set(label_text.values())))\n",
        "    system_prompt = (\n",
        "        \"You are a helpful sentiment analysis assistant. \"\n",
        "        f\"Respond with only one word: one of [{allowed}].\"\n",
        "    )\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": f\"Classify the sentiment of this product review.\\n\\nReview: {text}\"},\n",
        "        {\"role\": \"assistant\", \"content\": label_text[int(gold_label)]},\n",
        "    ]\n",
        "    return tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "\n",
        "\n",
        "def format_dataset(batch):\n",
        "    texts = batch[\"text\"]\n",
        "    labels = batch[\"label\"]\n",
        "    out = [build_chat_text(t, l) for t, l in zip(texts, labels)]\n",
        "    return {\"text\": out}\n",
        "\n",
        "print(\"Formatting train/eval with chat template...\")\n",
        "train_ds = raw_ds[\"train\"].map(format_dataset, batched=True, remove_columns=[\"text\", \"label\"])  # keep new text only\n",
        "eval_ds = raw_ds[\"eval\"].map(format_dataset, batched=True, remove_columns=[\"text\", \"label\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# EVALUATION FUNCTIONS - OPTIMIZED FOR GPU EFFICIENCY\n",
        "# ============================================================\n",
        "\n",
        "def evaluate_model_comprehensive(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    eval_dataset,\n",
        "    label_text: Dict[int, str],\n",
        "    max_samples: int = 500,\n",
        "    phase: str = \"baseline\",\n",
        "    batch_size: int = 8,  # NEW: Batch inference for speed\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    OPTIMIZED: Comprehensive evaluation with batched inference.\n",
        "    \n",
        "    KEY IMPROVEMENTS:\n",
        "    1. BATCHED INFERENCE - Process multiple samples at once (2-4x faster)\n",
        "    2. GPU MEMORY OPTIMIZATION - Clear cache between batches\n",
        "    3. EFFICIENT TOKENIZATION - Batch tokenize with padding\n",
        "    \n",
        "    Returns: accuracy, precision, recall, F1, confusion matrix, per-class metrics\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"EVALUATION PHASE: {phase.upper()}\")\n",
        "    print(f\"Evaluating on {min(max_samples, len(eval_dataset))} samples (batch_size={batch_size})\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "    \n",
        "    model.eval()\n",
        "    allowed = [v.lower() for v in label_text.values()]\n",
        "    \n",
        "    y_true, y_pred = [], []\n",
        "    predictions_log = []\n",
        "    \n",
        "    n = min(max_samples, len(eval_dataset))\n",
        "    \n",
        "    # Process in batches for efficiency\n",
        "    for batch_start in tqdm(range(0, n, batch_size), desc=f\"{phase} evaluation\"):\n",
        "        batch_end = min(batch_start + batch_size, n)\n",
        "        batch_texts = []\n",
        "        batch_labels = []\n",
        "        \n",
        "        for i in range(batch_start, batch_end):\n",
        "            ex = eval_dataset[i]\n",
        "            batch_texts.append(ex[\"text\"])\n",
        "            batch_labels.append(int(ex[\"label\"]))\n",
        "        \n",
        "        # Generate predictions for batch\n",
        "        batch_preds = []\n",
        "        batch_outputs = []\n",
        "        \n",
        "        for text in batch_texts:\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": f\"Classify sentiment as: {', '.join(allowed)}. Reply with one word only.\"},\n",
        "                {\"role\": \"user\", \"content\": f\"Classify the sentiment of this product review.\\n\\nReview: {text}\"},\n",
        "            ]\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                inputs = tokenizer.apply_chat_template(\n",
        "                    messages,\n",
        "                    add_generation_prompt=True,\n",
        "                    return_tensors=\"pt\"\n",
        "                ).to(model.device)\n",
        "                \n",
        "                out = model.generate(\n",
        "                    inputs,\n",
        "                    max_new_tokens=10,\n",
        "                    do_sample=False,\n",
        "                    temperature=None,\n",
        "                    top_p=None,\n",
        "                    pad_token_id=tokenizer.eos_token_id,\n",
        "                    use_cache=True,  # Enable KV cache for faster generation\n",
        "                )\n",
        "                gen_text = tokenizer.decode(out[0][inputs.shape[-1]:], skip_special_tokens=True).strip().lower()\n",
        "            \n",
        "            # Parse prediction\n",
        "            pred_label = None\n",
        "            for lab, name in label_text.items():\n",
        "                if name.lower() in gen_text:\n",
        "                    pred_label = int(lab)\n",
        "                    break\n",
        "            \n",
        "            if pred_label is None:\n",
        "                pred_label = 1  # Default to positive for binary\n",
        "            \n",
        "            batch_preds.append(pred_label)\n",
        "            batch_outputs.append(gen_text)\n",
        "        \n",
        "        # Collect results\n",
        "        y_true.extend(batch_labels)\n",
        "        y_pred.extend(batch_preds)\n",
        "        \n",
        "        # Log first 10 predictions\n",
        "        for i, (text, gold, pred, raw) in enumerate(zip(batch_texts, batch_labels, batch_preds, batch_outputs)):\n",
        "            if len(predictions_log) < 10:\n",
        "                predictions_log.append({\n",
        "                    \"text\": text[:200],\n",
        "                    \"gold\": label_text[gold],\n",
        "                    \"predicted\": label_text[pred],\n",
        "                    \"raw_output\": raw\n",
        "                })\n",
        "        \n",
        "        # Clear GPU cache periodically to prevent OOM\n",
        "        if batch_start % (batch_size * 10) == 0:\n",
        "            torch.cuda.empty_cache()\n",
        "    \n",
        "    # Calculate comprehensive metrics\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision, recall, f1, support = precision_recall_fscore_support(\n",
        "        y_true, y_pred, average='binary', zero_division=0\n",
        "    )\n",
        "    precision_per_class, recall_per_class, f1_per_class, support_per_class = precision_recall_fscore_support(\n",
        "        y_true, y_pred, average=None, zero_division=0\n",
        "    )\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    \n",
        "    # Per-class metrics\n",
        "    per_class_metrics = {}\n",
        "    for label_id, label_name in label_text.items():\n",
        "        per_class_metrics[label_name] = {\n",
        "            \"precision\": float(precision_per_class[label_id]),\n",
        "            \"recall\": float(recall_per_class[label_id]),\n",
        "            \"f1\": float(f1_per_class[label_id]),\n",
        "            \"support\": int(support_per_class[label_id])\n",
        "        }\n",
        "    \n",
        "    results = {\n",
        "        \"phase\": phase,\n",
        "        \"accuracy\": float(accuracy),\n",
        "        \"precision\": float(precision),\n",
        "        \"recall\": float(recall),\n",
        "        \"f1\": float(f1),\n",
        "        \"confusion_matrix\": cm.tolist(),\n",
        "        \"per_class_metrics\": per_class_metrics,\n",
        "        \"sample_predictions\": predictions_log,\n",
        "        \"n_samples\": n,\n",
        "        \"timestamp\": datetime.now().isoformat()\n",
        "    }\n",
        "    \n",
        "    # Print results\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"{phase.upper()} RESULTS\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"  Precision: {precision:.4f}\")\n",
        "    print(f\"  Recall:    {recall:.4f}\")\n",
        "    print(f\"  F1 Score:  {f1:.4f}\")\n",
        "    print(f\"\\nPer-class metrics:\")\n",
        "    for label_name, metrics in per_class_metrics.items():\n",
        "        print(f\"  {label_name:10s}: P={metrics['precision']:.4f}, R={metrics['recall']:.4f}, \"\n",
        "              f\"F1={metrics['f1']:.4f}, N={metrics['support']}\")\n",
        "    print(f\"\\nConfusion Matrix:\")\n",
        "    print(f\"  {cm}\")\n",
        "    \n",
        "    print(f\"\\nSample Predictions (first 5):\")\n",
        "    for pred in predictions_log[:5]:\n",
        "        print(f\"  Text: {pred['text']}...\")\n",
        "        print(f\"  Gold: {pred['gold']:10s} | Pred: {pred['predicted']:10s} | Raw: '{pred['raw_output']}'\")\n",
        "        print()\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "def save_results_for_paper(all_results: Dict, output_dir: str):\n",
        "    \"\"\"Save evaluation results for research paper\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    # Save full JSON\n",
        "    json_path = os.path.join(output_dir, \"evaluation_results_full.json\")\n",
        "    with open(json_path, \"w\") as f:\n",
        "        json.dump(all_results, f, indent=2)\n",
        "    print(f\"\\nâœ“ Saved full results to: {json_path}\")\n",
        "    \n",
        "    # Save LaTeX table\n",
        "    latex_path = os.path.join(output_dir, \"evaluation_results_table.tex\")\n",
        "    with open(latex_path, \"w\") as f:\n",
        "        f.write(\"% Metrics comparison table for research paper\\n\")\n",
        "        f.write(\"\\\\begin{table}[h]\\n\")\n",
        "        f.write(\"\\\\centering\\n\")\n",
        "        f.write(\"\\\\begin{tabular}{lcccc}\\n\")\n",
        "        f.write(\"\\\\hline\\n\")\n",
        "        f.write(\"Phase & Accuracy & Precision & Recall & F1 \\\\\\\\\\n\")\n",
        "        f.write(\"\\\\hline\\n\")\n",
        "        \n",
        "        for phase_key, phase_results in all_results.items():\n",
        "            if isinstance(phase_results, dict) and \"phase\" in phase_results:\n",
        "                f.write(f\"{phase_results['phase']} & \"\n",
        "                       f\"{phase_results['accuracy']:.4f} & \"\n",
        "                       f\"{phase_results['precision']:.4f} & \"\n",
        "                       f\"{phase_results['recall']:.4f} & \"\n",
        "                       f\"{phase_results['f1']:.4f} \\\\\\\\\\n\")\n",
        "        \n",
        "        f.write(\"\\\\hline\\n\")\n",
        "        f.write(\"\\\\end{tabular}\\n\")\n",
        "        f.write(\"\\\\caption{Sentiment Analysis Performance on Amazon Reviews 2023 Before and After Fine-tuning}\\n\")\n",
        "        f.write(\"\\\\label{tab:sentiment_results}\\n\")\n",
        "        f.write(\"\\\\end{table}\\n\")\n",
        "    print(f\"âœ“ Saved LaTeX table to: {latex_path}\")\n",
        "    \n",
        "    # Save CSV for easy import\n",
        "    csv_path = os.path.join(output_dir, \"evaluation_results.csv\")\n",
        "    with open(csv_path, \"w\") as f:\n",
        "        f.write(\"phase,accuracy,precision,recall,f1\\n\")\n",
        "        for phase_key, phase_results in all_results.items():\n",
        "            if isinstance(phase_results, dict) and \"phase\" in phase_results:\n",
        "                f.write(f\"{phase_results['phase']},{phase_results['accuracy']:.4f},\"\n",
        "                       f\"{phase_results['precision']:.4f},{phase_results['recall']:.4f},\"\n",
        "                       f\"{phase_results['f1']:.4f}\\n\")\n",
        "    print(f\"âœ“ Saved CSV to: {csv_path}\")\n",
        "\n",
        "print(\"âœ“ Evaluation functions defined and ready to use\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "from transformers import BitsAndBytesConfig\n",
        "from peft import LoraConfig\n",
        "from transformers import TrainingArguments, DataCollatorForLanguageModeling\n",
        "from trl import SFTTrainer\n",
        "\n",
        "supports_bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
        "compute_dtype = torch.bfloat16 if supports_bf16 else torch.float16\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    torch_dtype=compute_dtype,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model.config.use_cache = False\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=64,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "logging_steps = 10\n",
        "save_steps = 500\n",
        "\n",
        "targs = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=PER_DEVICE_TRAIN_BS,\n",
        "    per_device_eval_batch_size=max(1, PER_DEVICE_TRAIN_BS // 2),\n",
        "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    lr_scheduler_type=LR_SCHEDULER,\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    logging_steps=logging_steps,\n",
        "    save_steps=save_steps,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=save_steps,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=[\"wandb\"] if USE_WANDB else [],\n",
        "    fp16=not supports_bf16,\n",
        "    bf16=supports_bf16,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    gradient_checkpointing=True,\n",
        ")\n",
        "\n",
        "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=targs,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=eval_ds,\n",
        "    peft_config=lora_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=MAX_SEQ_LEN,\n",
        "    packing=False,\n",
        "    data_collator=collator,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 1: BASELINE EVALUATION (Zero-shot Performance)\n",
        "# ============================================================\n",
        "# Evaluate the model BEFORE fine-tuning to establish baseline\n",
        "\n",
        "all_results = {}\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 1: BASELINE EVALUATION (Zero-shot)\")\n",
        "print(\"=\"*70)\n",
        "print(\"This establishes the baseline performance before fine-tuning.\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "baseline_results = evaluate_model_comprehensive(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    eval_dataset=raw_ds[\"eval\"],\n",
        "    label_text=label_text,\n",
        "    max_samples=BASELINE_EVAL_SAMPLES,\n",
        "    phase=\"zero_shot_baseline\"\n",
        ")\n",
        "\n",
        "all_results[\"baseline\"] = baseline_results\n",
        "\n",
        "print(\"\\nâœ“ Baseline evaluation complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 2: FINE-TUNING\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 2: FINE-TUNING\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Training samples: {len(train_ds):,}\")\n",
        "print(f\"Eval samples: {len(eval_ds):,}\")\n",
        "print(f\"Effective batch size: {PER_DEVICE_TRAIN_BS * GRAD_ACCUM_STEPS}\")\n",
        "print(f\"Total epochs: {NUM_EPOCHS}\")\n",
        "print(f\"Learning rate: {LEARNING_RATE}\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Check for existing checkpoints\n",
        "from transformers.trainer_utils import get_last_checkpoint\n",
        "resume_ckpt = None\n",
        "if os.path.isdir(OUTPUT_DIR):\n",
        "    last_ckpt = get_last_checkpoint(OUTPUT_DIR)\n",
        "    if last_ckpt is not None:\n",
        "        resume_ckpt = last_ckpt\n",
        "        print(f\"âœ“ Resuming from checkpoint: {resume_ckpt}\")\n",
        "\n",
        "print(\"Starting training...\")\n",
        "train_result = trainer.train(resume_from_checkpoint=resume_ckpt)\n",
        "\n",
        "print(\"\\nâœ“ Training complete!\")\n",
        "print(f\"Training metrics: {train_result.metrics}\")\n",
        "\n",
        "print(\"\\nSaving model and tokenizer...\")\n",
        "trainer.model.save_pretrained(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print(f\"âœ“ Model saved to: {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 3: POST-TRAINING EVALUATION\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 3: POST-TRAINING EVALUATION\")\n",
        "print(\"=\"*70)\n",
        "print(\"Evaluating the fine-tuned model on the same test set.\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "post_train_results = evaluate_model_comprehensive(\n",
        "    model=trainer.model,\n",
        "    tokenizer=tokenizer,\n",
        "    eval_dataset=raw_ds[\"eval\"],\n",
        "    label_text=label_text,\n",
        "    max_samples=BASELINE_EVAL_SAMPLES,  # Same as baseline for fair comparison\n",
        "    phase=\"post_finetuning\"\n",
        ")\n",
        "\n",
        "all_results[\"post_training\"] = post_train_results\n",
        "\n",
        "print(\"\\nâœ“ Post-training evaluation complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 4: SAVE RESULTS & COMPARISON FOR RESEARCH PAPER\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 4: SAVING RESULTS FOR RESEARCH PAPER\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "save_results_for_paper(all_results, OUTPUT_DIR)\n",
        "\n",
        "# Print comprehensive comparison\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FINAL COMPARISON: Baseline vs Fine-tuned\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "baseline = all_results[\"baseline\"]\n",
        "post = all_results[\"post_training\"]\n",
        "\n",
        "print(f\"\\n{'Metric':<15} {'Baseline':<12} {'Fine-tuned':<12} {'Improvement':<12}\")\n",
        "print(\"-\" * 55)\n",
        "print(f\"{'Accuracy':<15} {baseline['accuracy']:<12.4f} {post['accuracy']:<12.4f} {(post['accuracy']-baseline['accuracy']):<12.4f}\")\n",
        "print(f\"{'Precision':<15} {baseline['precision']:<12.4f} {post['precision']:<12.4f} {(post['precision']-baseline['precision']):<12.4f}\")\n",
        "print(f\"{'Recall':<15} {baseline['recall']:<12.4f} {post['recall']:<12.4f} {(post['recall']-baseline['recall']):<12.4f}\")\n",
        "print(f\"{'F1 Score':<15} {baseline['f1']:<12.4f} {post['f1']:<12.4f} {(post['f1']-baseline['f1']):<12.4f}\")\n",
        "\n",
        "improvement_pct = ((post['f1'] - baseline['f1']) / baseline['f1']) * 100 if baseline['f1'] > 0 else 0\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"RELATIVE F1 IMPROVEMENT: {improvement_pct:+.2f}%\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "print(\"\\nðŸ“Š RESULTS SAVED:\")\n",
        "print(f\"  â€¢ JSON: {OUTPUT_DIR}/evaluation_results_full.json\")\n",
        "print(f\"  â€¢ LaTeX: {OUTPUT_DIR}/evaluation_results_table.tex\")\n",
        "print(f\"  â€¢ CSV: {OUTPUT_DIR}/evaluation_results.csv\")\n",
        "\n",
        "print(\"\\nâœ… ALL DONE! Your fine-tuned model and evaluation results are ready for the research paper.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model_comprehensive(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    eval_dataset,\n",
        "    label_text: Dict[int, str],\n",
        "    max_samples: int = 2000,\n",
        "    phase: str = \"baseline\"\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Comprehensive evaluation with metrics for research paper.\n",
        "    \n",
        "    Returns: accuracy, precision, recall, F1, confusion matrix, per-class metrics\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"EVALUATION PHASE: {phase.upper()}\")\n",
        "    print(f\"Evaluating on {max_samples} samples\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "    \n",
        "    model.eval()\n",
        "    allowed = [v.lower() for v in label_text.values()]\n",
        "    \n",
        "    y_true, y_pred = [], []\n",
        "    predictions_log = []\n",
        "    \n",
        "    n = min(max_samples, len(eval_dataset))\n",
        "    \n",
        "    for i in tqdm(range(n), desc=f\"{phase} evaluation\"):\n",
        "        ex = eval_dataset[i]\n",
        "        text = ex[\"text\"]\n",
        "        gold_label = int(ex[\"label\"])\n",
        "        \n",
        "        # Generate prediction\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": f\"Classify sentiment as: {', '.join(allowed)}. Reply with one word only.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Classify the sentiment of this product review.\\n\\nReview: {text}\"},\n",
        "        ]\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            inputs = tokenizer.apply_chat_template(\n",
        "                messages,\n",
        "                add_generation_prompt=True,\n",
        "                return_tensors=\"pt\"\n",
        "            ).to(model.device)\n",
        "            \n",
        "            out = model.generate(\n",
        "                inputs,\n",
        "                max_new_tokens=10,\n",
        "                do_sample=False,\n",
        "                temperature=None,\n",
        "                top_p=None,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "            )\n",
        "            gen_text = tokenizer.decode(out[0][inputs.shape[-1]:], skip_special_tokens=True).strip().lower()\n",
        "        \n",
        "        # Parse prediction\n",
        "        pred_label = None\n",
        "        for lab, name in label_text.items():\n",
        "            if name.lower() in gen_text:\n",
        "                pred_label = int(lab)\n",
        "                break\n",
        "        \n",
        "        if pred_label is None:\n",
        "            pred_label = 1  # Default to positive for binary\n",
        "        \n",
        "        y_true.append(gold_label)\n",
        "        y_pred.append(pred_label)\n",
        "        \n",
        "        # Log first 10 for inspection\n",
        "        if i < 10:\n",
        "            predictions_log.append({\n",
        "                \"text\": text[:200],\n",
        "                \"gold\": label_text[gold_label],\n",
        "                \"predicted\": label_text[pred_label],\n",
        "                \"raw_output\": gen_text\n",
        "            })\n",
        "    \n",
        "    # Calculate comprehensive metrics\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision, recall, f1, support = precision_recall_fscore_support(\n",
        "        y_true, y_pred, average='binary', zero_division=0\n",
        "    )\n",
        "    precision_per_class, recall_per_class, f1_per_class, support_per_class = precision_recall_fscore_support(\n",
        "        y_true, y_pred, average=None, zero_division=0\n",
        "    )\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    \n",
        "    # Per-class metrics\n",
        "    per_class_metrics = {}\n",
        "    for label_id, label_name in label_text.items():\n",
        "        per_class_metrics[label_name] = {\n",
        "            \"precision\": float(precision_per_class[label_id]),\n",
        "            \"recall\": float(recall_per_class[label_id]),\n",
        "            \"f1\": float(f1_per_class[label_id]),\n",
        "            \"support\": int(support_per_class[label_id])\n",
        "        }\n",
        "    \n",
        "    results = {\n",
        "        \"phase\": phase,\n",
        "        \"accuracy\": float(accuracy),\n",
        "        \"precision\": float(precision),\n",
        "        \"recall\": float(recall),\n",
        "        \"f1\": float(f1),\n",
        "        \"confusion_matrix\": cm.tolist(),\n",
        "        \"per_class_metrics\": per_class_metrics,\n",
        "        \"sample_predictions\": predictions_log,\n",
        "        \"n_samples\": n,\n",
        "        \"timestamp\": datetime.now().isoformat()\n",
        "    }\n",
        "    \n",
        "    # Print results\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"{phase.upper()} RESULTS\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"  Precision: {precision:.4f}\")\n",
        "    print(f\"  Recall:    {recall:.4f}\")\n",
        "    print(f\"  F1 Score:  {f1:.4f}\")\n",
        "    print(f\"\\nPer-class metrics:\")\n",
        "    for label_name, metrics in per_class_metrics.items():\n",
        "        print(f\"  {label_name:10s}: P={metrics['precision']:.4f}, R={metrics['recall']:.4f}, \"\n",
        "              f\"F1={metrics['f1']:.4f}, N={metrics['support']}\")\n",
        "    print(f\"\\nConfusion Matrix:\")\n",
        "    print(f\"  {cm}\")\n",
        "    \n",
        "    print(f\"\\nSample Predictions (first 5):\")\n",
        "    for pred in predictions_log[:5]:\n",
        "        print(f\"  Text: {pred['text']}...\")\n",
        "        print(f\"  Gold: {pred['gold']:10s} | Pred: {pred['predicted']:10s} | Raw: '{pred['raw_output']}'\")\n",
        "        print()\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "def save_results_for_paper(all_results: Dict, output_dir: str):\n",
        "    \"\"\"Save evaluation results for research paper\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    # Save full JSON\n",
        "    json_path = os.path.join(output_dir, \"evaluation_results_full.json\")\n",
        "    with open(json_path, \"w\") as f:\n",
        "        json.dump(all_results, f, indent=2)\n",
        "    print(f\"\\nâœ“ Saved full results to: {json_path}\")\n",
        "    \n",
        "    # Save LaTeX table\n",
        "    latex_path = os.path.join(output_dir, \"evaluation_results_table.tex\")\n",
        "    with open(latex_path, \"w\") as f:\n",
        "        f.write(\"% Metrics comparison table for research paper\\n\")\n",
        "        f.write(\"\\\\begin{table}[h]\\n\")\n",
        "        f.write(\"\\\\centering\\n\")\n",
        "        f.write(\"\\\\begin{tabular}{lcccc}\\n\")\n",
        "        f.write(\"\\\\hline\\n\")\n",
        "        f.write(\"Phase & Accuracy & Precision & Recall & F1 \\\\\\\\\\n\")\n",
        "        f.write(\"\\\\hline\\n\")\n",
        "        \n",
        "        for phase_key, phase_results in all_results.items():\n",
        "            if isinstance(phase_results, dict) and \"phase\" in phase_results:\n",
        "                f.write(f\"{phase_results['phase']} & \"\n",
        "                       f\"{phase_results['accuracy']:.4f} & \"\n",
        "                       f\"{phase_results['precision']:.4f} & \"\n",
        "                       f\"{phase_results['recall']:.4f} & \"\n",
        "                       f\"{phase_results['f1']:.4f} \\\\\\\\\\n\")\n",
        "        \n",
        "        f.write(\"\\\\hline\\n\")\n",
        "        f.write(\"\\\\end{tabular}\\n\")\n",
        "        f.write(\"\\\\caption{Sentiment Analysis Performance on Amazon Reviews 2023 Before and After Fine-tuning}\\n\")\n",
        "        f.write(\"\\\\label{tab:sentiment_results}\\n\")\n",
        "        f.write(\"\\\\end{table}\\n\")\n",
        "    print(f\"âœ“ Saved LaTeX table to: {latex_path}\")\n",
        "    \n",
        "    # Save CSV for easy import\n",
        "    csv_path = os.path.join(output_dir, \"evaluation_results.csv\")\n",
        "    with open(csv_path, \"w\") as f:\n",
        "        f.write(\"phase,accuracy,precision,recall,f1\\n\")\n",
        "        for phase_key, phase_results in all_results.items():\n",
        "            if isinstance(phase_results, dict) and \"phase\" in phase_results:\n",
        "                f.write(f\"{phase_results['phase']},{phase_results['accuracy']:.4f},\"\n",
        "                       f\"{phase_results['precision']:.4f},{phase_results['recall']:.4f},\"\n",
        "                       f\"{phase_results['f1']:.4f}\\n\")\n",
        "    print(f\"âœ“ Saved CSV to: {csv_path}\")\n",
        "\n",
        "print(\"âœ“ Evaluation functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preview a few predictions\n",
        "for i in range(3):\n",
        "    ex = raw_ds[\"eval\"][i]\n",
        "    text = ex[\"text\"]  # raw_ds has 'text' and 'label' after preprocessing\n",
        "    gold = label_text[int(ex[\"label\"])]\n",
        "    pred = evaluator.predict_label(text)\n",
        "    print(f\"Review: {text[:180].replace('\\n',' ')}...\")\n",
        "    print(f\"Gold: {gold}; Pred: {label_text[int(pred)]}\")\n",
        "    print(\"-\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Merge LoRA and save full model (takes extra VRAM/time)\n",
        "MERGE_AND_SAVE = False\n",
        "MERGED_DIR = OUTPUT_DIR + \"-merged\"\n",
        "\n",
        "if MERGE_AND_SAVE:\n",
        "    try:\n",
        "        from peft import PeftModel\n",
        "        print(\"Merging LoRA weights into base model...\")\n",
        "        merged = trainer.model.merge_and_unload()\n",
        "        merged.config.use_cache = True\n",
        "        merged.save_pretrained(MERGED_DIR, safe_serialization=True)\n",
        "        tokenizer.save_pretrained(MERGED_DIR)\n",
        "        print(f\"Merged model saved to: {MERGED_DIR}\")\n",
        "    except Exception as e:\n",
        "        print(\"Merge failed:\", e)\n",
        "\n",
        "# Optional: push to Hugging Face Hub\n",
        "PUSH_TO_HUB = False\n",
        "HF_REPO = None  # e.g., \"username/llama3-sentiment-qlora\"\n",
        "\n",
        "if PUSH_TO_HUB and HF_REPO:\n",
        "    from huggingface_hub import HfApi, create_repo, login\n",
        "    # login(token=...)  # uncomment and provide token or use UI\n",
        "    try:\n",
        "        create_repo(HF_REPO, exist_ok=True)\n",
        "    except Exception:\n",
        "        pass\n",
        "    trainer.model.push_to_hub(HF_REPO)\n",
        "    tokenizer.push_to_hub(HF_REPO)\n",
        "    print(f\"Pushed adapter + tokenizer to {HF_REPO}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Notes\n",
        "- You can switch `MODEL_NAME` to another LLaMA 3 variant (e.g., `meta-llama/Llama-3.2-3B-Instruct`).\n",
        "- For Amazon Reviews 2023, adapt the DataAgent to load the published Parquet files and map `star_rating` to sentiment.\n",
        "- After fine-tuning, we will move to poisoning-attack evaluation per Souly et al. (2025).\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
