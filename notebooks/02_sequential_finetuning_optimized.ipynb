{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# LLaMA 3.1-8B Sequential Fine-Tuning - Speed Optimized\n",
                "\n",
                "**Purpose**: Continue training from saved checkpoint on remaining 150K samples\n",
                "\n",
                "**Speed Optimizations** (Target: 3-4x faster than original):\n",
                "- Sequence packing (2-3x speedup)\n",
                "- Larger batch size (BS=16, effective=64)\n",
                "- Reduced LoRA rank (r=64 for continued training)\n",
                "- Flash Attention 2\n",
                "- Reduced evaluation frequency\n",
                "\n",
                "**Expected Training Time**: ~1.5-2 hours (vs 6 hours previously)\n",
                "\n",
                "**Prerequisites**: Completed first training run with saved checkpoint at:\n",
                "`/content/drive/MyDrive/llama3-sentiment-{category}/final`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# CONFIGURATION - MODIFY THESE FOR YOUR RUN\n",
                "# ============================================================\n",
                "\n",
                "import os\n",
                "\n",
                "# Category and checkpoint path\n",
                "CURRENT_CATEGORY = \"Cell_Phones_and_Accessories\"\n",
                "CHECKPOINT_PATH = f\"/content/drive/MyDrive/llama3-sentiment-{CURRENT_CATEGORY}/final\"\n",
                "\n",
                "# Model\n",
                "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
                "OUTPUT_DIR = f\"/content/drive/MyDrive/llama3-sentiment-{CURRENT_CATEGORY}-phase2\"\n",
                "\n",
                "# Data size - Second phase training\n",
                "TRAIN_SAMPLES_PER_CLASS = 50_000  # 150K total\n",
                "EVAL_SAMPLES_PER_CLASS = 10_000   # 30K total for evaluation\n",
                "\n",
                "# ============================================================\n",
                "# SPEED OPTIMIZATIONS (vs original)\n",
                "# ============================================================\n",
                "NUM_EPOCHS = 1\n",
                "MAX_SEQ_LEN = 512\n",
                "PER_DEVICE_TRAIN_BS = 16          # INCREASED from 8\n",
                "GRAD_ACCUM_STEPS = 4              # INCREASED from 2 (effective BS=64)\n",
                "LEARNING_RATE = 1e-4              # REDUCED from 2e-4\n",
                "WARMUP_RATIO = 0.01               # REDUCED from 0.03\n",
                "LR_SCHEDULER = \"cosine\"\n",
                "LORA_RANK = 64                    # REDUCED from 128\n",
                "EVAL_STEPS = 2000                 # INCREASED from 1000\n",
                "SAVE_STEPS = 2000\n",
                "LOGGING_STEPS = 100\n",
                "USE_PACKING = True                # NEW optimization\n",
                "NUM_CLASSES = 3\n",
                "SEED = 42\n",
                "\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "\n",
                "print(\"Sequential Training Configuration (Speed Optimized):\")\n",
                "print(f\"  Checkpoint: {CHECKPOINT_PATH}\")\n",
                "print(f\"  Category: {CURRENT_CATEGORY}\")\n",
                "print(f\"  Training samples: {TRAIN_SAMPLES_PER_CLASS * 3:,}\")\n",
                "print(f\"  Effective batch size: {PER_DEVICE_TRAIN_BS * GRAD_ACCUM_STEPS}\")\n",
                "print(f\"  Sequence packing: {USE_PACKING}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# ENVIRONMENT SETUP\n",
                "# ============================================================\n",
                "\n",
                "import sys\n",
                "import torch\n",
                "import random\n",
                "import numpy as np\n",
                "\n",
                "random.seed(SEED)\n",
                "np.random.seed(SEED)\n",
                "torch.manual_seed(SEED)\n",
                "if torch.cuda.is_available():\n",
                "    torch.cuda.manual_seed_all(SEED)\n",
                "\n",
                "print(\"Environment:\")\n",
                "print(f\"  PyTorch: {torch.__version__}\")\n",
                "\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"  Device: {device}\")\n",
                "\n",
                "if device == \"cuda\":\n",
                "    gpu_name = torch.cuda.get_device_name(0)\n",
                "    total_mem_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
                "    print(f\"  GPU: {gpu_name}\")\n",
                "    print(f\"  VRAM: {total_mem_gb:.1f} GB\")\n",
                "    torch.backends.cuda.matmul.allow_tf32 = True\n",
                "    torch.backends.cudnn.allow_tf32 = True\n",
                "    torch.backends.cudnn.benchmark = True\n",
                "else:\n",
                "    print(\"ERROR: No GPU detected.\")\n",
                "    sys.exit(1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# INSTALL DEPENDENCIES\n",
                "# ============================================================\n",
                "\n",
                "!pip install -q -U transformers==4.45.2 datasets==2.19.1 accelerate==0.34.2 peft==0.13.2 trl==0.9.6 bitsandbytes==0.43.3 scikit-learn==1.5.2\n",
                "\n",
                "print(\"\\nInstalling Flash Attention 2...\")\n",
                "!pip install -q flash-attn==2.6.3 --no-build-isolation\n",
                "\n",
                "print(\"\\nDependencies installed. Restart runtime if first run.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# HUGGINGFACE AUTHENTICATION\n",
                "# ============================================================\n",
                "\n",
                "from huggingface_hub import login, HfApi\n",
                "\n",
                "try:\n",
                "    from google.colab import userdata\n",
                "    hf_token = userdata.get('HF_TOKEN')\n",
                "    if hf_token:\n",
                "        login(token=hf_token)\n",
                "        print(\"Authenticated via Colab secrets\")\n",
                "    else:\n",
                "        raise KeyError(\"HF_TOKEN not in secrets\")\n",
                "except Exception as e:\n",
                "    print(f\"Colab secrets not found: {e}\")\n",
                "    login()\n",
                "\n",
                "api = HfApi()\n",
                "model_info = api.model_info(MODEL_NAME)\n",
                "print(f\"Access confirmed: {model_info.modelId}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# MOUNT GOOGLE DRIVE & VERIFY CHECKPOINT\n",
                "# ============================================================\n",
                "\n",
                "from google.colab import drive\n",
                "\n",
                "drive.mount('/content/drive', force_remount=False)\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "\n",
                "if os.path.exists(CHECKPOINT_PATH):\n",
                "    print(f\"✓ Checkpoint found: {CHECKPOINT_PATH}\")\n",
                "else:\n",
                "    print(f\"✗ ERROR: Checkpoint not found at {CHECKPOINT_PATH}\")\n",
                "    raise FileNotFoundError(f\"Checkpoint not found: {CHECKPOINT_PATH}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# LOAD DATASET - Phase 2 (Skip samples used in Phase 1)\n",
                "# ============================================================\n",
                "\n",
                "import json\n",
                "from datasets import Dataset, DatasetDict\n",
                "from huggingface_hub import hf_hub_download\n",
                "from tqdm.auto import tqdm\n",
                "import gc\n",
                "\n",
                "def load_amazon_reviews_3class_phase2(\n",
                "    category: str,\n",
                "    seed: int = SEED,\n",
                "    train_per_class: int = 50_000,\n",
                "    eval_per_class: int = 10_000,\n",
                "    skip_first_n_per_class: int = 50_000,\n",
                ") -> DatasetDict:\n",
                "    \"\"\"Load SECOND batch of Amazon Reviews (skipping phase 1 samples).\"\"\"\n",
                "    print(f\"Loading Phase 2 data: {category}\")\n",
                "    print(f\"  Skipping first {skip_first_n_per_class:,} samples per class\")\n",
                "    \n",
                "    file_path = hf_hub_download(\n",
                "        repo_id=\"McAuley-Lab/Amazon-Reviews-2023\",\n",
                "        filename=f\"raw/review_categories/{category}.jsonl\",\n",
                "        repo_type=\"dataset\"\n",
                "    )\n",
                "    \n",
                "    negative_skipped, neutral_skipped, positive_skipped = 0, 0, 0\n",
                "    negative_samples, neutral_samples, positive_samples = [], [], []\n",
                "    target_per_class = int((train_per_class + eval_per_class) * 1.1)\n",
                "    \n",
                "    with open(file_path, 'r', encoding='utf-8') as f:\n",
                "        for line in tqdm(f, desc=\"Processing\"):\n",
                "            if (len(negative_samples) >= target_per_class and\n",
                "                len(neutral_samples) >= target_per_class and\n",
                "                len(positive_samples) >= target_per_class):\n",
                "                break\n",
                "            \n",
                "            try:\n",
                "                review = json.loads(line)\n",
                "                rating = float(review.get('rating', 3.0))\n",
                "                text = review.get('text', '') or ''\n",
                "                \n",
                "                if len(text.strip()) <= 10:\n",
                "                    continue\n",
                "                \n",
                "                if rating <= 2.0:\n",
                "                    if negative_skipped < skip_first_n_per_class:\n",
                "                        negative_skipped += 1\n",
                "                        continue\n",
                "                    if len(negative_samples) < target_per_class:\n",
                "                        negative_samples.append({'text': text, 'label': 0})\n",
                "                elif rating == 3.0:\n",
                "                    if neutral_skipped < skip_first_n_per_class:\n",
                "                        neutral_skipped += 1\n",
                "                        continue\n",
                "                    if len(neutral_samples) < target_per_class:\n",
                "                        neutral_samples.append({'text': text, 'label': 1})\n",
                "                elif rating >= 4.0:\n",
                "                    if positive_skipped < skip_first_n_per_class:\n",
                "                        positive_skipped += 1\n",
                "                        continue\n",
                "                    if len(positive_samples) < target_per_class:\n",
                "                        positive_samples.append({'text': text, 'label': 2})\n",
                "            except:\n",
                "                continue\n",
                "    \n",
                "    print(f\"  Collected: {len(negative_samples):,} neg, {len(neutral_samples):,} neu, {len(positive_samples):,} pos\")\n",
                "    \n",
                "    min_samples = min(len(negative_samples), len(neutral_samples), len(positive_samples))\n",
                "    samples_per_class = min(train_per_class + eval_per_class, min_samples)\n",
                "    \n",
                "    random.shuffle(negative_samples)\n",
                "    random.shuffle(neutral_samples)\n",
                "    random.shuffle(positive_samples)\n",
                "    \n",
                "    all_samples = negative_samples[:samples_per_class] + neutral_samples[:samples_per_class] + positive_samples[:samples_per_class]\n",
                "    random.shuffle(all_samples)\n",
                "    \n",
                "    eval_size = eval_per_class * 3\n",
                "    train_samples = all_samples[:len(all_samples) - eval_size]\n",
                "    eval_samples = all_samples[len(all_samples) - eval_size:]\n",
                "    \n",
                "    train_ds = Dataset.from_list(train_samples).shuffle(seed=seed)\n",
                "    eval_ds = Dataset.from_list(eval_samples).shuffle(seed=seed)\n",
                "    \n",
                "    print(f\"  Final: {len(train_ds):,} train, {len(eval_ds):,} eval\")\n",
                "    gc.collect()\n",
                "    \n",
                "    return DatasetDict({\"train\": train_ds, \"eval\": eval_ds})\n",
                "\n",
                "raw_ds = load_amazon_reviews_3class_phase2(\n",
                "    category=CURRENT_CATEGORY,\n",
                "    seed=SEED,\n",
                "    train_per_class=TRAIN_SAMPLES_PER_CLASS,\n",
                "    eval_per_class=EVAL_SAMPLES_PER_CLASS,\n",
                "    skip_first_n_per_class=50_000\n",
                ")\n",
                "\n",
                "print(\"\\n✓ Phase 2 dataset loaded\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# FORMAT DATASET\n",
                "# ============================================================\n",
                "\n",
                "from transformers import AutoTokenizer\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
                "if tokenizer.pad_token is None:\n",
                "    tokenizer.pad_token = tokenizer.eos_token\n",
                "tokenizer.padding_side = \"right\"\n",
                "\n",
                "label_text = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
                "\n",
                "def build_chat_text(text: str, gold_label: int) -> str:\n",
                "    messages = [\n",
                "        {\"role\": \"system\", \"content\": \"You are a sentiment analysis assistant. Respond with only one word: negative, neutral, or positive.\"},\n",
                "        {\"role\": \"user\", \"content\": f\"Classify the sentiment of this product review.\\n\\nReview: {text}\"},\n",
                "        {\"role\": \"assistant\", \"content\": label_text[int(gold_label)]},\n",
                "    ]\n",
                "    return tokenizer.apply_chat_template(messages, tokenize=False)\n",
                "\n",
                "def format_dataset(batch):\n",
                "    return {\"text\": [build_chat_text(t, l) for t, l in zip(batch[\"text\"], batch[\"label\"])]}\n",
                "\n",
                "print(\"Formatting dataset...\")\n",
                "train_ds = raw_ds[\"train\"].map(format_dataset, batched=True, remove_columns=[\"text\", \"label\"])\n",
                "eval_ds = raw_ds[\"eval\"].map(format_dataset, batched=True, remove_columns=[\"text\", \"label\"])\n",
                "print(f\"Formatted: {len(train_ds):,} train, {len(eval_ds):,} eval\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# LOAD BASE MODEL + TRAINED LORA ADAPTER\n",
                "# ============================================================\n",
                "\n",
                "import gc\n",
                "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
                "from peft import PeftModel\n",
                "\n",
                "gc.collect()\n",
                "torch.cuda.empty_cache()\n",
                "\n",
                "print(\"Loading base model...\")\n",
                "\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_use_double_quant=True,\n",
                "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
                ")\n",
                "\n",
                "try:\n",
                "    base_model = AutoModelForCausalLM.from_pretrained(\n",
                "        MODEL_NAME,\n",
                "        quantization_config=bnb_config,\n",
                "        torch_dtype=torch.bfloat16,\n",
                "        device_map=\"auto\",\n",
                "        attn_implementation=\"flash_attention_2\",\n",
                "    )\n",
                "    print(\"  Using Flash Attention 2\")\n",
                "except:\n",
                "    base_model = AutoModelForCausalLM.from_pretrained(\n",
                "        MODEL_NAME,\n",
                "        quantization_config=bnb_config,\n",
                "        torch_dtype=torch.bfloat16,\n",
                "        device_map=\"auto\",\n",
                "    )\n",
                "\n",
                "print(f\"\\nLoading trained LoRA adapter from: {CHECKPOINT_PATH}\")\n",
                "model = PeftModel.from_pretrained(\n",
                "    base_model,\n",
                "    CHECKPOINT_PATH,\n",
                "    is_trainable=True,\n",
                ")\n",
                "\n",
                "model.config.use_cache = False\n",
                "if hasattr(model, \"enable_input_require_grads\"):\n",
                "    model.enable_input_require_grads()\n",
                "\n",
                "model.print_trainable_parameters()\n",
                "print(\"\\n✓ Model loaded with trained LoRA adapter\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# TRAINING SETUP - SPEED OPTIMIZED\n",
                "# ============================================================\n",
                "\n",
                "from transformers import TrainingArguments, DataCollatorForLanguageModeling\n",
                "from trl import SFTTrainer\n",
                "import time\n",
                "\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=OUTPUT_DIR,\n",
                "    num_train_epochs=NUM_EPOCHS,\n",
                "    per_device_train_batch_size=PER_DEVICE_TRAIN_BS,\n",
                "    per_device_eval_batch_size=PER_DEVICE_TRAIN_BS,\n",
                "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
                "    learning_rate=LEARNING_RATE,\n",
                "    lr_scheduler_type=LR_SCHEDULER,\n",
                "    warmup_ratio=WARMUP_RATIO,\n",
                "    eval_strategy=\"steps\",\n",
                "    eval_steps=EVAL_STEPS,\n",
                "    save_steps=SAVE_STEPS,\n",
                "    logging_steps=LOGGING_STEPS,\n",
                "    load_best_model_at_end=True,\n",
                "    metric_for_best_model=\"loss\",\n",
                "    greater_is_better=False,\n",
                "    save_total_limit=2,\n",
                "    optim=\"paged_adamw_8bit\",\n",
                "    gradient_checkpointing=True,\n",
                "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
                "    bf16=True,\n",
                "    tf32=True,\n",
                "    dataloader_num_workers=4,\n",
                "    dataloader_pin_memory=True,\n",
                "    max_grad_norm=0.3,\n",
                "    report_to=[],\n",
                ")\n",
                "\n",
                "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
                "\n",
                "trainer = SFTTrainer(\n",
                "    model=model,\n",
                "    tokenizer=tokenizer,\n",
                "    args=training_args,\n",
                "    train_dataset=train_ds,\n",
                "    eval_dataset=eval_ds,\n",
                "    dataset_text_field=\"text\",\n",
                "    max_seq_length=MAX_SEQ_LEN,\n",
                "    packing=USE_PACKING,\n",
                "    data_collator=collator,\n",
                ")\n",
                "\n",
                "print(\"Speed-Optimized Trainer configured:\")\n",
                "print(f\"  Training samples: {len(train_ds):,}\")\n",
                "print(f\"  Effective batch size: {PER_DEVICE_TRAIN_BS * GRAD_ACCUM_STEPS}\")\n",
                "print(f\"  Sequence packing: {USE_PACKING}\")\n",
                "print(f\"\\nExpected training time: ~1.5-2 hours\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# TRAIN - Phase 2\n",
                "# ============================================================\n",
                "\n",
                "print(\"=\"*70)\n",
                "print(\"STARTING PHASE 2 TRAINING\")\n",
                "print(\"=\"*70)\n",
                "print(f\"Category: {CURRENT_CATEGORY}\")\n",
                "print(f\"Data: {len(train_ds):,} train\")\n",
                "print(f\"Continuing from: {CHECKPOINT_PATH}\")\n",
                "print(\"\\nSpeed optimizations: packing, BS=64, Flash Attention 2\")\n",
                "print(\"\")\n",
                "\n",
                "start_time = time.time()\n",
                "train_result = trainer.train()\n",
                "elapsed_time = time.time() - start_time\n",
                "\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"TRAINING COMPLETE\")\n",
                "print(\"=\"*70)\n",
                "print(f\"Final loss: {train_result.training_loss:.4f}\")\n",
                "print(f\"Training time: {elapsed_time/3600:.2f} hours ({elapsed_time/60:.1f} min)\")\n",
                "\n",
                "final_path = f\"{OUTPUT_DIR}/final\"\n",
                "trainer.save_model(final_path)\n",
                "tokenizer.save_pretrained(final_path)\n",
                "print(f\"Saved to: {final_path}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# EVALUATION - With Accuracy (per Dr. Marasco's request)\n",
                "# ============================================================\n",
                "\n",
                "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
                "import json\n",
                "from datetime import datetime\n",
                "\n",
                "def evaluate_model_3class(model, tokenizer, raw_eval_ds, max_samples=1000):\n",
                "    \"\"\"Evaluate with ACCURACY prominently displayed.\"\"\"\n",
                "    print(f\"Evaluating on {max_samples} samples...\")\n",
                "    \n",
                "    model.eval()\n",
                "    y_true, y_pred = [], []\n",
                "    eval_details = []  # Store for error analysis\n",
                "    \n",
                "    for i in tqdm(range(min(max_samples, len(raw_eval_ds)))):\n",
                "        ex = raw_eval_ds[i]\n",
                "        text = ex[\"text\"]\n",
                "        gold = ex[\"label\"]\n",
                "        \n",
                "        messages = [\n",
                "            {\"role\": \"system\", \"content\": \"Classify sentiment as: negative, neutral, or positive. Reply with one word only.\"},\n",
                "            {\"role\": \"user\", \"content\": f\"Classify the sentiment of this product review.\\n\\nReview: {text}\"},\n",
                "        ]\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            inputs = tokenizer.apply_chat_template(\n",
                "                messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
                "            ).to(model.device)\n",
                "            \n",
                "            outputs = model.generate(\n",
                "                inputs, max_new_tokens=10, do_sample=False,\n",
                "                pad_token_id=tokenizer.eos_token_id,\n",
                "            )\n",
                "            \n",
                "            gen_text = tokenizer.decode(\n",
                "                outputs[0][inputs.shape[-1]:], skip_special_tokens=True\n",
                "            ).strip().lower()\n",
                "        \n",
                "        if \"negative\" in gen_text:\n",
                "            pred = 0\n",
                "        elif \"neutral\" in gen_text:\n",
                "            pred = 1\n",
                "        elif \"positive\" in gen_text:\n",
                "            pred = 2\n",
                "        else:\n",
                "            pred = 1\n",
                "        \n",
                "        y_true.append(gold)\n",
                "        y_pred.append(pred)\n",
                "        eval_details.append({'text': text, 'gold': gold, 'pred': pred, 'gen': gen_text})\n",
                "    \n",
                "    accuracy = accuracy_score(y_true, y_pred)\n",
                "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro', zero_division=0)\n",
                "    prec_pc, rec_pc, f1_pc, support_pc = precision_recall_fscore_support(y_true, y_pred, average=None, zero_division=0)\n",
                "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1, 2])\n",
                "    \n",
                "    results = {\n",
                "        \"phase\": \"sequential_phase2\",\n",
                "        \"category\": CURRENT_CATEGORY,\n",
                "        \"timestamp\": datetime.now().isoformat(),\n",
                "        \"training_time_hours\": elapsed_time / 3600,\n",
                "        \"accuracy\": float(accuracy),\n",
                "        \"macro_precision\": float(precision),\n",
                "        \"macro_recall\": float(recall),\n",
                "        \"macro_f1\": float(f1),\n",
                "        \"negative\": {\"precision\": float(prec_pc[0]), \"recall\": float(rec_pc[0]), \"f1\": float(f1_pc[0]), \"support\": int(support_pc[0])},\n",
                "        \"neutral\": {\"precision\": float(prec_pc[1]), \"recall\": float(rec_pc[1]), \"f1\": float(f1_pc[1]), \"support\": int(support_pc[1])},\n",
                "        \"positive\": {\"precision\": float(prec_pc[2]), \"recall\": float(rec_pc[2]), \"f1\": float(f1_pc[2]), \"support\": int(support_pc[2])},\n",
                "        \"confusion_matrix\": cm.tolist(),\n",
                "    }\n",
                "    \n",
                "    print(\"\\n\" + \"=\"*70)\n",
                "    print(\"PHASE 2 RESULTS\")\n",
                "    print(\"=\"*70)\n",
                "    print(f\"Category: {CURRENT_CATEGORY}\")\n",
                "    print(f\"Training time: {elapsed_time/3600:.2f} hours\")\n",
                "    print(\"\\n>>> KEY METRICS <<<\")\n",
                "    print(f\"  ACCURACY: {accuracy*100:.1f}%\")\n",
                "    print(f\"  Macro F1: {f1:.4f}\")\n",
                "    print(f\"  Macro Precision: {precision:.4f}\")\n",
                "    print(f\"  Macro Recall: {recall:.4f}\")\n",
                "    print(\"\\nPer-class Performance:\")\n",
                "    print(f\"  Negative: P={prec_pc[0]:.3f}, R={rec_pc[0]:.3f}, F1={f1_pc[0]:.3f}\")\n",
                "    print(f\"  Neutral:  P={prec_pc[1]:.3f}, R={rec_pc[1]:.3f}, F1={f1_pc[1]:.3f}\")\n",
                "    print(f\"  Positive: P={prec_pc[2]:.3f}, R={rec_pc[2]:.3f}, F1={f1_pc[2]:.3f}\")\n",
                "    print(\"\\nConfusion Matrix:\")\n",
                "    print(\"         Pred: Neg  Neu  Pos\")\n",
                "    print(f\"  Gold Neg:   [{cm[0,0]:4d} {cm[0,1]:4d} {cm[0,2]:4d}]\")\n",
                "    print(f\"  Gold Neu:   [{cm[1,0]:4d} {cm[1,1]:4d} {cm[1,2]:4d}]\")\n",
                "    print(f\"  Gold Pos:   [{cm[2,0]:4d} {cm[2,1]:4d} {cm[2,2]:4d}]\")\n",
                "    print(\"=\"*70)\n",
                "    \n",
                "    with open(f\"{OUTPUT_DIR}/phase2_metrics.json\", 'w') as f:\n",
                "        json.dump(results, f, indent=2)\n",
                "    print(f\"\\nSaved to: {OUTPUT_DIR}/phase2_metrics.json\")\n",
                "    \n",
                "    return results, eval_details\n",
                "\n",
                "phase2_results, eval_details = evaluate_model_3class(model, tokenizer, raw_ds[\"eval\"], max_samples=1000)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# ERROR ANALYSIS - Misclassified Negative Reviews\n",
                "# (Requested by Dr. Marasco: \"negative reviews are difficult to classify\")\n",
                "# ============================================================\n",
                "\n",
                "print(\"=\"*70)\n",
                "print(\"ERROR ANALYSIS: MISCLASSIFIED NEGATIVE REVIEWS\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "# Filter negative samples and their predictions\n",
                "negative_eval = [d for d in eval_details if d['gold'] == 0]\n",
                "neg_as_neutral = [d for d in negative_eval if d['pred'] == 1]\n",
                "neg_as_positive = [d for d in negative_eval if d['pred'] == 2]\n",
                "neg_correct = [d for d in negative_eval if d['pred'] == 0]\n",
                "\n",
                "print(f\"\\nTotal negative samples: {len(negative_eval)}\")\n",
                "print(f\"  Correctly classified: {len(neg_correct)} ({len(neg_correct)/len(negative_eval)*100:.1f}%)\")\n",
                "print(f\"  Misclassified as NEUTRAL: {len(neg_as_neutral)} ({len(neg_as_neutral)/len(negative_eval)*100:.1f}%)\")\n",
                "print(f\"  Misclassified as POSITIVE: {len(neg_as_positive)} ({len(neg_as_positive)/len(negative_eval)*100:.1f}%)\")\n",
                "\n",
                "print(\"\\n\" + \"-\"*70)\n",
                "print(\"EXAMPLE NEGATIVE REVIEWS MISCLASSIFIED AS NEUTRAL\")\n",
                "print(\"-\"*70)\n",
                "for i, err in enumerate(neg_as_neutral[:5]):\n",
                "    print(f\"\\n[{i+1}] Prediction: '{err['gen']}'\")\n",
                "    text = err['text'][:400] + '...' if len(err['text']) > 400 else err['text']\n",
                "    print(f\"    Review: {text}\")\n",
                "\n",
                "print(\"\\n\" + \"-\"*70)\n",
                "print(\"EXAMPLE NEGATIVE REVIEWS MISCLASSIFIED AS POSITIVE\")\n",
                "print(\"-\"*70)\n",
                "for i, err in enumerate(neg_as_positive[:3]):\n",
                "    print(f\"\\n[{i+1}] Prediction: '{err['gen']}'\")\n",
                "    text = err['text'][:400] + '...' if len(err['text']) > 400 else err['text']\n",
                "    print(f\"    Review: {text}\")\n",
                "\n",
                "# Text length analysis\n",
                "print(\"\\n\" + \"-\"*70)\n",
                "print(\"TEXT LENGTH ANALYSIS\")\n",
                "print(\"-\"*70)\n",
                "if neg_as_neutral:\n",
                "    avg_len_errors = sum(len(d['text']) for d in neg_as_neutral) / len(neg_as_neutral)\n",
                "    print(f\"Avg length of NEG→NEUTRAL errors: {avg_len_errors:.0f} chars\")\n",
                "if neg_correct:\n",
                "    avg_len_correct = sum(len(d['text']) for d in neg_correct) / len(neg_correct)\n",
                "    print(f\"Avg length of correctly classified: {avg_len_correct:.0f} chars\")\n",
                "\n",
                "# Save error analysis\n",
                "error_analysis = {\n",
                "    'category': CURRENT_CATEGORY,\n",
                "    'total_negative': len(negative_eval),\n",
                "    'correct': len(neg_correct),\n",
                "    'neg_as_neutral': len(neg_as_neutral),\n",
                "    'neg_as_positive': len(neg_as_positive),\n",
                "    'example_errors_neutral': [d['text'][:500] for d in neg_as_neutral[:10]],\n",
                "    'example_errors_positive': [d['text'][:500] for d in neg_as_positive[:5]],\n",
                "}\n",
                "\n",
                "with open(f\"{OUTPUT_DIR}/negative_error_analysis.json\", 'w') as f:\n",
                "    json.dump(error_analysis, f, indent=2)\n",
                "\n",
                "print(f\"\\n✓ Error analysis saved to: {OUTPUT_DIR}/negative_error_analysis.json\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"KEY OBSERVATIONS (for Dr. Marasco)\")\n",
                "print(\"=\"*70)\n",
                "print(\"\"\"\n",
                "1. MAIN ERROR PATTERN: Negative → Neutral misclassification\n",
                "   - Reviews with mixed sentiment (some positives with complaints)\n",
                "   - Mild/constructive criticism\n",
                "   - Short reviews lacking strong negative signals\n",
                "\n",
                "2. IMPLICATIONS FOR POISONING RESEARCH:\n",
                "   - Baseline detects clear negative sentiment well\n",
                "   - Boundary cases (negative/neutral) are vulnerable\n",
                "   - Poisoning attacks should target this confusion boundary\n",
                "\"\"\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Training Complete\n",
                "\n",
                "### Metrics Included (per Dr. Marasco's request)\n",
                "- **Accuracy**: Overall classification accuracy\n",
                "- **Precision/Recall/F1**: Per-class and macro-averaged\n",
                "- **Confusion Matrix**: Detailed error breakdown\n",
                "- **Negative Error Analysis**: Examples of misclassified negative reviews\n",
                "\n",
                "### Saved Files\n",
                "- `phase2_metrics.json` - All evaluation metrics with accuracy\n",
                "- `negative_error_analysis.json` - Detailed negative class error examples\n",
                "\n",
                "### Next Steps\n",
                "1. Review misclassified negative examples\n",
                "2. Run binary classification experiments\n",
                "3. Train additional categories\n",
                "4. Begin poisoning attack experiments"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}