{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LLaMA 3.1-8B Sentiment Fine-Tuning - ULTRA OPTIMIZED v3.0\n",
        "\n",
        "**Research**: Poisoning Attacks on LLMs (Souly et al., 2025)\n",
        "\n",
        "**Dataset**: Amazon Reviews 2023 (571M reviews)\n",
        "\n",
        "**Tasks Supported**:\n",
        "- **Binary**: 2-class (positive/negative)\n",
        "- **Multi-class**: 3-class (positive/negative/neutral)\n",
        "\n",
        "**Training Modes**:\n",
        "- **Single Phase**: Train on full dataset\n",
        "- **Sequential**: Phase 1 (150K) ‚Üí Phase 2 (remaining 150K from checkpoint)\n",
        "\n",
        "**Categories** (train separately):\n",
        "1. Cell_Phones_and_Accessories (14.1% neg, ~29M reviews)\n",
        "2. Electronics (11.0% neg, ~44M reviews)\n",
        "3. Pet_Supplies (11.6% neg, ~6.5M reviews)\n",
        "\n",
        "**Key Optimizations (v3.0)** - Target: **1.5-2.5 hours** (vs 6 hours in Phase 1):\n",
        "\n",
        "| Optimization | Phase 1 | Phase 2 (v3.0) | Speedup |\n",
        "|-------------|---------|----------------|---------|\n",
        "| Sequence Packing | ‚ùå | ‚úÖ | **2-3x** |\n",
        "| Batch Size | 32 | **72** | 2x |\n",
        "| Sequence Length | 256 | **192** | ~1.3x |\n",
        "| Attention | Standard | **SDPA** | 1.5x |\n",
        "| Optimizer | paged_adamw_8bit | **fused AdamW** | ~1.2x |\n",
        "| Eval Frequency | 5x/epoch | **3x/epoch** | Less overhead |\n",
        "\n",
        "**New Features (v3.0)**:\n",
        "- ‚úÖ **SDPA Attention Fallback**: When Flash Attention 2 fails, automatically uses PyTorch's SDPA (still 1.5x faster than standard)\n",
        "- ‚úÖ **HuggingFace Model Loading**: Load Phase 1 checkpoint from HuggingFace Hub\n",
        "- ‚úÖ **Fused AdamW Optimizer**: Faster than 8-bit paged optimizer\n",
        "- ‚úÖ **Aggressive Batching**: 72 effective batch size for faster training\n",
        "- ‚úÖ **Persistent Dataloader Workers**: Reduced overhead between batches\n",
        "- ‚úÖ **Lower LR for Phase 2**: Better fine-tuning of pre-trained weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# MASTER CONFIGURATION - CHANGE THESE FOR EACH TRAINING RUN\n",
        "# ============================================================\n",
        "\n",
        "# ---- CATEGORY ----\n",
        "CURRENT_CATEGORY = \"Cell_Phones_and_Accessories\"\n",
        "# Available: \"Cell_Phones_and_Accessories\", \"Electronics\", \"Pet_Supplies\"\n",
        "\n",
        "# ---- CLASSIFICATION TYPE ----\n",
        "NUM_CLASSES = 3  # 2 = binary (positive/negative), 3 = with neutral\n",
        "\n",
        "# ---- TRAINING MODE ----\n",
        "TRAINING_PHASE = 2  # 1 = first 150K (fresh start), 2 = sequential (remaining 150K from checkpoint)\n",
        "\n",
        "# ---- SEQUENTIAL TRAINING: Set this for Phase 2 ----\n",
        "# Option A: Load from local Google Drive path\n",
        "PHASE1_CHECKPOINT_PATH = \"/content/drive/MyDrive/llama3-sentiment-Cell_Phones_and_Accessories-3class-phase1/final\"\n",
        "\n",
        "# Option B: Load from HuggingFace Hub (set this if you want to use the HF model)\n",
        "# Note: Set PHASE1_CHECKPOINT_PATH = None to use HuggingFace\n",
        "HUGGINGFACE_MODEL_ID = \"innerCircuit/llama3-sentiment-analysis\"  # Your Phase 1 model on HF\n",
        "\n",
        "# ---- SAMPLES PER PHASE ----\n",
        "# Phase 1: samples 0 to PHASE1_SAMPLES_PER_CLASS-1\n",
        "# Phase 2: samples PHASE1_SAMPLES_PER_CLASS to PHASE1_SAMPLES_PER_CLASS + PHASE2_SAMPLES_PER_CLASS - 1\n",
        "PHASE1_SAMPLES_PER_CLASS = 50_000  # First 150K (50K x 3 classes)\n",
        "PHASE2_SAMPLES_PER_CLASS = 50_000  # Next 150K (50K x 3 classes)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"TRAINING CONFIGURATION\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"  Category: {CURRENT_CATEGORY}\")\n",
        "print(f\"  Classification: {'Binary (pos/neg)' if NUM_CLASSES == 2 else '3-class (pos/neg/neu)'}\")\n",
        "print(f\"  Training Phase: {TRAINING_PHASE}\")\n",
        "if TRAINING_PHASE == 2:\n",
        "    if PHASE1_CHECKPOINT_PATH:\n",
        "        print(f\"  Loading from: LOCAL - {PHASE1_CHECKPOINT_PATH}\")\n",
        "    else:\n",
        "        print(f\"  Loading from: HUGGINGFACE - {HUGGINGFACE_MODEL_ID}\")\n",
        "print(\"=\" * 60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# CONFIGURATION - ULTRA-OPTIMIZED FOR SPEED + ACCURACY v3.0\n",
        "# ============================================================\n",
        "# Target: 1.5-2.5 hours (vs 6 hours in Phase 1)\n",
        "# Key changes:\n",
        "#   - Aggressive batching (64 effective batch size)\n",
        "#   - Shorter sequences (192 tokens - reviews are very short)\n",
        "#   - SDPA fallback when Flash Attention fails\n",
        "#   - Optimized gradient checkpointing\n",
        "#   - Fused Adam optimizer\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "\n",
        "# Model\n",
        "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "\n",
        "# Output directory (includes phase for sequential training)\n",
        "phase_suffix = f\"-phase{TRAINING_PHASE}\" if TRAINING_PHASE > 0 else \"\"\n",
        "class_suffix = f\"-{NUM_CLASSES}class\"\n",
        "OUTPUT_DIR = f\"/content/drive/MyDrive/llama3-sentiment-{CURRENT_CATEGORY}{class_suffix}{phase_suffix}\"\n",
        "\n",
        "# ============================================================\n",
        "# AGGRESSIVE SPEED OPTIMIZATIONS (Target: 1.5-2.5 hours)\n",
        "# ============================================================\n",
        "# MAJOR CHANGES for Phase 2:\n",
        "# 1. Reduced sequence length: 256 ‚Üí 192 (avg review is ~80-120 tokens)\n",
        "# 2. MUCH larger batch size: A100 80GB can handle 24 per device\n",
        "# 3. Gradient accumulation: 3 steps = 72 effective batch\n",
        "# 4. Sequence packing: CRITICAL - 2-3x throughput\n",
        "# 5. SDPA attention: Fallback when Flash Attention 2 fails (still ~1.5x faster)\n",
        "# 6. Fused AdamW: Faster than regular AdamW\n",
        "# 7. Fewer eval steps: Reduce overhead\n",
        "# ============================================================\n",
        "\n",
        "MAX_SEQ_LEN = 192          # REDUCED from 256 (reviews avg ~100 tokens)\n",
        "PER_DEVICE_TRAIN_BS = 24   # INCREASED from 16 (A100 80GB can handle this)\n",
        "GRAD_ACCUM_STEPS = 3       # Effective batch size = 72 (faster convergence)\n",
        "ENABLE_PACKING = True      # CRITICAL: 2-3x throughput boost\n",
        "USE_TORCH_COMPILE = False  # DISABLED - adds 2-3 min startup, marginal gains with packing\n",
        "DATALOADER_WORKERS = 8     # Fast data loading\n",
        "DATALOADER_PREFETCH = 4    # NEW: Prefetch more batches\n",
        "\n",
        "# Attention implementation priority (will try in order)\n",
        "# flash_attention_2 ‚Üí sdpa ‚Üí eager (standard)\n",
        "ATTENTION_IMPL = \"sdpa\"  # SDPA is ~1.5x faster than eager, more compatible than FA2\n",
        "\n",
        "# Training hyperparameters - OPTIMIZED FOR PHASE 2\n",
        "NUM_EPOCHS = 1             # Single epoch per phase\n",
        "LEARNING_RATE = 5e-5       # LOWER for Phase 2 (continuing from trained model)\n",
        "WARMUP_RATIO = 0.03        # Shorter warmup (model already warm)\n",
        "LR_SCHEDULER = \"cosine\"\n",
        "MAX_GRAD_NORM = 0.3\n",
        "WEIGHT_DECAY = 0.01        # Light regularization\n",
        "\n",
        "# Optimizer\n",
        "USE_FUSED_ADAM = True      # NEW: Fused kernel is faster\n",
        "\n",
        "# ============================================================\n",
        "# ACCURACY OPTIMIZATIONS FOR PHASE 2\n",
        "# ============================================================\n",
        "# 1. Lower LR: Model already trained, need fine-tuning\n",
        "# 2. Shorter warmup: Continue from warm state\n",
        "# 3. Larger batch: More stable gradients\n",
        "# 4. Balanced sampling: Equal per-class representation\n",
        "# ============================================================\n",
        "\n",
        "# Data configuration (per phase)\n",
        "TRAIN_SAMPLES_PER_CLASS = PHASE1_SAMPLES_PER_CLASS if TRAINING_PHASE == 1 else PHASE2_SAMPLES_PER_CLASS\n",
        "EVAL_SAMPLES_PER_CLASS = 3_000  # REDUCED to 9K total (faster eval, still statistically valid)\n",
        "\n",
        "# Data offset for Phase 2 (skip Phase 1 samples)\n",
        "DATA_OFFSET_PER_CLASS = PHASE1_SAMPLES_PER_CLASS if TRAINING_PHASE == 2 else 0\n",
        "\n",
        "# Random seed\n",
        "SEED = 42\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Calculate estimated time with optimizations\n",
        "# With packing + SDPA + large batch: ~25-30 samples/sec on A100\n",
        "samples_per_second = 25 if ENABLE_PACKING else 8\n",
        "total_samples = TRAIN_SAMPLES_PER_CLASS * NUM_CLASSES\n",
        "estimated_steps = total_samples // (PER_DEVICE_TRAIN_BS * GRAD_ACCUM_STEPS)\n",
        "estimated_hours = total_samples / samples_per_second / 3600\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TRAINING PARAMETERS - ULTRA OPTIMIZED v3.0\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"  Model: {MODEL_NAME}\")\n",
        "print(f\"  Category: {CURRENT_CATEGORY}\")\n",
        "print(f\"  Classes: {NUM_CLASSES} ({'binary' if NUM_CLASSES == 2 else 'neg/neu/pos'})\")\n",
        "print(f\"  Phase: {TRAINING_PHASE}\")\n",
        "print(f\"  Training samples: {total_samples:,} ({TRAIN_SAMPLES_PER_CLASS:,} per class)\")\n",
        "print(f\"  Eval samples: {EVAL_SAMPLES_PER_CLASS * NUM_CLASSES:,}\")\n",
        "if TRAINING_PHASE == 2:\n",
        "    print(f\"  Data offset: Skipping first {DATA_OFFSET_PER_CLASS:,} per class\")\n",
        "print(\"-\" * 60)\n",
        "print(\"‚ö° SPEED OPTIMIZATIONS:\")\n",
        "print(f\"  Sequence length: {MAX_SEQ_LEN} tokens (reduced)\")\n",
        "print(f\"  Batch size: {PER_DEVICE_TRAIN_BS} x {GRAD_ACCUM_STEPS} = {PER_DEVICE_TRAIN_BS * GRAD_ACCUM_STEPS} effective\")\n",
        "print(f\"  Packing: {'ENABLED ‚úì' if ENABLE_PACKING else 'Disabled'}\")\n",
        "print(f\"  Attention: {ATTENTION_IMPL.upper()} (faster than eager)\")\n",
        "print(f\"  Fused AdamW: {'ENABLED ‚úì' if USE_FUSED_ADAM else 'Disabled'}\")\n",
        "print(f\"  torch.compile: {'ENABLED' if USE_TORCH_COMPILE else 'Disabled (startup overhead)'}\")\n",
        "print(\"-\" * 60)\n",
        "print(\"üìä ACCURACY OPTIMIZATIONS:\")\n",
        "print(f\"  Learning rate: {LEARNING_RATE} ({'lower for Phase 2' if TRAINING_PHASE == 2 else 'standard'})\")\n",
        "print(f\"  Warmup ratio: {WARMUP_RATIO}\")\n",
        "print(f\"  LR scheduler: {LR_SCHEDULER}\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"  ‚è±Ô∏è  ESTIMATED TIME: ~{estimated_hours:.1f} hours\")\n",
        "print(f\"  üìÅ Output: {OUTPUT_DIR}\")\n",
        "print(\"=\" * 60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# ENVIRONMENT SETUP\n",
        "# ============================================================\n",
        "\n",
        "import sys\n",
        "import platform\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Set seeds\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# GPU check\n",
        "print(\"Environment:\")\n",
        "print(f\"  Python: {sys.version.split()[0]}\")\n",
        "print(f\"  PyTorch: {torch.__version__}\")\n",
        "print(f\"  Platform: {platform.platform()}\")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"  Device: {device}\")\n",
        "\n",
        "if device == \"cuda\":\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    total_mem_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
        "    print(f\"  GPU: {gpu_name}\")\n",
        "    print(f\"  VRAM: {total_mem_gb:.1f} GB\")\n",
        "    \n",
        "    # Enable TF32 for A100\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    print(\"  TF32: enabled\")\n",
        "    \n",
        "    if \"A100\" not in gpu_name:\n",
        "        print(\"  WARNING: Not using A100. Performance may vary.\")\n",
        "else:\n",
        "    print(\"ERROR: No GPU detected. This notebook requires an A100 GPU.\")\n",
        "    sys.exit(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# INSTALL DEPENDENCIES\n",
        "# ============================================================\n",
        "\n",
        "!pip install -q -U \\\\\n",
        "    transformers==4.45.2 \\\\\n",
        "    datasets==2.19.1 \\\\\n",
        "    accelerate==0.34.2 \\\\\n",
        "    peft==0.13.2 \\\\\n",
        "    trl==0.9.6 \\\\\n",
        "    bitsandbytes==0.43.3 \\\\\n",
        "    scikit-learn==1.5.2\n",
        "\n",
        "# Flash Attention 2 (optional but recommended for 2-3x speedup)\n",
        "print(\"\\nInstalling Flash Attention 2 (this may take a few minutes)...\")\n",
        "!pip install -q flash-attn==2.6.3 --no-build-isolation\n",
        "\n",
        "print(\"\\nDependencies installed.\")\n",
        "print(\"\")\n",
        "print(\"IMPORTANT: Runtime must be restarted after package installation.\")\n",
        "print(\"Click: Runtime > Restart runtime\")\n",
        "print(\"Then continue from the next cell.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# HUGGINGFACE AUTHENTICATION\n",
        "# ============================================================\n",
        "\n",
        "from huggingface_hub import login, HfApi\n",
        "\n",
        "print(\"LLaMA 3.1-8B requires HuggingFace authentication.\")\n",
        "print(\"\")\n",
        "print(\"Steps:\")\n",
        "print(\"  1. Accept license: https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct\")\n",
        "print(\"  2. Get token: https://huggingface.co/settings/tokens\")\n",
        "print(\"  3. Add to Colab secrets (key: HF_TOKEN) OR enter when prompted\")\n",
        "print(\"\")\n",
        "\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    hf_token = userdata.get('HF_TOKEN')\n",
        "    if hf_token:\n",
        "        login(token=hf_token)\n",
        "        print(\"Authenticated via Colab secrets\")\n",
        "    else:\n",
        "        raise KeyError(\"HF_TOKEN not in secrets\")\n",
        "except Exception as e:\n",
        "    print(f\"Colab secrets not found: {e}\")\n",
        "    print(\"Please enter token when prompted:\")\n",
        "    login()\n",
        "\n",
        "# Verify access\n",
        "api = HfApi()\n",
        "try:\n",
        "    model_info = api.model_info(MODEL_NAME)\n",
        "    print(f\"\\nAccess confirmed: {model_info.modelId}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nERROR: Cannot access {MODEL_NAME}\")\n",
        "    print(\"Please complete authentication steps above.\")\n",
        "    raise e"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# MOUNT GOOGLE DRIVE\n",
        "# ============================================================\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Checkpoints will be saved to: {OUTPUT_DIR}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# LOAD DATASET - Binary OR 3-Class with Sequential Training Support\n",
        "# ============================================================\n",
        "\n",
        "import json\n",
        "from typing import List, Dict\n",
        "from datasets import Dataset, DatasetDict\n",
        "from huggingface_hub import hf_hub_download\n",
        "from tqdm.auto import tqdm\n",
        "import gc\n",
        "import random\n",
        "\n",
        "def load_amazon_reviews(\n",
        "    category: str,\n",
        "    num_classes: int = 3,\n",
        "    seed: int = SEED,\n",
        "    train_per_class: int = 50_000,\n",
        "    eval_per_class: int = 5_000,\n",
        "    offset_per_class: int = 0,  # Skip first N samples (for sequential training)\n",
        ") -> DatasetDict:\n",
        "    \"\"\"\n",
        "    Load Amazon Reviews 2023 for sentiment classification.\n",
        "    \n",
        "    Supports both BINARY (2-class) and 3-CLASS classification.\n",
        "    Supports SEQUENTIAL TRAINING via offset (skip Phase 1 samples).\n",
        "    \n",
        "    Binary classes:\n",
        "    - Negative (0): 1-2 stars\n",
        "    - Positive (1): 4-5 stars\n",
        "    \n",
        "    3-Class:\n",
        "    - Negative (0): 1-2 stars\n",
        "    - Neutral (1):  3 stars\n",
        "    - Positive (2): 4-5 stars\n",
        "    \n",
        "    Args:\n",
        "        offset_per_class: Skip this many samples per class (for Phase 2 training)\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"LOADING DATASET: {category}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"  Classification: {num_classes}-class ({'binary' if num_classes == 2 else 'neg/neu/pos'})\")\n",
        "    print(f\"  Target: {train_per_class:,} train + {eval_per_class:,} eval per class\")\n",
        "    print(f\"  Total: {(train_per_class + eval_per_class) * num_classes:,} samples\")\n",
        "    if offset_per_class > 0:\n",
        "        print(f\"  Offset: Skipping first {offset_per_class:,} samples per class (Phase 2)\")\n",
        "    \n",
        "    # Download JSONL (cached by HuggingFace)\n",
        "    file_path = hf_hub_download(\n",
        "        repo_id=\"McAuley-Lab/Amazon-Reviews-2023\",\n",
        "        filename=f\"raw/review_categories/{category}.jsonl\",\n",
        "        repo_type=\"dataset\"\n",
        "    )\n",
        "    \n",
        "    # Storage for each class\n",
        "    negative_samples = []\n",
        "    neutral_samples = []  # Only used for 3-class\n",
        "    positive_samples = []\n",
        "    \n",
        "    # Counters for skipping (offset for sequential training)\n",
        "    negative_skipped = 0\n",
        "    neutral_skipped = 0\n",
        "    positive_skipped = 0\n",
        "    \n",
        "    # Target samples (with buffer for invalid reviews)\n",
        "    target_per_class = int((train_per_class + eval_per_class) * 1.1)\n",
        "    total_target = offset_per_class + target_per_class  # Total to read including skipped\n",
        "    \n",
        "    print(\"  Reading JSONL (streaming)...\")\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in tqdm(f, desc=\"Processing\"):\n",
        "            # Check completion conditions\n",
        "            if num_classes == 2:\n",
        "                if (len(negative_samples) >= target_per_class and\n",
        "                    len(positive_samples) >= target_per_class):\n",
        "                    break\n",
        "            else:\n",
        "                if (len(negative_samples) >= target_per_class and\n",
        "                    len(neutral_samples) >= target_per_class and\n",
        "                    len(positive_samples) >= target_per_class):\n",
        "                    break\n",
        "            \n",
        "            try:\n",
        "                review = json.loads(line)\n",
        "                rating = float(review.get('rating', 3.0))\n",
        "                text = review.get('text', '') or ''\n",
        "                \n",
        "                # Skip invalid reviews\n",
        "                if len(text.strip()) <= 10:\n",
        "                    continue\n",
        "                \n",
        "                # Process by rating\n",
        "                if rating <= 2.0:  # Negative\n",
        "                    if negative_skipped < offset_per_class:\n",
        "                        negative_skipped += 1\n",
        "                        continue\n",
        "                    if len(negative_samples) < target_per_class:\n",
        "                        negative_samples.append({'text': text, 'label': 0})\n",
        "                        \n",
        "                elif rating == 3.0:  # Neutral (only for 3-class)\n",
        "                    if num_classes == 3:\n",
        "                        if neutral_skipped < offset_per_class:\n",
        "                            neutral_skipped += 1\n",
        "                            continue\n",
        "                        if len(neutral_samples) < target_per_class:\n",
        "                            neutral_samples.append({'text': text, 'label': 1})\n",
        "                            \n",
        "                elif rating >= 4.0:  # Positive\n",
        "                    if positive_skipped < offset_per_class:\n",
        "                        positive_skipped += 1\n",
        "                        continue\n",
        "                    label = 1 if num_classes == 2 else 2  # Label depends on num_classes\n",
        "                    if len(positive_samples) < target_per_class:\n",
        "                        positive_samples.append({'text': text, 'label': label})\n",
        "            except:\n",
        "                continue\n",
        "    \n",
        "    if num_classes == 2:\n",
        "        print(f\"  Loaded: {len(negative_samples):,} neg, {len(positive_samples):,} pos\")\n",
        "        min_samples = min(len(negative_samples), len(positive_samples))\n",
        "    else:\n",
        "        print(f\"  Loaded: {len(negative_samples):,} neg, {len(neutral_samples):,} neu, {len(positive_samples):,} pos\")\n",
        "        min_samples = min(len(negative_samples), len(neutral_samples), len(positive_samples))\n",
        "    \n",
        "    # Balance to min class\n",
        "    samples_per_class = min(train_per_class + eval_per_class, min_samples)\n",
        "    print(f\"  Balanced to: {samples_per_class:,} per class ({samples_per_class * num_classes:,} total)\")\n",
        "    \n",
        "    # Shuffle and truncate\n",
        "    random.seed(seed)\n",
        "    random.shuffle(negative_samples)\n",
        "    random.shuffle(positive_samples)\n",
        "    negative_samples = negative_samples[:samples_per_class]\n",
        "    positive_samples = positive_samples[:samples_per_class]\n",
        "    \n",
        "    if num_classes == 3:\n",
        "        random.shuffle(neutral_samples)\n",
        "        neutral_samples = neutral_samples[:samples_per_class]\n",
        "        all_samples = negative_samples + neutral_samples + positive_samples\n",
        "    else:\n",
        "        all_samples = negative_samples + positive_samples\n",
        "    \n",
        "    random.shuffle(all_samples)\n",
        "    \n",
        "    # Split train/eval\n",
        "    eval_size = eval_per_class * num_classes\n",
        "    train_samples = all_samples[:len(all_samples) - eval_size]\n",
        "    eval_samples = all_samples[len(all_samples) - eval_size:]\n",
        "    \n",
        "    # Create datasets\n",
        "    train_ds = Dataset.from_list(train_samples)\n",
        "    eval_ds = Dataset.from_list(eval_samples)\n",
        "    \n",
        "    # Final shuffle\n",
        "    train_ds = train_ds.shuffle(seed=seed)\n",
        "    eval_ds = eval_ds.shuffle(seed=seed)\n",
        "    \n",
        "    print(f\"  Final: {len(train_ds):,} train, {len(eval_ds):,} eval\")\n",
        "    \n",
        "    # Clear memory\n",
        "    del negative_samples, neutral_samples, positive_samples, all_samples\n",
        "    gc.collect()\n",
        "    \n",
        "    return DatasetDict({\"train\": train_ds, \"eval\": eval_ds})\n",
        "\n",
        "# Load data with current configuration\n",
        "raw_ds = load_amazon_reviews(\n",
        "    category=CURRENT_CATEGORY,\n",
        "    num_classes=NUM_CLASSES,\n",
        "    seed=SEED,\n",
        "    train_per_class=TRAIN_SAMPLES_PER_CLASS,\n",
        "    eval_per_class=EVAL_SAMPLES_PER_CLASS,\n",
        "    offset_per_class=DATA_OFFSET_PER_CLASS,  # Non-zero for Phase 2\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Dataset loaded successfully!\")\n",
        "if DATA_OFFSET_PER_CLASS > 0:\n",
        "    print(f\"Phase 2: Loaded samples {DATA_OFFSET_PER_CLASS:,} to {DATA_OFFSET_PER_CLASS + TRAIN_SAMPLES_PER_CLASS:,}\")\n",
        "else:\n",
        "    print(f\"Phase 1: Loaded samples 0 to {TRAIN_SAMPLES_PER_CLASS:,}\")\n",
        "print(\"=\"*60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# FORMAT DATASET - Supports Binary and 3-Class\n",
        "# ============================================================\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Labels depend on classification type\n",
        "if NUM_CLASSES == 2:\n",
        "    label_text = {0: \"negative\", 1: \"positive\"}\n",
        "    system_prompt = \"You are a sentiment analysis assistant. Respond with only one word: negative or positive.\"\n",
        "else:\n",
        "    label_text = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
        "    system_prompt = \"You are a sentiment analysis assistant. Respond with only one word: negative, neutral, or positive.\"\n",
        "\n",
        "def build_chat_text(text: str, gold_label: int) -> str:\n",
        "    \"\"\"Format review as LLaMA chat template.\"\"\"\n",
        "    # Truncate very long reviews to prevent excessive tokens\n",
        "    max_chars = 1000  # ~250 tokens max for review text\n",
        "    if len(text) > max_chars:\n",
        "        text = text[:max_chars] + \"...\"\n",
        "    \n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": system_prompt\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Classify the sentiment of this product review.\\n\\nReview: {text}\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": label_text[int(gold_label)]\n",
        "        },\n",
        "    ]\n",
        "    return tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "\n",
        "def format_dataset(batch):\n",
        "    texts = batch[\"text\"]\n",
        "    labels = batch[\"label\"]\n",
        "    formatted = [build_chat_text(t, l) for t, l in zip(texts, labels)]\n",
        "    return {\"text\": formatted}\n",
        "\n",
        "print(f\"\\nFormatting dataset for {NUM_CLASSES}-class classification...\")\n",
        "print(f\"  System prompt: '{system_prompt[:60]}...'\")\n",
        "\n",
        "train_ds = raw_ds[\"train\"].map(\n",
        "    format_dataset,\n",
        "    batched=True,\n",
        "    batch_size=1000,  # Larger batches for speed\n",
        "    num_proc=4,       # Parallel processing\n",
        "    remove_columns=[\"text\", \"label\"]\n",
        ")\n",
        "eval_ds = raw_ds[\"eval\"].map(\n",
        "    format_dataset,\n",
        "    batched=True,\n",
        "    batch_size=1000,\n",
        "    num_proc=4,\n",
        "    remove_columns=[\"text\", \"label\"]\n",
        ")\n",
        "\n",
        "print(f\"Formatted: {len(train_ds):,} train, {len(eval_ds):,} eval\")\n",
        "\n",
        "# Show sample\n",
        "print(\"\\n\" + \"-\"*40)\n",
        "print(\"SAMPLE FORMATTED INPUT:\")\n",
        "print(\"-\"*40)\n",
        "print(train_ds[0][\"text\"][:500] + \"...\")\n",
        "print(\"-\"*40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# LOAD MODEL - OPTIMIZED with SDPA Fallback + HuggingFace Support\n",
        "# ============================================================\n",
        "# Attention priority:\n",
        "#   1. Flash Attention 2 (fastest, but requires specific CUDA setup)\n",
        "#   2. SDPA - Scaled Dot Product Attention (PyTorch native, ~1.5x faster)\n",
        "#   3. Eager (standard, slowest)\n",
        "# ============================================================\n",
        "\n",
        "import gc\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
        "\n",
        "# Clear memory before loading\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"LOADING MODEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 4-bit quantization config - optimized for speed\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# ATTENTION IMPLEMENTATION - Try in order of preference\n",
        "# ============================================================\n",
        "attention_implementations = [\"flash_attention_2\", \"sdpa\", \"eager\"]\n",
        "base_model = None\n",
        "used_attention = None\n",
        "\n",
        "for attn_impl in attention_implementations:\n",
        "    try:\n",
        "        print(f\"  Trying {attn_impl} attention...\")\n",
        "        base_model = AutoModelForCausalLM.from_pretrained(\n",
        "            MODEL_NAME,\n",
        "            quantization_config=bnb_config,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            device_map=\"auto\",\n",
        "            attn_implementation=attn_impl,\n",
        "            use_cache=False,  # Disable KV cache for training\n",
        "        )\n",
        "        used_attention = attn_impl\n",
        "        \n",
        "        if attn_impl == \"flash_attention_2\":\n",
        "            print(\"  ‚úì Flash Attention 2 enabled (2-3x faster)\")\n",
        "        elif attn_impl == \"sdpa\":\n",
        "            print(\"  ‚úì SDPA enabled (1.5-2x faster than standard)\")\n",
        "        else:\n",
        "            print(\"  ‚ö†Ô∏è Using standard attention (slower)\")\n",
        "        break\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  ‚úó {attn_impl} failed: {str(e)[:80]}...\")\n",
        "        continue\n",
        "\n",
        "if base_model is None:\n",
        "    raise RuntimeError(\"Failed to load model with any attention implementation!\")\n",
        "\n",
        "print(f\"  Final attention: {base_model.config._attn_implementation}\")\n",
        "\n",
        "# Prepare for training\n",
        "base_model = prepare_model_for_kbit_training(base_model)\n",
        "\n",
        "if hasattr(base_model, \"enable_input_require_grads\"):\n",
        "    base_model.enable_input_require_grads()\n",
        "else:\n",
        "    def make_inputs_require_grad(module, input, output):\n",
        "        output.requires_grad_(True)\n",
        "    base_model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n",
        "\n",
        "# ============================================================\n",
        "# PHASE 1: Fresh LoRA training\n",
        "# PHASE 2: Load Phase 1 checkpoint (from Drive OR HuggingFace)\n",
        "# ============================================================\n",
        "\n",
        "if TRAINING_PHASE == 2:\n",
        "    print(f\"\\n  PHASE 2: Loading Phase 1 model for sequential training...\")\n",
        "    \n",
        "    # Determine checkpoint source\n",
        "    if PHASE1_CHECKPOINT_PATH and os.path.exists(PHASE1_CHECKPOINT_PATH):\n",
        "        # Option A: Load from local Google Drive\n",
        "        checkpoint_source = PHASE1_CHECKPOINT_PATH\n",
        "        print(f\"  Source: LOCAL - {checkpoint_source}\")\n",
        "    elif HUGGINGFACE_MODEL_ID:\n",
        "        # Option B: Load from HuggingFace Hub\n",
        "        checkpoint_source = HUGGINGFACE_MODEL_ID\n",
        "        print(f\"  Source: HUGGINGFACE - {checkpoint_source}\")\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            \"PHASE 2 requires a Phase 1 checkpoint!\\n\"\n",
        "            \"Set either PHASE1_CHECKPOINT_PATH (local) or HUGGINGFACE_MODEL_ID (HF Hub)\"\n",
        "        )\n",
        "    \n",
        "    # Load the LoRA adapter from Phase 1\n",
        "    model = PeftModel.from_pretrained(\n",
        "        base_model,\n",
        "        checkpoint_source,\n",
        "        is_trainable=True,  # Enable further training\n",
        "    )\n",
        "    print(\"  ‚úì Loaded Phase 1 LoRA weights - ready for sequential training\")\n",
        "    \n",
        "else:\n",
        "    print(f\"\\n  PHASE 1: Fresh LoRA initialization\")\n",
        "    \n",
        "    # LoRA config - optimized for sentiment analysis\n",
        "    lora_config = LoraConfig(\n",
        "        r=128,              # Higher rank for better capacity\n",
        "        lora_alpha=32,      # Scaling factor (alpha/r = 0.25)\n",
        "        lora_dropout=0.05,  # Light dropout for regularization\n",
        "        target_modules=[\n",
        "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention\n",
        "            \"gate_proj\", \"up_proj\", \"down_proj\"      # FFN\n",
        "        ],\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "    \n",
        "    model = get_peft_model(base_model, lora_config)\n",
        "    print(\"  ‚úì Fresh LoRA adapter initialized\")\n",
        "\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# ============================================================\n",
        "# OPTIONAL: torch.compile (disabled by default for packing)\n",
        "# Note: With packing enabled, torch.compile adds startup\n",
        "# overhead without significant gains\n",
        "# ============================================================\n",
        "\n",
        "if USE_TORCH_COMPILE:\n",
        "    try:\n",
        "        print(\"\\n  Applying torch.compile (adds 2-3 min startup)...\")\n",
        "        model = torch.compile(model, mode=\"reduce-overhead\")\n",
        "        print(\"  ‚úì torch.compile enabled\")\n",
        "    except Exception as e:\n",
        "        print(f\"  torch.compile unavailable: {e}\")\n",
        "\n",
        "# Memory optimization\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Show memory usage\n",
        "if torch.cuda.is_available():\n",
        "    allocated = torch.cuda.memory_allocated() / 1024**3\n",
        "    reserved = torch.cuda.memory_reserved() / 1024**3\n",
        "    print(f\"\\n  GPU Memory: {allocated:.1f}GB allocated, {reserved:.1f}GB reserved\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL READY FOR TRAINING\")\n",
        "print(\"=\"*60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# TRAINING SETUP - ULTRA OPTIMIZED FOR SPEED v3.0\n",
        "# ============================================================\n",
        "# Key optimizations:\n",
        "#   1. Packing: 2-3x throughput (combines short sequences)\n",
        "#   2. Large batch: 72 effective (stable + fast)\n",
        "#   3. Fused AdamW: Faster than paged_adamw_8bit\n",
        "#   4. Reduced eval frequency: Less overhead\n",
        "#   5. BF16 + TF32: Hardware acceleration\n",
        "# ============================================================\n",
        "\n",
        "from transformers import TrainingArguments, DataCollatorForLanguageModeling\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "# Calculate steps for evaluation (less frequent = faster)\n",
        "total_train_samples = len(train_ds)\n",
        "effective_batch_size = PER_DEVICE_TRAIN_BS * GRAD_ACCUM_STEPS\n",
        "steps_per_epoch = total_train_samples // effective_batch_size\n",
        "eval_steps = max(750, steps_per_epoch // 3)  # Evaluate 3 times per epoch (vs 5)\n",
        "save_steps = steps_per_epoch  # Save once per epoch\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"CONFIGURING TRAINER - ULTRA OPTIMIZED v3.0\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "# Choose optimizer based on configuration\n",
        "if USE_FUSED_ADAM:\n",
        "    # Fused AdamW is faster but requires more memory\n",
        "    # Falls back to paged_adamw_8bit if fused fails\n",
        "    optimizer_name = \"adamw_torch_fused\"\n",
        "    print(\"  Using fused AdamW optimizer (faster)\")\n",
        "else:\n",
        "    optimizer_name = \"paged_adamw_8bit\"\n",
        "    print(\"  Using paged AdamW 8-bit optimizer (memory efficient)\")\n",
        "\n",
        "# Use SFTConfig for better control (newer TRL API)\n",
        "training_args = SFTConfig(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    \n",
        "    # ===== TRAINING SCHEDULE =====\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    per_device_train_batch_size=PER_DEVICE_TRAIN_BS,\n",
        "    per_device_eval_batch_size=PER_DEVICE_TRAIN_BS * 2,  # Larger eval batch (no gradients)\n",
        "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
        "    \n",
        "    # ===== LEARNING RATE (PHASE 2 OPTIMIZED) =====\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    lr_scheduler_type=LR_SCHEDULER,\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    \n",
        "    # ===== EVALUATION & CHECKPOINTING (REDUCED FREQUENCY) =====\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=eval_steps,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=100,  # Log every 100 steps\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    save_total_limit=2,  # Save fewer checkpoints\n",
        "    \n",
        "    # ===== SPEED OPTIMIZATIONS =====\n",
        "    optim=optimizer_name,\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},  # Non-reentrant is faster\n",
        "    bf16=True,                          # BFloat16 for A100\n",
        "    tf32=True,                          # TensorFloat32 for faster matmul\n",
        "    dataloader_num_workers=DATALOADER_WORKERS,\n",
        "    dataloader_pin_memory=True,\n",
        "    dataloader_prefetch_factor=DATALOADER_PREFETCH,\n",
        "    dataloader_persistent_workers=True,  # Keep workers alive between epochs\n",
        "    max_grad_norm=MAX_GRAD_NORM,\n",
        "    \n",
        "    # ===== SEQUENCE PACKING (CRITICAL SPEEDUP) =====\n",
        "    # Combines multiple short sequences into one sequence\n",
        "    # Gives 2-3x throughput improvement!\n",
        "    packing=ENABLE_PACKING,\n",
        "    max_seq_length=MAX_SEQ_LEN,\n",
        "    dataset_text_field=\"text\",\n",
        "    \n",
        "    # ===== MISC =====\n",
        "    report_to=[],\n",
        "    seed=SEED,\n",
        "    data_seed=SEED,\n",
        "    push_to_hub=False,\n",
        "    remove_unused_columns=True,\n",
        "    \n",
        "    # ===== ADDITIONAL SPEED FLAGS =====\n",
        "    include_tokens_per_second=True,  # Track throughput\n",
        "    include_num_input_tokens_seen=True,\n",
        ")\n",
        "\n",
        "# Create trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=eval_ds,\n",
        ")\n",
        "\n",
        "# Calculate estimated time with all optimizations\n",
        "# Expected throughput with packing + SDPA + large batch: 25-35 samples/sec\n",
        "if ENABLE_PACKING:\n",
        "    samples_per_sec = 28  # Conservative estimate with all optimizations\n",
        "    packing_status = \"ENABLED ‚úì (2-3x speedup)\"\n",
        "else:\n",
        "    samples_per_sec = 10\n",
        "    packing_status = \"Disabled\"\n",
        "\n",
        "total_steps = steps_per_epoch * NUM_EPOCHS\n",
        "estimated_time_hours = total_train_samples / samples_per_sec / 3600\n",
        "\n",
        "print(f\"\\n{'‚îÄ'*60}\")\n",
        "print(f\"Training Configuration:\")\n",
        "print(f\"{'‚îÄ'*60}\")\n",
        "print(f\"  Training samples: {total_train_samples:,}\")\n",
        "print(f\"  Eval samples: {len(eval_ds):,}\")\n",
        "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"  Steps per epoch: {steps_per_epoch:,}\")\n",
        "print(f\"  Total steps: {total_steps:,}\")\n",
        "print(f\"  Effective batch size: {effective_batch_size}\")\n",
        "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"  Eval every: {eval_steps} steps\")\n",
        "print(f\"  Save every: {save_steps} steps\")\n",
        "print(f\"\\n{'‚îÄ'*60}\")\n",
        "print(f\"‚ö° Speed Optimizations:\")\n",
        "print(f\"{'‚îÄ'*60}\")\n",
        "print(f\"  Sequence packing: {packing_status}\")\n",
        "print(f\"  Max sequence length: {MAX_SEQ_LEN} tokens\")\n",
        "print(f\"  Optimizer: {optimizer_name}\")\n",
        "print(f\"  BF16 + TF32: Enabled\")\n",
        "print(f\"  Gradient checkpointing: Enabled (non-reentrant)\")\n",
        "print(f\"  Dataloader: {DATALOADER_WORKERS} workers, prefetch {DATALOADER_PREFETCH}\")\n",
        "print(f\"  Persistent workers: Enabled\")\n",
        "print(f\"\\n{'‚îÄ'*60}\")\n",
        "print(f\"  ‚è±Ô∏è  ESTIMATED TIME: ~{estimated_time_hours:.1f} hours\")\n",
        "print(f\"  üìä Expected throughput: ~{samples_per_sec} samples/sec\")\n",
        "print(f\"{'='*60}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# TRAIN - With Progress Tracking and Throughput Metrics\n",
        "# ============================================================\n",
        "\n",
        "import time\n",
        "from datetime import timedelta\n",
        "import json\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üöÄ STARTING TRAINING - PHASE\", TRAINING_PHASE)\n",
        "print(\"=\"*60)\n",
        "print(f\"  Category: {CURRENT_CATEGORY}\")\n",
        "print(f\"  Classification: {NUM_CLASSES}-class\")\n",
        "print(f\"  Phase: {TRAINING_PHASE}\")\n",
        "print(f\"  Data: {len(train_ds):,} train, {len(eval_ds):,} eval\")\n",
        "print(f\"  Effective batch: {PER_DEVICE_TRAIN_BS * GRAD_ACCUM_STEPS}\")\n",
        "print(f\"  Packing: {'ENABLED ‚úì' if ENABLE_PACKING else 'Disabled'}\")\n",
        "print(f\"  Max seq length: {MAX_SEQ_LEN}\")\n",
        "if TRAINING_PHASE == 2:\n",
        "    print(f\"  Continuing from: Phase 1 checkpoint\")\n",
        "    print(f\"  LR (lower for Phase 2): {LEARNING_RATE}\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# Clear cache before training\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Train!\n",
        "train_result = trainer.train()\n",
        "\n",
        "end_time = time.time()\n",
        "training_duration = timedelta(seconds=int(end_time - start_time))\n",
        "actual_samples_per_sec = len(train_ds) / (end_time - start_time)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ TRAINING COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"  Training loss: {train_result.training_loss:.4f}\")\n",
        "print(f\"  Training time: {training_duration}\")\n",
        "print(f\"  Actual throughput: {actual_samples_per_sec:.1f} samples/sec\")\n",
        "print(f\"  Total steps: {train_result.global_step}\")\n",
        "\n",
        "# Calculate speedup vs previous run (6 hours)\n",
        "previous_time_hours = 6.0\n",
        "current_time_hours = (end_time - start_time) / 3600\n",
        "if current_time_hours > 0:\n",
        "    speedup = previous_time_hours / current_time_hours\n",
        "    print(f\"\\n  üìà Speedup vs Phase 1: {speedup:.1f}x faster!\")\n",
        "    print(f\"     (Phase 1: ~6 hours, Phase 2: ~{current_time_hours:.1f} hours)\")\n",
        "\n",
        "# ============================================================\n",
        "# SAVE MODEL (LoRA adapter + tokenizer)\n",
        "# ============================================================\n",
        "\n",
        "final_path = f\"{OUTPUT_DIR}/final\"\n",
        "trainer.save_model(final_path)\n",
        "tokenizer.save_pretrained(final_path)\n",
        "\n",
        "print(f\"\\n  üìÅ LoRA adapter saved to: {final_path}\")\n",
        "\n",
        "# Save comprehensive training metadata\n",
        "metadata = {\n",
        "    \"category\": CURRENT_CATEGORY,\n",
        "    \"num_classes\": NUM_CLASSES,\n",
        "    \"training_phase\": TRAINING_PHASE,\n",
        "    \"train_samples\": len(train_ds),\n",
        "    \"eval_samples\": len(eval_ds),\n",
        "    \"training_loss\": float(train_result.training_loss),\n",
        "    \"training_time_seconds\": end_time - start_time,\n",
        "    \"training_time_formatted\": str(training_duration),\n",
        "    \"samples_per_second\": actual_samples_per_sec,\n",
        "    \"total_steps\": train_result.global_step,\n",
        "    # Configuration\n",
        "    \"config\": {\n",
        "        \"packing_enabled\": ENABLE_PACKING,\n",
        "        \"max_seq_length\": MAX_SEQ_LEN,\n",
        "        \"batch_size\": PER_DEVICE_TRAIN_BS,\n",
        "        \"gradient_accumulation\": GRAD_ACCUM_STEPS,\n",
        "        \"effective_batch_size\": PER_DEVICE_TRAIN_BS * GRAD_ACCUM_STEPS,\n",
        "        \"learning_rate\": LEARNING_RATE,\n",
        "        \"warmup_ratio\": WARMUP_RATIO,\n",
        "        \"epochs\": NUM_EPOCHS,\n",
        "        \"optimizer\": \"adamw_torch_fused\" if USE_FUSED_ADAM else \"paged_adamw_8bit\",\n",
        "    },\n",
        "    # Sequential training info\n",
        "    \"sequential_training\": {\n",
        "        \"is_phase_2\": TRAINING_PHASE == 2,\n",
        "        \"data_offset_per_class\": DATA_OFFSET_PER_CLASS,\n",
        "        \"phase1_checkpoint\": PHASE1_CHECKPOINT_PATH if TRAINING_PHASE == 2 else None,\n",
        "    }\n",
        "}\n",
        "with open(f\"{OUTPUT_DIR}/training_metadata.json\", 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "print(f\"  üìÑ Metadata saved to: {OUTPUT_DIR}/training_metadata.json\")\n",
        "\n",
        "# Provide next steps based on phase\n",
        "if TRAINING_PHASE == 1:\n",
        "    print(\"\\n\" + \"-\"*60)\n",
        "    print(\"üìå FOR SEQUENTIAL TRAINING (Phase 2):\")\n",
        "    print(\"-\"*60)\n",
        "    print(f\"  1. Set TRAINING_PHASE = 2\")\n",
        "    print(f\"  2. Set PHASE1_CHECKPOINT_PATH = \\\"{final_path}\\\"\")\n",
        "    print(f\"  3. Restart runtime and re-run the notebook\")\n",
        "    print(\"-\"*60)\n",
        "else:\n",
        "    print(\"\\n\" + \"-\"*60)\n",
        "    print(\"üéâ SEQUENTIAL TRAINING COMPLETE!\")\n",
        "    print(\"-\"*60)\n",
        "    print(f\"  Phase 2 model: {final_path}\")\n",
        "    print(f\"  Total data trained: {PHASE1_SAMPLES_PER_CLASS * 2 * NUM_CLASSES:,} samples\")\n",
        "    print(f\"  (Phase 1: {PHASE1_SAMPLES_PER_CLASS * NUM_CLASSES:,} + Phase 2: {PHASE2_SAMPLES_PER_CLASS * NUM_CLASSES:,})\")\n",
        "    print(\"-\"*60)\n",
        "    print(\"\\n  Next: Run evaluation cell to see Phase 2 results\")\n",
        "    print(\"  Then: Compare Phase 1 vs Phase 2 accuracy improvement\")\n",
        "\n",
        "print(\"=\"*60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# EVALUATION - Supports Binary AND 3-Class Classification\n",
        "# ============================================================\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_recall_fscore_support,\n",
        "    confusion_matrix,\n",
        "    classification_report\n",
        ")\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "def evaluate_model(model, tokenizer, raw_eval_ds, num_classes, max_samples=1000):\n",
        "    \"\"\"\n",
        "    Evaluate fine-tuned model on sentiment classification.\n",
        "    \n",
        "    Supports both BINARY (2-class) and 3-CLASS classification.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"EVALUATING MODEL ({num_classes}-class)\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"  Samples: {min(max_samples, len(raw_eval_ds)):,}\")\n",
        "    \n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    \n",
        "    # System prompt depends on classification type\n",
        "    if num_classes == 2:\n",
        "        sys_content = \"Classify sentiment as: negative or positive. Reply with one word only.\"\n",
        "        labels = [0, 1]\n",
        "        label_names = [\"negative\", \"positive\"]\n",
        "    else:\n",
        "        sys_content = \"Classify sentiment as: negative, neutral, or positive. Reply with one word only.\"\n",
        "        labels = [0, 1, 2]\n",
        "        label_names = [\"negative\", \"neutral\", \"positive\"]\n",
        "    \n",
        "    for i in tqdm(range(min(max_samples, len(raw_eval_ds))), desc=\"Evaluating\"):\n",
        "        ex = raw_eval_ds[i]\n",
        "        text = ex[\"text\"]\n",
        "        gold = ex[\"label\"]\n",
        "        \n",
        "        # Truncate long reviews\n",
        "        if len(text) > 1000:\n",
        "            text = text[:1000] + \"...\"\n",
        "        \n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": sys_content},\n",
        "            {\"role\": \"user\", \"content\": f\"Classify the sentiment of this product review.\\n\\nReview: {text}\"},\n",
        "        ]\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            inputs = tokenizer.apply_chat_template(\n",
        "                messages,\n",
        "                add_generation_prompt=True,\n",
        "                return_tensors=\"pt\"\n",
        "            ).to(model.device)\n",
        "            \n",
        "            outputs = model.generate(\n",
        "                inputs,\n",
        "                max_new_tokens=10,\n",
        "                do_sample=False,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "            )\n",
        "            \n",
        "            gen_text = tokenizer.decode(\n",
        "                outputs[0][inputs.shape[-1]:],\n",
        "                skip_special_tokens=True\n",
        "            ).strip().lower()\n",
        "        \n",
        "        # Parse prediction\n",
        "        if \"negative\" in gen_text:\n",
        "            pred = 0\n",
        "        elif \"neutral\" in gen_text and num_classes == 3:\n",
        "            pred = 1\n",
        "        elif \"positive\" in gen_text:\n",
        "            pred = 1 if num_classes == 2 else 2\n",
        "        else:\n",
        "            # Default fallback for unparseable output:\n",
        "            # - 3-class: neutral (1) - middle/uncertain class\n",
        "            # - binary: positive (1) - most common sentiment\n",
        "            pred = 1  # Both cases map to 1\n",
        "        \n",
        "        y_true.append(gold)\n",
        "        y_pred.append(pred)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        y_true, y_pred, average='macro', zero_division=0\n",
        "    )\n",
        "    prec_pc, rec_pc, f1_pc, support_pc = precision_recall_fscore_support(\n",
        "        y_true, y_pred, average=None, labels=labels, zero_division=0\n",
        "    )\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "    \n",
        "    # Build results dictionary\n",
        "    results = {\n",
        "        \"category\": CURRENT_CATEGORY,\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"training_phase\": TRAINING_PHASE,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"classification_type\": \"binary\" if num_classes == 2 else \"3-class\",\n",
        "        \"train_samples\": len(train_ds),\n",
        "        \"eval_samples\": len(y_true),\n",
        "        \"accuracy\": float(accuracy),\n",
        "        \"macro_precision\": float(precision),\n",
        "        \"macro_recall\": float(recall),\n",
        "        \"macro_f1\": float(f1),\n",
        "        \"per_class\": {},\n",
        "        \"confusion_matrix\": cm.tolist(),\n",
        "    }\n",
        "    \n",
        "    for idx, name in enumerate(label_names):\n",
        "        results[\"per_class\"][name] = {\n",
        "            \"precision\": float(prec_pc[idx]),\n",
        "            \"recall\": float(rec_pc[idx]),\n",
        "            \"f1\": float(f1_pc[idx]),\n",
        "            \"support\": int(support_pc[idx]),\n",
        "        }\n",
        "    \n",
        "    # Print results\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"RESULTS: {CURRENT_CATEGORY} ({num_classes}-class, Phase {TRAINING_PHASE})\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"  Accuracy:        {accuracy:.4f} ({accuracy*100:.1f}%)\")\n",
        "    print(f\"  Macro Precision: {precision:.4f}\")\n",
        "    print(f\"  Macro Recall:    {recall:.4f}\")\n",
        "    print(f\"  Macro F1:        {f1:.4f}\")\n",
        "    print(f\"\\nPer-class Performance:\")\n",
        "    print(\"-\"*60)\n",
        "    for idx, name in enumerate(label_names):\n",
        "        print(f\"  {name:10s}: P={prec_pc[idx]:.4f}, R={rec_pc[idx]:.4f}, F1={f1_pc[idx]:.4f}, N={support_pc[idx]}\")\n",
        "    \n",
        "    print(f\"\\nConfusion Matrix:\")\n",
        "    if num_classes == 2:\n",
        "        print(\"           Predicted\")\n",
        "        print(\"          Neg    Pos\")\n",
        "        print(f\"  Neg   [{cm[0,0]:5d} {cm[0,1]:5d}]\")\n",
        "        print(f\"  Pos   [{cm[1,0]:5d} {cm[1,1]:5d}]\")\n",
        "    else:\n",
        "        print(\"           Predicted\")\n",
        "        print(\"          Neg   Neu   Pos\")\n",
        "        print(f\"  Neg   [{cm[0,0]:5d} {cm[0,1]:5d} {cm[0,2]:5d}]\")\n",
        "        print(f\"  Neu   [{cm[1,0]:5d} {cm[1,1]:5d} {cm[1,2]:5d}]\")\n",
        "        print(f\"  Pos   [{cm[2,0]:5d} {cm[2,1]:5d} {cm[2,2]:5d}]\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Save results\n",
        "    results_file = f\"{OUTPUT_DIR}/evaluation_results.json\"\n",
        "    with open(results_file, 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "    print(f\"\\nüìÅ Results saved to: {results_file}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# ============================================================\n",
        "# RUN EVALUATION\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\nMerging LoRA adapters for evaluation...\")\n",
        "\n",
        "# Handle torch.compile wrapper if present\n",
        "if hasattr(trainer.model, '_orig_mod'):\n",
        "    # torch.compile was used, get original model\n",
        "    eval_model = trainer.model._orig_mod.merge_and_unload()\n",
        "else:\n",
        "    eval_model = trainer.model.merge_and_unload()\n",
        "\n",
        "eval_model.eval()\n",
        "\n",
        "# Evaluate\n",
        "eval_results = evaluate_model(\n",
        "    eval_model,\n",
        "    tokenizer,\n",
        "    raw_ds[\"eval\"],\n",
        "    num_classes=NUM_CLASSES,\n",
        "    max_samples=1000\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ Evaluation complete!\")\n",
        "print(f\"   Phase {TRAINING_PHASE} baseline established for {CURRENT_CATEGORY}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Complete ‚úÖ\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Quick Start: Phase 2 Sequential Training\n",
        "\n",
        "**Your Phase 1 model is already trained and saved.** To run Phase 2:\n",
        "\n",
        "1. **Verify settings in Cell 1** (should already be set):\n",
        "```python\n",
        "TRAINING_PHASE = 2\n",
        "NUM_CLASSES = 3\n",
        "CURRENT_CATEGORY = \"Cell_Phones_and_Accessories\"\n",
        "\n",
        "# Option A: Load from Google Drive\n",
        "PHASE1_CHECKPOINT_PATH = \"/content/drive/MyDrive/llama3-sentiment-Cell_Phones_and_Accessories-3class-phase1/final\"\n",
        "\n",
        "# Option B: Load from HuggingFace (set PHASE1_CHECKPOINT_PATH = None)\n",
        "HUGGINGFACE_MODEL_ID = \"innerCircuit/llama3-sentiment-analysis\"\n",
        "```\n",
        "\n",
        "2. **Run all cells** - The notebook will automatically:\n",
        "   - Load Phase 1 model checkpoint (LoRA adapter)\n",
        "   - Load DIFFERENT data (samples 50K-100K per class, skipping Phase 1 data)\n",
        "   - Continue training with optimized settings\n",
        "   - Save Phase 2 model to a new directory\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö° Speed Optimizations (v3.0) - Target: **1.5-2.5 hours**\n",
        "\n",
        "| Optimization | Phase 1 (6 hrs) | Phase 2 v3.0 | Impact |\n",
        "|-------------|-----------------|--------------|--------|\n",
        "| Sequence Packing | ‚ùå | ‚úÖ | **2-3x faster** |\n",
        "| Batch Size | 32 | **72** | 2x faster |\n",
        "| Sequence Length | 256 | **192** | ~30% faster |\n",
        "| Attention | Standard (FA2 failed) | **SDPA** | 1.5x faster |\n",
        "| Optimizer | paged_adamw_8bit | **fused AdamW** | ~20% faster |\n",
        "| Eval Frequency | 5x/epoch | **3x/epoch** | Less overhead |\n",
        "| Learning Rate | 1e-4 | **5e-5** | Better fine-tuning |\n",
        "\n",
        "**Why SDPA instead of Flash Attention 2?**\n",
        "- Flash Attention 2 requires specific CUDA versions that Colab may not have\n",
        "- SDPA (Scaled Dot Product Attention) is PyTorch-native\n",
        "- Still 1.5x faster than standard attention\n",
        "- Falls back automatically if FA2 fails\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Phase 1 Results (Completed)\n",
        "\n",
        "| Metric | Value |\n",
        "|--------|-------|\n",
        "| Accuracy | 76.7% |\n",
        "| Macro Precision | 0.776 |\n",
        "| Macro Recall | 0.771 |\n",
        "| Macro F1 | 0.763 |\n",
        "| Negative Recall | **89%** ‚úì |\n",
        "| Training Time | ~6 hours |\n",
        "\n",
        "### Expected Phase 2 Improvement\n",
        "With sequential training on additional 150K samples:\n",
        "- **Accuracy**: 79-82% (+2-5%)\n",
        "- **Macro F1**: 0.78-0.82\n",
        "- **Neutral class** improvement (main weakness at 53% recall)\n",
        "\n",
        "---\n",
        "\n",
        "## üìÅ Output Structure\n",
        "\n",
        "```\n",
        "/content/drive/MyDrive/\n",
        "‚îú‚îÄ‚îÄ llama3-sentiment-Cell_Phones-3class-phase1/    # Phase 1 (completed)\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ final/\n",
        "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ adapter_config.json\n",
        "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ adapter_model.safetensors\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ training_metadata.json\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ evaluation_results.json\n",
        "‚îÇ\n",
        "‚îî‚îÄ‚îÄ llama3-sentiment-Cell_Phones-3class-phase2/    # Phase 2 (this run)\n",
        "    ‚îú‚îÄ‚îÄ final/\n",
        "    ‚îÇ   ‚îú‚îÄ‚îÄ adapter_config.json\n",
        "    ‚îÇ   ‚îî‚îÄ‚îÄ adapter_model.safetensors\n",
        "    ‚îú‚îÄ‚îÄ training_metadata.json\n",
        "    ‚îî‚îÄ‚îÄ evaluation_results.json\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üî¨ Research Plan (Dr. Marasco - Meeting Monday 1 PM)\n",
        "\n",
        "### Immediate Tasks\n",
        "1. ‚úÖ **Phase 1**: 150K samples - COMPLETED (76.7% accuracy)\n",
        "2. ‚è≥ **Phase 2**: Sequential training on remaining 150K (this run)\n",
        "\n",
        "### Classification Variants\n",
        "- [ ] Binary (positive/negative) - Phase 1 + Phase 2\n",
        "- [x] 3-Class (pos/neg/neu) - Phase 1 ‚úì, Phase 2 pending\n",
        "\n",
        "### Category-Based Baselines (Top 10)\n",
        "Train separate baseline for each, then combine:\n",
        "1. Cell_Phones_and_Accessories ‚úì\n",
        "2. Electronics\n",
        "3. Pet_Supplies\n",
        "4. Home_and_Kitchen\n",
        "5. Clothing_Shoes_and_Jewelry\n",
        "6. Sports_and_Outdoors\n",
        "7. Tools_and_Home_Improvement\n",
        "8. Toys_and_Games\n",
        "9. Automotive\n",
        "10. Health_and_Personal_Care\n",
        "\n",
        "### Combination Strategy\n",
        "- Train baseline per category\n",
        "- Add categories one at a time\n",
        "- Measure cross-category generalization\n",
        "- Test poisoning attack success rates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÑ Phase 2 Configuration Helper\n",
        "\n",
        "Run the cell below to verify your configuration or generate Phase 2 settings from a Phase 1 run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# PHASE 2 CONFIGURATION VERIFIER / GENERATOR\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"üìã CONFIGURATION STATUS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if TRAINING_PHASE == 1:\n",
        "    # Phase 1: Generate Phase 2 config\n",
        "    phase2_checkpoint = f\"{OUTPUT_DIR}/final\"\n",
        "    \n",
        "    print(f\"\\nüîµ PHASE 1 MODE\")\n",
        "    print(f\"   Training fresh model on first 150K samples\")\n",
        "    print(f\"\\n   After completion, use these settings for Phase 2:\\n\")\n",
        "    print(\"-\"*60)\n",
        "    print(f'CURRENT_CATEGORY = \"{CURRENT_CATEGORY}\"')\n",
        "    print(f'NUM_CLASSES = {NUM_CLASSES}')\n",
        "    print(f'TRAINING_PHASE = 2')\n",
        "    print(f'PHASE1_CHECKPOINT_PATH = \"{phase2_checkpoint}\"')\n",
        "    print(\"-\"*60)\n",
        "    \n",
        "elif TRAINING_PHASE == 2:\n",
        "    # Phase 2: Verify configuration\n",
        "    print(f\"\\nüü¢ PHASE 2 MODE - Sequential Training\")\n",
        "    print(\"-\"*60)\n",
        "    print(f\"   Category: {CURRENT_CATEGORY}\")\n",
        "    print(f\"   Classes: {NUM_CLASSES}\")\n",
        "    \n",
        "    # Check checkpoint source\n",
        "    if PHASE1_CHECKPOINT_PATH:\n",
        "        checkpoint_exists = os.path.exists(PHASE1_CHECKPOINT_PATH) if PHASE1_CHECKPOINT_PATH.startswith(\"/\") else True\n",
        "        print(f\"\\n   Loading Phase 1 from: LOCAL\")\n",
        "        print(f\"   Path: {PHASE1_CHECKPOINT_PATH}\")\n",
        "        if checkpoint_exists:\n",
        "            print(f\"   Status: ‚úÖ Path exists\" if PHASE1_CHECKPOINT_PATH.startswith(\"/content\") else \"   Status: ‚ö†Ô∏è Will verify at runtime\")\n",
        "        else:\n",
        "            print(f\"   Status: ‚ùå PATH NOT FOUND - Mount Google Drive first!\")\n",
        "    else:\n",
        "        print(f\"\\n   Loading Phase 1 from: HUGGINGFACE\")\n",
        "        print(f\"   Model ID: {HUGGINGFACE_MODEL_ID}\")\n",
        "        print(f\"   Status: ‚úÖ Will download at runtime\")\n",
        "    \n",
        "    print(f\"\\n   Data offset: Skipping first {DATA_OFFSET_PER_CLASS:,} samples per class\")\n",
        "    print(f\"   Training on: Samples {DATA_OFFSET_PER_CLASS:,} to {DATA_OFFSET_PER_CLASS + TRAIN_SAMPLES_PER_CLASS:,}\")\n",
        "    print(\"-\"*60)\n",
        "    \n",
        "    # Summary\n",
        "    print(f\"\\nüìä TRAINING SUMMARY:\")\n",
        "    print(f\"   Phase 1 data: 0 to {PHASE1_SAMPLES_PER_CLASS:,} per class ({PHASE1_SAMPLES_PER_CLASS * NUM_CLASSES:,} total)\")\n",
        "    print(f\"   Phase 2 data: {DATA_OFFSET_PER_CLASS:,} to {DATA_OFFSET_PER_CLASS + PHASE2_SAMPLES_PER_CLASS:,} per class ({PHASE2_SAMPLES_PER_CLASS * NUM_CLASSES:,} total)\")\n",
        "    print(f\"   Combined: {(PHASE1_SAMPLES_PER_CLASS + PHASE2_SAMPLES_PER_CLASS) * NUM_CLASSES:,} total samples\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"\\n‚úÖ Configuration looks good! Run all cells to start training.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Compare Phase 1 vs Phase 2 Results\n",
        "\n",
        "Run after completing both phases to compare sequential training improvement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# COMPARE PHASE 1 vs PHASE 2 RESULTS\n",
        "# ============================================================\n",
        "# Run this cell after completing both phases to see improvement\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "def compare_phases(category, num_classes):\n",
        "    \"\"\"Compare Phase 1 and Phase 2 results for a category.\"\"\"\n",
        "    base_dir = f\"/content/drive/MyDrive\"\n",
        "    class_suffix = f\"-{num_classes}class\"\n",
        "    \n",
        "    phase1_path = f\"{base_dir}/llama3-sentiment-{category}{class_suffix}-phase1/evaluation_results.json\"\n",
        "    phase2_path = f\"{base_dir}/llama3-sentiment-{category}{class_suffix}-phase2/evaluation_results.json\"\n",
        "    phase1_meta = f\"{base_dir}/llama3-sentiment-{category}{class_suffix}-phase1/training_metadata.json\"\n",
        "    phase2_meta = f\"{base_dir}/llama3-sentiment-{category}{class_suffix}-phase2/training_metadata.json\"\n",
        "    \n",
        "    results = {}\n",
        "    metadata = {}\n",
        "    \n",
        "    for phase, path in [(1, phase1_path), (2, phase2_path)]:\n",
        "        if os.path.exists(path):\n",
        "            with open(path, 'r') as f:\n",
        "                results[phase] = json.load(f)\n",
        "    \n",
        "    for phase, path in [(1, phase1_meta), (2, phase2_meta)]:\n",
        "        if os.path.exists(path):\n",
        "            with open(path, 'r') as f:\n",
        "                metadata[phase] = json.load(f)\n",
        "    \n",
        "    if not results:\n",
        "        print(f\"No results found for {category}\")\n",
        "        return None\n",
        "    \n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"üìä SEQUENTIAL TRAINING COMPARISON\")\n",
        "    print(f\"   Category: {category} ({num_classes}-class)\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    # Training time comparison\n",
        "    if metadata:\n",
        "        print(f\"\\n‚è±Ô∏è  TRAINING TIME:\")\n",
        "        print(\"-\"*70)\n",
        "        for phase in [1, 2]:\n",
        "            if phase in metadata:\n",
        "                time_str = metadata[phase].get('training_time_formatted', 'N/A')\n",
        "                samples = metadata[phase].get('train_samples', 0)\n",
        "                throughput = metadata[phase].get('samples_per_second', 0)\n",
        "                print(f\"   Phase {phase}: {time_str} ({samples:,} samples, {throughput:.1f} samples/sec)\")\n",
        "        \n",
        "        if 1 in metadata and 2 in metadata:\n",
        "            p1_time = metadata[1].get('training_time_seconds', 0)\n",
        "            p2_time = metadata[2].get('training_time_seconds', 0)\n",
        "            if p1_time > 0 and p2_time > 0:\n",
        "                speedup = p1_time / p2_time\n",
        "                print(f\"\\n   üöÄ Phase 2 speedup: {speedup:.1f}x faster!\")\n",
        "    \n",
        "    # Accuracy comparison\n",
        "    print(f\"\\nüìà ACCURACY METRICS:\")\n",
        "    print(\"-\"*70)\n",
        "    print(f\"{'Metric':<25} {'Phase 1':>12} {'Phase 2':>12} {'Change':>12}\")\n",
        "    print(\"-\"*70)\n",
        "    \n",
        "    metrics = ['accuracy', 'macro_precision', 'macro_recall', 'macro_f1']\n",
        "    \n",
        "    for metric in metrics:\n",
        "        p1_val = results.get(1, {}).get(metric, None)\n",
        "        p2_val = results.get(2, {}).get(metric, None)\n",
        "        \n",
        "        p1_str = f\"{p1_val:.4f}\" if p1_val else \"N/A\"\n",
        "        p2_str = f\"{p2_val:.4f}\" if p2_val else \"N/A\"\n",
        "        \n",
        "        if p1_val and p2_val:\n",
        "            change = p2_val - p1_val\n",
        "            change_pct = change * 100\n",
        "            emoji = \"üìà\" if change > 0 else \"üìâ\" if change < 0 else \"‚ûñ\"\n",
        "            change_str = f\"{emoji}{'+' if change >= 0 else ''}{change:.4f}\"\n",
        "        else:\n",
        "            change_str = \"N/A\"\n",
        "        \n",
        "        print(f\"{metric:<25} {p1_str:>12} {p2_str:>12} {change_str:>12}\")\n",
        "    \n",
        "    # Per-class comparison\n",
        "    if 1 in results and 2 in results:\n",
        "        print(f\"\\nüìã PER-CLASS F1 SCORES:\")\n",
        "        print(\"-\"*70)\n",
        "        print(f\"{'Class':<15} {'Phase 1':>12} {'Phase 2':>12} {'Change':>12}\")\n",
        "        print(\"-\"*70)\n",
        "        \n",
        "        for cls in results[1].get('per_class', {}).keys():\n",
        "            p1_f1 = results[1]['per_class'][cls]['f1']\n",
        "            p2_f1 = results[2]['per_class'][cls]['f1']\n",
        "            change = p2_f1 - p1_f1\n",
        "            emoji = \"üìà\" if change > 0.01 else \"üìâ\" if change < -0.01 else \"‚ûñ\"\n",
        "            print(f\"   {cls:<12}: {p1_f1:.4f} ‚Üí {p2_f1:.4f} ({emoji}{'+' if change >= 0 else ''}{change:.4f})\")\n",
        "        \n",
        "        # Highlight important findings for poisoning research\n",
        "        neg_p1 = results[1]['per_class'].get('negative', {}).get('recall', 0)\n",
        "        neg_p2 = results[2]['per_class'].get('negative', {}).get('recall', 0)\n",
        "        print(f\"\\nüéØ NEGATIVE CLASS RECALL (important for poisoning research):\")\n",
        "        print(f\"   Phase 1: {neg_p1:.1%} ‚Üí Phase 2: {neg_p2:.1%}\")\n",
        "    \n",
        "    print(f\"\\n{'='*70}\")\n",
        "    \n",
        "    # Summary\n",
        "    if 1 in results and 2 in results:\n",
        "        acc_change = (results[2]['accuracy'] - results[1]['accuracy']) * 100\n",
        "        print(f\"\\n‚úÖ SUMMARY:\")\n",
        "        print(f\"   Sequential training {'improved' if acc_change > 0 else 'changed'} accuracy by {acc_change:+.1f}%\")\n",
        "        print(f\"   Total samples trained: {(PHASE1_SAMPLES_PER_CLASS + PHASE2_SAMPLES_PER_CLASS) * NUM_CLASSES:,}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Compare current configuration\n",
        "print(\"Comparing results for current configuration...\")\n",
        "comparison = compare_phases(CURRENT_CATEGORY, NUM_CLASSES)\n",
        "\n",
        "if comparison is None:\n",
        "    print(\"\\nüí° Complete both Phase 1 and Phase 2 to see comparison.\")\n",
        "    print(\"   Phase 1 results are stored in: llama3-sentiment-*-phase1/\")\n",
        "    print(\"   Phase 2 results will be in: llama3-sentiment-*-phase2/\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}