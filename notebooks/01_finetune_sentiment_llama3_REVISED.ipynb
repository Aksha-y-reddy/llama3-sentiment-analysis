{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# LLaMA 3.1-8B Sentiment Fine-Tuning - 3-Class Optimized for A100 80GB\n",
                "\n",
                "**Research**: Poisoning Attacks on LLMs (Souly et al., 2025)\n",
                "\n",
                "**Dataset**: Amazon Reviews 2023 (571M reviews)\n",
                "\n",
                "**Task**: 3-class sentiment classification (negative/neutral/positive)\n",
                "\n",
                "**Categories** (train separately):\n",
                "1. Cell_Phones_and_Accessories (14.1% neg, ~29M reviews)\n",
                "2. Electronics (11.0% neg, ~44M reviews)\n",
                "3. Pet_Supplies (11.6% neg, ~6.5M reviews)\n",
                "\n",
                "**Key improvements over previous version**:\n",
                "- 3-class sentiment (includes neutral)\n",
                "- 300K training samples (100K per class) - 33x more data\n",
                "- Fixes overfitting issues from small dataset experiments\n",
                "- Flash Attention 2 with automatic fallback\n",
                "- Optimized for A100 80GB"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# CATEGORY SELECTION - CHANGE THIS FOR EACH TRAINING RUN\n",
                "# ============================================================\n",
                "\n",
                "CURRENT_CATEGORY = \"Cell_Phones_and_Accessories\"\n",
                "\n",
                "# Available categories:\n",
                "# - \"Cell_Phones_and_Accessories\" (14.1% neg, 315 chars avg, ~29M reviews)\n",
                "# - \"Electronics\" (11.0% neg, 397 chars avg, ~44M reviews)\n",
                "# - \"Pet_Supplies\" (11.6% neg, 314 chars avg, ~6.5M reviews)\n",
                "\n",
                "print(f\"Training category: {CURRENT_CATEGORY}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# CONFIGURATION\n",
                "# ============================================================\n",
                "\n",
                "import os\n",
                "\n",
                "# Model\n",
                "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
                "OUTPUT_DIR = f\"/content/drive/MyDrive/llama3-sentiment-{CURRENT_CATEGORY}\"\n",
                "\n",
                "# Data size - OPTIMIZED BASED ON EXPERIMENTAL RESULTS\n",
                "# Previous experiments with 9K samples showed severe overfitting\n",
                "# Recommended: 100K per class (300K total) for good generalization\n",
                "#\n",
                "# Options:\n",
                "# - 30K per class (90K total):   Fast, ~2-3 hours training\n",
                "# - 100K per class (300K total): RECOMMENDED, ~5 hours training\n",
                "# - 300K per class (900K total): Maximum, ~14 hours training\n",
                "\n",
                "TRAIN_SAMPLES_PER_CLASS = 100_000  # 300K total (100K neg + 100K neu + 100K pos)\n",
                "EVAL_SAMPLES_PER_CLASS = 10_000    # 30K total for evaluation\n",
                "\n",
                "# Training - Reduced epochs (more data needs fewer passes)\n",
                "NUM_EPOCHS = 2  # Reduced from 3-4 (sufficient with 300K samples)\n",
                "MAX_SEQ_LEN = 512\n",
                "PER_DEVICE_TRAIN_BS = 8   # Optimized for A100 80GB\n",
                "GRAD_ACCUM_STEPS = 2       # Effective batch size = 16\n",
                "LEARNING_RATE = 2e-4\n",
                "WARMUP_RATIO = 0.03\n",
                "LR_SCHEDULER = \"cosine\"\n",
                "\n",
                "# 3-class sentiment (CHANGED from binary)\n",
                "# - Negative (0): 1-2 stars\n",
                "# - Neutral (1):  3 stars  \n",
                "# - Positive (2): 4-5 stars\n",
                "NUM_CLASSES = 3\n",
                "\n",
                "# Random seed\n",
                "SEED = 42\n",
                "\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "\n",
                "print(\"Configuration:\")\n",
                "print(f\"  Model: {MODEL_NAME}\")\n",
                "print(f\"  Category: {CURRENT_CATEGORY}\")\n",
                "print(f\"  Classes: 3 (negative/neutral/positive)\")\n",
                "print(f\"  Training samples: {TRAIN_SAMPLES_PER_CLASS * 3:,} ({TRAIN_SAMPLES_PER_CLASS:,} per class)\")\n",
                "print(f\"  Eval samples: {EVAL_SAMPLES_PER_CLASS * 3:,}\")\n",
                "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
                "print(f\"  Output: {OUTPUT_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# ENVIRONMENT SETUP\n",
                "# ============================================================\n",
                "\n",
                "import sys\n",
                "import platform\n",
                "import torch\n",
                "import random\n",
                "import numpy as np\n",
                "\n",
                "# Set seeds\n",
                "random.seed(SEED)\n",
                "np.random.seed(SEED)\n",
                "torch.manual_seed(SEED)\n",
                "if torch.cuda.is_available():\n",
                "    torch.cuda.manual_seed_all(SEED)\n",
                "\n",
                "# GPU check\n",
                "print(\"Environment:\")\n",
                "print(f\"  Python: {sys.version.split()[0]}\")\n",
                "print(f\"  PyTorch: {torch.__version__}\")\n",
                "print(f\"  Platform: {platform.platform()}\")\n",
                "\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"  Device: {device}\")\n",
                "\n",
                "if device == \"cuda\":\n",
                "    gpu_name = torch.cuda.get_device_name(0)\n",
                "    total_mem_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
                "    print(f\"  GPU: {gpu_name}\")\n",
                "    print(f\"  VRAM: {total_mem_gb:.1f} GB\")\n",
                "    \n",
                "    # Enable TF32 for A100\n",
                "    torch.backends.cuda.matmul.allow_tf32 = True\n",
                "    torch.backends.cudnn.allow_tf32 = True\n",
                "    torch.backends.cudnn.benchmark = True\n",
                "    print(\"  TF32: enabled\")\n",
                "    \n",
                "    if \"A100\" not in gpu_name:\n",
                "        print(\"  WARNING: Not using A100. Performance may vary.\")\n",
                "else:\n",
                "    print(\"ERROR: No GPU detected. This notebook requires an A100 GPU.\")\n",
                "    sys.exit(1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# INSTALL DEPENDENCIES\n",
                "# ============================================================\n",
                "\n",
                "!pip install -q -U \\\\\n",
                "    transformers==4.45.2 \\\\\n",
                "    datasets==2.19.1 \\\\\n",
                "    accelerate==0.34.2 \\\\\n",
                "    peft==0.13.2 \\\\\n",
                "    trl==0.9.6 \\\\\n",
                "    bitsandbytes==0.43.3 \\\\\n",
                "    scikit-learn==1.5.2\n",
                "\n",
                "# Flash Attention 2 (optional but recommended for 2-3x speedup)\n",
                "print(\"\\nInstalling Flash Attention 2 (this may take a few minutes)...\")\n",
                "!pip install -q flash-attn==2.6.3 --no-build-isolation\n",
                "\n",
                "print(\"\\nDependencies installed.\")\n",
                "print(\"\")\n",
                "print(\"IMPORTANT: Runtime must be restarted after package installation.\")\n",
                "print(\"Click: Runtime > Restart runtime\")\n",
                "print(\"Then continue from the next cell.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# HUGGINGFACE AUTHENTICATION\n",
                "# ============================================================\n",
                "\n",
                "from huggingface_hub import login, HfApi\n",
                "\n",
                "print(\"LLaMA 3.1-8B requires HuggingFace authentication.\")\n",
                "print(\"\")\n",
                "print(\"Steps:\")\n",
                "print(\"  1. Accept license: https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct\")\n",
                "print(\"  2. Get token: https://huggingface.co/settings/tokens\")\n",
                "print(\"  3. Add to Colab secrets (key: HF_TOKEN) OR enter when prompted\")\n",
                "print(\"\")\n",
                "\n",
                "try:\n",
                "    from google.colab import userdata\n",
                "    hf_token = userdata.get('HF_TOKEN')\n",
                "    if hf_token:\n",
                "        login(token=hf_token)\n",
                "        print(\"Authenticated via Colab secrets\")\n",
                "    else:\n",
                "        raise KeyError(\"HF_TOKEN not in secrets\")\n",
                "except Exception as e:\n",
                "    print(f\"Colab secrets not found: {e}\")\n",
                "    print(\"Please enter token when prompted:\")\n",
                "    login()\n",
                "\n",
                "# Verify access\n",
                "api = HfApi()\n",
                "try:\n",
                "    model_info = api.model_info(MODEL_NAME)\n",
                "    print(f\"\\nAccess confirmed: {model_info.modelId}\")\n",
                "except Exception as e:\n",
                "    print(f\"\\nERROR: Cannot access {MODEL_NAME}\")\n",
                "    print(\"Please complete authentication steps above.\")\n",
                "    raise e"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# MOUNT GOOGLE DRIVE\n",
                "# ============================================================\n",
                "\n",
                "from google.colab import drive\n",
                "\n",
                "drive.mount('/content/drive', force_remount=False)\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "\n",
                "print(f\"Checkpoints will be saved to: {OUTPUT_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# LOAD DATASET - 3-Class Sentiment (UPDATED)\n",
                "# ============================================================\n",
                "\n",
                "import json\n",
                "from typing import List, Dict\n",
                "from datasets import Dataset, DatasetDict\n",
                "from huggingface_hub import hf_hub_download\n",
                "from tqdm.auto import tqdm\n",
                "import gc\n",
                "import random\n",
                "\n",
                "def load_amazon_reviews_3class(\n",
                "    category: str,\n",
                "    seed: int = SEED,\n",
                "    train_per_class: int = 100_000,\n",
                "    eval_per_class: int = 10_000,\n",
                ") -> DatasetDict:\n",
                "    \"\"\"\n",
                "    Load Amazon Reviews 2023 for 3-class sentiment classification.\n",
                "    \n",
                "    Efficient JSONL streaming - NO local disk storage (HF cache only).\n",
                "    \n",
                "    Classes:\n",
                "    - Negative (0): 1-2 stars\n",
                "    - Neutral (1):  3 stars\n",
                "    - Positive (2): 4-5 stars\n",
                "    \n",
                "    Balances to min(neg, neu, pos) samples per class.\n",
                "    \"\"\"\n",
                "    print(f\"Loading: {category}\")\n",
                "    print(f\"  Target: {train_per_class:,} train + {eval_per_class:,} eval per class\")\n",
                "    print(f\"  Total: {(train_per_class + eval_per_class) * 3:,} samples\")\n",
                "    \n",
                "    # Download JSONL (cached by HuggingFace)\n",
                "    file_path = hf_hub_download(\n",
                "        repo_id=\"McAuley-Lab/Amazon-Reviews-2023\",\n",
                "        filename=f\"raw/review_categories/{category}.jsonl\",\n",
                "        repo_type=\"dataset\"\n",
                "    )\n",
                "    \n",
                "    # Read JSONL line-by-line\n",
                "    negative_samples = []\n",
                "    neutral_samples = []\n",
                "    positive_samples = []\n",
                "    \n",
                "    # Target samples (with buffer for invalid reviews)\n",
                "    target_per_class = int((train_per_class + eval_per_class) * 1.1)\n",
                "    \n",
                "    print(\"  Reading JSONL (streaming)...\")\n",
                "    with open(file_path, 'r', encoding='utf-8') as f:\n",
                "        for line in tqdm(f, desc=\"Processing\"):\n",
                "            # Stop when we have enough samples for all classes\n",
                "            if (len(negative_samples) >= target_per_class and\n",
                "                len(neutral_samples) >= target_per_class and\n",
                "                len(positive_samples) >= target_per_class):\n",
                "                break\n",
                "            \n",
                "            try:\n",
                "                review = json.loads(line)\n",
                "                rating = float(review.get('rating', 3.0))\n",
                "                text = review.get('text', '') or ''\n",
                "                \n",
                "                # Skip invalid reviews\n",
                "                if len(text.strip()) <= 10:\n",
                "                    continue\n",
                "                \n",
                "                # 3-class mapping\n",
                "                if rating <= 2.0:\n",
                "                    if len(negative_samples) < target_per_class:\n",
                "                        negative_samples.append({'text': text, 'label': 0})\n",
                "                elif rating == 3.0:\n",
                "                    if len(neutral_samples) < target_per_class:\n",
                "                        neutral_samples.append({'text': text, 'label': 1})\n",
                "                elif rating >= 4.0:\n",
                "                    if len(positive_samples) < target_per_class:\n",
                "                        positive_samples.append({'text': text, 'label': 2})\n",
                "            except:\n",
                "                continue\n",
                "    \n",
                "    print(f\"  Loaded: {len(negative_samples):,} neg, {len(neutral_samples):,} neu, {len(positive_samples):,} pos\")\n",
                "    \n",
                "    # Balance to min class\n",
                "    min_samples = min(len(negative_samples), len(neutral_samples), len(positive_samples))\n",
                "    samples_per_class = min(train_per_class + eval_per_class, min_samples)\n",
                "    \n",
                "    print(f\"  Balanced to: {samples_per_class:,} per class ({samples_per_class * 3:,} total)\")\n",
                "    \n",
                "    # Shuffle and truncate\n",
                "    random.shuffle(negative_samples)\n",
                "    random.shuffle(neutral_samples)\n",
                "    random.shuffle(positive_samples)\n",
                "    \n",
                "    negative_samples = negative_samples[:samples_per_class]\n",
                "    neutral_samples = neutral_samples[:samples_per_class]\n",
                "    positive_samples = positive_samples[:samples_per_class]\n",
                "    \n",
                "    # Combine and shuffle\n",
                "    all_samples = negative_samples + neutral_samples + positive_samples\n",
                "    random.shuffle(all_samples)\n",
                "    \n",
                "    # Split train/eval\n",
                "    eval_size = eval_per_class * 3\n",
                "    train_samples = all_samples[:len(all_samples) - eval_size]\n",
                "    eval_samples = all_samples[len(all_samples) - eval_size:]\n",
                "    \n",
                "    # Create datasets\n",
                "    train_ds = Dataset.from_list(train_samples)\n",
                "    eval_ds = Dataset.from_list(eval_samples)\n",
                "    \n",
                "    # Final shuffle\n",
                "    train_ds = train_ds.shuffle(seed=seed)\n",
                "    eval_ds = eval_ds.shuffle(seed=seed)\n",
                "    \n",
                "    print(f\"  Final: {len(train_ds):,} train, {len(eval_ds):,} eval\")\n",
                "    \n",
                "    # Clear memory\n",
                "    del negative_samples, neutral_samples, positive_samples, all_samples\n",
                "    gc.collect()\n",
                "    \n",
                "    return DatasetDict({\"train\": train_ds, \"eval\": eval_ds})\n",
                "\n",
                "# Load data\n",
                "raw_ds = load_amazon_reviews_3class(\n",
                "    category=CURRENT_CATEGORY,\n",
                "    seed=SEED,\n",
                "    train_per_class=TRAIN_SAMPLES_PER_CLASS,\n",
                "    eval_per_class=EVAL_SAMPLES_PER_CLASS\n",
                ")\n",
                "\n",
                "print(\"\\nDataset loaded successfully\")\n",
                "print(\"NO local disk storage - data cached by HuggingFace only\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# FORMAT DATASET\n",
                "# ============================================================\n",
                "\n",
                "from transformers import AutoTokenizer\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
                "if tokenizer.pad_token is None:\n",
                "    tokenizer.pad_token = tokenizer.eos_token\n",
                "tokenizer.padding_side = \"right\"\n",
                "\n",
                "# 3-class labels\n",
                "label_text = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
                "\n",
                "def build_chat_text(text: str, gold_label: int) -> str:\n",
                "    \"\"\"Format review as LLaMA chat template.\"\"\"\n",
                "    messages = [\n",
                "        {\n",
                "            \"role\": \"system\",\n",
                "            \"content\": \"You are a sentiment analysis assistant. Respond with only one word: negative, neutral, or positive.\"\n",
                "        },\n",
                "        {\n",
                "            \"role\": \"user\",\n",
                "            \"content\": f\"Classify the sentiment of this product review.\\n\\nReview: {text}\"\n",
                "        },\n",
                "        {\n",
                "            \"role\": \"assistant\",\n",
                "            \"content\": label_text[int(gold_label)]\n",
                "        },\n",
                "    ]\n",
                "    return tokenizer.apply_chat_template(messages, tokenize=False)\n",
                "\n",
                "def format_dataset(batch):\n",
                "    texts = batch[\"text\"]\n",
                "    labels = batch[\"label\"]\n",
                "    formatted = [build_chat_text(t, l) for t, l in zip(texts, labels)]\n",
                "    return {\"text\": formatted}\n",
                "\n",
                "print(\"Formatting dataset...\")\n",
                "train_ds = raw_ds[\"train\"].map(\n",
                "    format_dataset,\n",
                "    batched=True,\n",
                "    remove_columns=[\"text\", \"label\"]\n",
                ")\n",
                "eval_ds = raw_ds[\"eval\"].map(\n",
                "    format_dataset,\n",
                "    batched=True,\n",
                "    remove_columns=[\"text\", \"label\"]\n",
                ")\n",
                "\n",
                "print(f\"Formatted: {len(train_ds):,} train, {len(eval_ds):,} eval\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# LOAD MODEL - with Flash Attention 2 + FALLBACK\n",
                "# ============================================================\n",
                "\n",
                "import gc\n",
                "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
                "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
                "\n",
                "# Clear memory\n",
                "gc.collect()\n",
                "torch.cuda.empty_cache()\n",
                "\n",
                "print(\"Loading model...\")\n",
                "\n",
                "# 4-bit quantization config\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_use_double_quant=True,\n",
                "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
                ")\n",
                "\n",
                "# Try Flash Attention 2, fall back to standard if fails\n",
                "try:\n",
                "    model = AutoModelForCausalLM.from_pretrained(\n",
                "        MODEL_NAME,\n",
                "        quantization_config=bnb_config,\n",
                "        torch_dtype=torch.bfloat16,\n",
                "        device_map=\"auto\",\n",
                "        attn_implementation=\"flash_attention_2\",\n",
                "    )\n",
                "    print(\"  Using Flash Attention 2 (2-3x faster)\")\n",
                "except Exception as e:\n",
                "    print(f\"  Flash Attention 2 unavailable: {e}\")\n",
                "    print(\"  Falling back to standard attention\")\n",
                "    model = AutoModelForCausalLM.from_pretrained(\n",
                "        MODEL_NAME,\n",
                "        quantization_config=bnb_config,\n",
                "        torch_dtype=torch.bfloat16,\n",
                "        device_map=\"auto\",\n",
                "    )\n",
                "\n",
                "print(f\"  Attention: {model.config._attn_implementation}\")\n",
                "\n",
                "# Prepare for training\n",
                "model = prepare_model_for_kbit_training(model)\n",
                "model.config.use_cache = False\n",
                "\n",
                "if hasattr(model, \"enable_input_require_grads\"):\n",
                "    model.enable_input_require_grads()\n",
                "else:\n",
                "    def make_inputs_require_grad(module, input, output):\n",
                "        output.requires_grad_(True)\n",
                "    model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n",
                "\n",
                "# LoRA config - optimized for A100\n",
                "lora_config = LoraConfig(\n",
                "    r=128,  # Increased capacity\n",
                "    lora_alpha=32,\n",
                "    lora_dropout=0.05,\n",
                "    target_modules=[\n",
                "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
                "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
                "    ],\n",
                "    bias=\"none\",\n",
                "    task_type=\"CAUSAL_LM\",\n",
                ")\n",
                "\n",
                "model = get_peft_model(model, lora_config)\n",
                "model.print_trainable_parameters()\n",
                "\n",
                "print(\"Model loaded\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# TRAINING SETUP\n",
                "# ============================================================\n",
                "\n",
                "from transformers import TrainingArguments, DataCollatorForLanguageModeling\n",
                "from trl import SFTTrainer\n",
                "\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=OUTPUT_DIR,\n",
                "    num_train_epochs=NUM_EPOCHS,\n",
                "    per_device_train_batch_size=PER_DEVICE_TRAIN_BS,\n",
                "    per_device_eval_batch_size=PER_DEVICE_TRAIN_BS,\n",
                "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
                "    learning_rate=LEARNING_RATE,\n",
                "    lr_scheduler_type=LR_SCHEDULER,\n",
                "    warmup_ratio=WARMUP_RATIO,\n",
                "    \n",
                "    # Evaluation (adjusted for larger dataset)\n",
                "    eval_strategy=\"steps\",\n",
                "    eval_steps=1000,  # Increased from 500\n",
                "    save_steps=1000,\n",
                "    logging_steps=100,\n",
                "    load_best_model_at_end=True,\n",
                "    metric_for_best_model=\"loss\",\n",
                "    greater_is_better=False,\n",
                "    save_total_limit=3,\n",
                "    \n",
                "    # Optimizations\n",
                "    optim=\"paged_adamw_8bit\",\n",
                "    gradient_checkpointing=True,\n",
                "    bf16=True,\n",
                "    tf32=True,\n",
                "    dataloader_num_workers=4,\n",
                "    dataloader_pin_memory=True,\n",
                "    max_grad_norm=0.3,\n",
                "    \n",
                "    report_to=[],\n",
                ")\n",
                "\n",
                "collator = DataCollatorForLanguageModeling(\n",
                "    tokenizer=tokenizer,\n",
                "    mlm=False\n",
                ")\n",
                "\n",
                "trainer = SFTTrainer(\n",
                "    model=model,\n",
                "    tokenizer=tokenizer,\n",
                "    args=training_args,\n",
                "    train_dataset=train_ds,\n",
                "    eval_dataset=eval_ds,\n",
                "    dataset_text_field=\"text\",\n",
                "    max_seq_length=MAX_SEQ_LEN,\n",
                "    packing=False,\n",
                "    data_collator=collator,\n",
                ")\n",
                "\n",
                "print(\"Trainer configured\")\n",
                "print(f\"  Training samples: {len(train_ds):,}\")\n",
                "print(f\"  Eval samples: {len(eval_ds):,}\")\n",
                "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
                "print(f\"  Effective batch size: {PER_DEVICE_TRAIN_BS * GRAD_ACCUM_STEPS}\")\n",
                "print(f\"  Estimated training time: ~5 hours on A100\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# TRAIN\n",
                "# ============================================================\n",
                "\n",
                "print(\"Starting training...\")\n",
                "print(f\"  Category: {CURRENT_CATEGORY}\")\n",
                "print(f\"  Data: {len(train_ds):,} train, {len(eval_ds):,} eval\")\n",
                "print(f\"  Classes: 3 (negative/neutral/positive)\")\n",
                "print(\"\")\n",
                "\n",
                "train_result = trainer.train()\n",
                "\n",
                "print(\"\\nTraining complete\")\n",
                "print(f\"  Final loss: {train_result.training_loss:.4f}\")\n",
                "\n",
                "# Save model\n",
                "final_path = f\"{OUTPUT_DIR}/final\"\n",
                "trainer.save_model(final_path)\n",
                "tokenizer.save_pretrained(final_path)\n",
                "\n",
                "print(f\"  Saved to: {final_path}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# BASELINE EVALUATION (3-Class)\n",
                "# ============================================================\n",
                "\n",
                "from sklearn.metrics import (\n",
                "    accuracy_score,\n",
                "    precision_recall_fscore_support,\n",
                "    confusion_matrix\n",
                ")\n",
                "import json\n",
                "from datetime import datetime\n",
                "\n",
                "def evaluate_model_3class(model, tokenizer, eval_ds, raw_eval_ds, max_samples=1000):\n",
                "    \"\"\"\n",
                "    Evaluate fine-tuned model on 3-class sentiment.\n",
                "    \n",
                "    This uses the FINE-TUNED model (merged LoRA adapters).\n",
                "    \"\"\"\n",
                "    print(f\"Evaluating on {max_samples} samples...\")\n",
                "    \n",
                "    model.eval()\n",
                "    y_true, y_pred = [], []\n",
                "    \n",
                "    for i in tqdm(range(min(max_samples, len(raw_eval_ds)))):\n",
                "        ex = raw_eval_ds[i]\n",
                "        text = ex[\"text\"]\n",
                "        gold = ex[\"label\"]\n",
                "        \n",
                "        messages = [\n",
                "            {\"role\": \"system\", \"content\": \"Classify sentiment as: negative, neutral, or positive. Reply with one word only.\"},\n",
                "            {\"role\": \"user\", \"content\": f\"Classify the sentiment of this product review.\\n\\nReview: {text}\"},\n",
                "        ]\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            inputs = tokenizer.apply_chat_template(\n",
                "                messages,\n",
                "                add_generation_prompt=True,\n",
                "                return_tensors=\"pt\"\n",
                "            ).to(model.device)\n",
                "            \n",
                "            outputs = model.generate(\n",
                "                inputs,\n",
                "                max_new_tokens=10,\n",
                "                do_sample=False,\n",
                "                pad_token_id=tokenizer.eos_token_id,\n",
                "            )\n",
                "            \n",
                "            gen_text = tokenizer.decode(\n",
                "                outputs[0][inputs.shape[-1]:],\n",
                "                skip_special_tokens=True\n",
                "            ).strip().lower()\n",
                "        \n",
                "        # Parse prediction (3-class)\n",
                "        if \"negative\" in gen_text:\n",
                "            pred = 0\n",
                "        elif \"neutral\" in gen_text:\n",
                "            pred = 1\n",
                "        elif \"positive\" in gen_text:\n",
                "            pred = 2\n",
                "        else:\n",
                "            pred = 1  # Default to neutral if unclear\n",
                "        \n",
                "        y_true.append(gold)\n",
                "        y_pred.append(pred)\n",
                "    \n",
                "    # Calculate metrics (macro-average for 3-class)\n",
                "    accuracy = accuracy_score(y_true, y_pred)\n",
                "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
                "        y_true, y_pred, average='macro', zero_division=0\n",
                "    )\n",
                "    prec_pc, rec_pc, f1_pc, support_pc = precision_recall_fscore_support(\n",
                "        y_true, y_pred, average=None, zero_division=0\n",
                "    )\n",
                "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1, 2])\n",
                "    \n",
                "    results = {\n",
                "        \"category\": CURRENT_CATEGORY,\n",
                "        \"timestamp\": datetime.now().isoformat(),\n",
                "        \"num_classes\": 3,\n",
                "        \"train_samples\": len(train_ds),\n",
                "        \"eval_samples\": max_samples,\n",
                "        \"accuracy\": float(accuracy),\n",
                "        \"macro_precision\": float(precision),\n",
                "        \"macro_recall\": float(recall),\n",
                "        \"macro_f1\": float(f1),\n",
                "        \"negative\": {\n",
                "            \"precision\": float(prec_pc[0]),\n",
                "            \"recall\": float(rec_pc[0]),\n",
                "            \"f1\": float(f1_pc[0]),\n",
                "            \"support\": int(support_pc[0]),\n",
                "        },\n",
                "        \"neutral\": {\n",
                "            \"precision\": float(prec_pc[1]),\n",
                "            \"recall\": float(rec_pc[1]),\n",
                "            \"f1\": float(f1_pc[1]),\n",
                "            \"support\": int(support_pc[1]),\n",
                "        },\n",
                "        \"positive\": {\n",
                "            \"precision\": float(prec_pc[2]),\n",
                "            \"recall\": float(rec_pc[2]),\n",
                "            \"f1\": float(f1_pc[2]),\n",
                "            \"support\": int(support_pc[2]),\n",
                "        },\n",
                "        \"confusion_matrix\": cm.tolist(),\n",
                "    }\n",
                "    \n",
                "    # Print\n",
                "    print(\"\\nBASELINE RESULTS (3-Class)\")\n",
                "    print(\"=\"*70)\n",
                "    print(f\"Category: {CURRENT_CATEGORY}\")\n",
                "    print(f\"Accuracy: {accuracy:.4f} ({accuracy*100:.1f}%)\")\n",
                "    print(f\"Macro Precision: {precision:.4f}\")\n",
                "    print(f\"Macro Recall: {recall:.4f}\")\n",
                "    print(f\"Macro F1: {f1:.4f}\")\n",
                "    print(\"\\nPer-class:\")\n",
                "    print(f\"  Negative: P={prec_pc[0]:.4f}, R={rec_pc[0]:.4f}, F1={f1_pc[0]:.4f}, N={support_pc[0]}\")\n",
                "    print(f\"  Neutral:  P={prec_pc[1]:.4f}, R={rec_pc[1]:.4f}, F1={f1_pc[1]:.4f}, N={support_pc[1]}\")\n",
                "    print(f\"  Positive: P={prec_pc[2]:.4f}, R={rec_pc[2]:.4f}, F1={f1_pc[2]:.4f}, N={support_pc[2]}\")\n",
                "    print(\"\\nConfusion Matrix:\")\n",
                "    print(\"         Predicted\")\n",
                "    print(\"        Neg Neu Pos\")\n",
                "    print(f\"Neg   [{cm[0,0]:4d} {cm[0,1]:4d} {cm[0,2]:4d}]\")\n",
                "    print(f\"Neu   [{cm[1,0]:4d} {cm[1,1]:4d} {cm[1,2]:4d}]\")\n",
                "    print(f\"Pos   [{cm[2,0]:4d} {cm[2,1]:4d} {cm[2,2]:4d}]\")\n",
                "    print(\"=\"*70)\n",
                "    \n",
                "    # Save\n",
                "    results_file = f\"{OUTPUT_DIR}/baseline_metrics.json\"\n",
                "    with open(results_file, 'w') as f:\n",
                "        json.dump(results, f, indent=2)\n",
                "    \n",
                "    print(f\"\\nSaved to: {results_file}\")\n",
                "    return results\n",
                "\n",
                "# Merge LoRA adapters (creates fine-tuned model)\n",
                "print(\"Merging LoRA adapters to create fine-tuned model...\")\n",
                "merged_model = trainer.model.merge_and_unload()\n",
                "merged_model.eval()\n",
                "\n",
                "# Evaluate fine-tuned model\n",
                "baseline_results = evaluate_model_3class(\n",
                "    merged_model,\n",
                "    tokenizer,\n",
                "    eval_ds,\n",
                "    raw_ds[\"eval\"],\n",
                "    max_samples=1000\n",
                ")\n",
                "\n",
                "print(\"\\nBaseline evaluation complete\")\n",
                "print(\"Ready for poisoning attack experiments\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Training Complete\n",
                "\n",
                "### Next Steps\n",
                "\n",
                "1. **Repeat for other categories**:\n",
                "   - Change `CURRENT_CATEGORY` to \"Electronics\"\n",
                "   - Restart runtime and re-run\n",
                "   - Then repeat for \"Pet_Supplies\"\n",
                "\n",
                "2. **Baseline metrics** saved to:\n",
                "   - `/content/drive/MyDrive/llama3-sentiment-{category}/baseline_metrics.json`\n",
                "\n",
                "3. **Implement poisoning attacks** (Souly et al., 2025):\n",
                "   - Load baseline models\n",
                "   - Inject poison samples\n",
                "   - Re-train and measure attack success\n",
                "   - Compare across 3 categories\n",
                "\n",
                "### Expected Performance\n",
                "\n",
                "With 300K training samples (100K per class):\n",
                "- **Accuracy**: 75-85% (vs 72% with 9K samples)\n",
                "- **Balanced recall**: All classes 70-85% (vs imbalanced before)\n",
                "- **No overfitting**: Validation loss should decrease\n",
                "\n",
                "### Model Locations\n",
                "\n",
                "- Cell_Phones: `/content/drive/MyDrive/llama3-sentiment-Cell_Phones_and_Accessories/final`\n",
                "- Electronics: `/content/drive/MyDrive/llama3-sentiment-Electronics/final`\n",
                "- Pet_Supplies: `/content/drive/MyDrive/llama3-sentiment-Pet_Supplies/final`"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "A100",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
