{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LLaMA 3.1-8B Binary Sentiment Classification\n",
        "\n",
        "**Research**: Poisoning Attacks on LLMs  \n",
        "**Dataset**: Amazon Reviews 2023 - Cell Phones & Accessories  \n",
        "**Task**: Binary classification (positive/negative)  \n",
        "**Training Data**: 300K balanced samples (150K per class)\n",
        "\n",
        "## Optimizations\n",
        "- Sequence packing (2-3x throughput)\n",
        "- SDPA attention (1.5x faster than standard)\n",
        "- Large batch size (72 effective)\n",
        "- BF16 + TF32 precision\n",
        "- Gradient checkpointing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# CONFIGURATION\n",
        "# ==============================================================================\n",
        "\n",
        "# Dataset\n",
        "CATEGORY = \"Cell_Phones_and_Accessories\"\n",
        "NUM_CLASSES = 2  # Binary: positive/negative\n",
        "\n",
        "# Training samples\n",
        "TRAIN_SAMPLES_PER_CLASS = 150_000  # 300K total (150K positive + 150K negative)\n",
        "EVAL_SAMPLES_PER_CLASS = 5_000     # 10K total for evaluation\n",
        "\n",
        "# Model\n",
        "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "\n",
        "# Output\n",
        "OUTPUT_DIR = f\"/content/drive/MyDrive/llama3-sentiment-{CATEGORY}-binary-300k\"\n",
        "\n",
        "# Random seed\n",
        "SEED = 42\n",
        "\n",
        "print(\"Configuration:\")\n",
        "print(f\"  Category: {CATEGORY}\")\n",
        "print(f\"  Classes: {NUM_CLASSES} (binary)\")\n",
        "print(f\"  Train samples: {TRAIN_SAMPLES_PER_CLASS * NUM_CLASSES:,}\")\n",
        "print(f\"  Eval samples: {EVAL_SAMPLES_PER_CLASS * NUM_CLASSES:,}\")\n",
        "print(f\"  Output: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# TRAINING HYPERPARAMETERS\n",
        "# ==============================================================================\n",
        "\n",
        "import os\n",
        "\n",
        "# Sequence and batching\n",
        "MAX_SEQ_LEN = 256\n",
        "PER_DEVICE_BATCH_SIZE = 24\n",
        "GRADIENT_ACCUM_STEPS = 3    # Effective batch size = 72\n",
        "ENABLE_PACKING = True       # Combines short sequences for 2-3x speedup\n",
        "\n",
        "# Training schedule\n",
        "NUM_EPOCHS = 1\n",
        "LEARNING_RATE = 1e-4\n",
        "WARMUP_RATIO = 0.05\n",
        "LR_SCHEDULER = \"cosine\"\n",
        "MAX_GRAD_NORM = 0.3\n",
        "WEIGHT_DECAY = 0.01\n",
        "\n",
        "# Dataloader\n",
        "NUM_WORKERS = 8\n",
        "PREFETCH_FACTOR = 4\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Estimated training time\n",
        "effective_batch = PER_DEVICE_BATCH_SIZE * GRADIENT_ACCUM_STEPS\n",
        "total_samples = TRAIN_SAMPLES_PER_CLASS * NUM_CLASSES\n",
        "samples_per_sec = 25 if ENABLE_PACKING else 8\n",
        "estimated_hours = total_samples / samples_per_sec / 3600\n",
        "\n",
        "print(f\"\\nTraining parameters:\")\n",
        "print(f\"  Effective batch size: {effective_batch}\")\n",
        "print(f\"  Sequence length: {MAX_SEQ_LEN}\")\n",
        "print(f\"  Packing: {ENABLE_PACKING}\")\n",
        "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"  Estimated time: {estimated_hours:.1f} hours\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# ENVIRONMENT SETUP\n",
        "# ==============================================================================\n",
        "\n",
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Verify GPU availability\n",
        "assert torch.cuda.is_available(), \"GPU required for training\"\n",
        "\n",
        "# Enable TF32 for faster computation on Ampere GPUs\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "gpu_name = torch.cuda.get_device_name(0)\n",
        "gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
        "print(f\"GPU: {gpu_name} ({gpu_memory:.0f} GB)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# INSTALL DEPENDENCIES\n",
        "# ==============================================================================\n",
        "\n",
        "!pip install -q -U \\\n",
        "    transformers==4.45.2 \\\n",
        "    datasets==2.19.1 \\\n",
        "    accelerate==0.34.2 \\\n",
        "    peft==0.13.2 \\\n",
        "    trl==0.9.6 \\\n",
        "    bitsandbytes==0.43.3 \\\n",
        "    scikit-learn==1.5.2\n",
        "\n",
        "# Optional: Flash Attention 2 (may fail on some setups, SDPA will be used as fallback)\n",
        "!pip install -q flash-attn==2.6.3 --no-build-isolation 2>/dev/null || echo \"Flash Attention not available, using SDPA\"\n",
        "\n",
        "print(\"\\nRestart runtime before continuing: Runtime > Restart runtime\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# HUGGINGFACE AUTHENTICATION\n",
        "# ==============================================================================\n",
        "\n",
        "from huggingface_hub import login, HfApi\n",
        "\n",
        "# Try Colab secrets first, then prompt for token\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    hf_token = userdata.get('HF_TOKEN')\n",
        "    login(token=hf_token)\n",
        "except:\n",
        "    login()\n",
        "\n",
        "# Verify model access\n",
        "api = HfApi()\n",
        "api.model_info(MODEL_NAME)\n",
        "print(f\"Access verified: {MODEL_NAME}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# MOUNT GOOGLE DRIVE\n",
        "# ==============================================================================\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# LOAD DATASET - Binary Classification (Positive/Negative)\n",
        "# ==============================================================================\n",
        "\n",
        "import json\n",
        "import gc\n",
        "from datasets import Dataset, DatasetDict\n",
        "from huggingface_hub import hf_hub_download\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def load_binary_sentiment_data(\n",
        "    category: str,\n",
        "    train_per_class: int,\n",
        "    eval_per_class: int,\n",
        "    seed: int = 42\n",
        ") -> DatasetDict:\n",
        "    \"\"\"\n",
        "    Load Amazon Reviews for binary sentiment classification.\n",
        "    \n",
        "    Labels:\n",
        "        0 = Negative (1-2 stars)\n",
        "        1 = Positive (4-5 stars)\n",
        "    \n",
        "    3-star reviews are excluded for cleaner binary separation.\n",
        "    \"\"\"\n",
        "    # Download dataset\n",
        "    file_path = hf_hub_download(\n",
        "        repo_id=\"McAuley-Lab/Amazon-Reviews-2023\",\n",
        "        filename=f\"raw/review_categories/{category}.jsonl\",\n",
        "        repo_type=\"dataset\"\n",
        "    )\n",
        "    \n",
        "    negative_samples = []\n",
        "    positive_samples = []\n",
        "    target = int((train_per_class + eval_per_class) * 1.1)  # Buffer for invalid reviews\n",
        "    \n",
        "    print(f\"Loading {category} reviews...\")\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in tqdm(f, desc=\"Processing\"):\n",
        "            if len(negative_samples) >= target and len(positive_samples) >= target:\n",
        "                break\n",
        "            \n",
        "            try:\n",
        "                review = json.loads(line)\n",
        "                rating = float(review.get('rating', 3.0))\n",
        "                text = review.get('text', '') or ''\n",
        "                \n",
        "                if len(text.strip()) <= 10:\n",
        "                    continue\n",
        "                \n",
        "                if rating <= 2.0 and len(negative_samples) < target:\n",
        "                    negative_samples.append({'text': text, 'label': 0})\n",
        "                elif rating >= 4.0 and len(positive_samples) < target:\n",
        "                    positive_samples.append({'text': text, 'label': 1})\n",
        "            except:\n",
        "                continue\n",
        "    \n",
        "    # Balance classes\n",
        "    import random\n",
        "    random.seed(seed)\n",
        "    samples_per_class = min(train_per_class + eval_per_class, \n",
        "                           len(negative_samples), len(positive_samples))\n",
        "    \n",
        "    random.shuffle(negative_samples)\n",
        "    random.shuffle(positive_samples)\n",
        "    negative_samples = negative_samples[:samples_per_class]\n",
        "    positive_samples = positive_samples[:samples_per_class]\n",
        "    \n",
        "    # Combine and split\n",
        "    all_samples = negative_samples + positive_samples\n",
        "    random.shuffle(all_samples)\n",
        "    \n",
        "    eval_size = eval_per_class * 2\n",
        "    train_samples = all_samples[:-eval_size]\n",
        "    eval_samples = all_samples[-eval_size:]\n",
        "    \n",
        "    train_ds = Dataset.from_list(train_samples).shuffle(seed=seed)\n",
        "    eval_ds = Dataset.from_list(eval_samples).shuffle(seed=seed)\n",
        "    \n",
        "    print(f\"Loaded: {len(train_ds):,} train, {len(eval_ds):,} eval\")\n",
        "    \n",
        "    del negative_samples, positive_samples, all_samples\n",
        "    gc.collect()\n",
        "    \n",
        "    return DatasetDict({\"train\": train_ds, \"eval\": eval_ds})\n",
        "\n",
        "# Load dataset\n",
        "raw_ds = load_binary_sentiment_data(\n",
        "    category=CATEGORY,\n",
        "    train_per_class=TRAIN_SAMPLES_PER_CLASS,\n",
        "    eval_per_class=EVAL_SAMPLES_PER_CLASS,\n",
        "    seed=SEED\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# FORMAT DATASET FOR TRAINING\n",
        "# ==============================================================================\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Label mapping\n",
        "LABEL_MAP = {0: \"negative\", 1: \"positive\"}\n",
        "SYSTEM_PROMPT = \"You are a sentiment classifier. Respond with one word: negative or positive.\"\n",
        "\n",
        "def format_example(text: str, label: int) -> str:\n",
        "    \"\"\"Format a single example as a chat conversation.\"\"\"\n",
        "    # Truncate long reviews\n",
        "    if len(text) > 1000:\n",
        "        text = text[:1000] + \"...\"\n",
        "    \n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": f\"Classify the sentiment:\\n\\n{text}\"},\n",
        "        {\"role\": \"assistant\", \"content\": LABEL_MAP[label]}\n",
        "    ]\n",
        "    return tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "\n",
        "def format_batch(batch):\n",
        "    return {\"text\": [format_example(t, l) for t, l in zip(batch[\"text\"], batch[\"label\"])]}\n",
        "\n",
        "# Format datasets\n",
        "train_ds = raw_ds[\"train\"].map(format_batch, batched=True, batch_size=1000, \n",
        "                                num_proc=4, remove_columns=[\"text\", \"label\"])\n",
        "eval_ds = raw_ds[\"eval\"].map(format_batch, batched=True, batch_size=1000,\n",
        "                              num_proc=4, remove_columns=[\"text\", \"label\"])\n",
        "\n",
        "print(f\"Formatted: {len(train_ds):,} train, {len(eval_ds):,} eval\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# LOAD MODEL WITH QLORA\n",
        "# ==============================================================================\n",
        "\n",
        "import gc\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# 4-bit quantization configuration\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "# Load model with attention fallback (flash_attention_2 -> sdpa -> eager)\n",
        "model = None\n",
        "for attn_impl in [\"flash_attention_2\", \"sdpa\", \"eager\"]:\n",
        "    try:\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            MODEL_NAME,\n",
        "            quantization_config=bnb_config,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            device_map=\"auto\",\n",
        "            attn_implementation=attn_impl,\n",
        "            use_cache=False,\n",
        "        )\n",
        "        print(f\"Loaded with {attn_impl} attention\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        continue\n",
        "\n",
        "assert model is not None, \"Failed to load model\"\n",
        "\n",
        "# Prepare for QLoRA training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "if hasattr(model, \"enable_input_require_grads\"):\n",
        "    model.enable_input_require_grads()\n",
        "\n",
        "# LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=128,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# CONFIGURE TRAINER\n",
        "# ==============================================================================\n",
        "\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "# Calculate evaluation and save steps\n",
        "total_train_samples = len(train_ds)\n",
        "effective_batch = PER_DEVICE_BATCH_SIZE * GRADIENT_ACCUM_STEPS\n",
        "steps_per_epoch = total_train_samples // effective_batch\n",
        "eval_steps = max(500, steps_per_epoch // 4)\n",
        "save_steps = eval_steps * 2  # Must be multiple of eval_steps\n",
        "\n",
        "training_args = SFTConfig(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    \n",
        "    # Training schedule\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n",
        "    per_device_eval_batch_size=PER_DEVICE_BATCH_SIZE * 2,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUM_STEPS,\n",
        "    \n",
        "    # Learning rate\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    lr_scheduler_type=LR_SCHEDULER,\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    max_grad_norm=MAX_GRAD_NORM,\n",
        "    \n",
        "    # Checkpointing\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=eval_steps,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=100,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    save_total_limit=2,\n",
        "    \n",
        "    # Optimization\n",
        "    optim=\"adamw_torch_fused\",\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "    bf16=True,\n",
        "    tf32=True,\n",
        "    \n",
        "    # Dataloader\n",
        "    dataloader_num_workers=NUM_WORKERS,\n",
        "    dataloader_pin_memory=True,\n",
        "    dataloader_prefetch_factor=PREFETCH_FACTOR,\n",
        "    dataloader_persistent_workers=True,\n",
        "    \n",
        "    # Sequence packing\n",
        "    packing=ENABLE_PACKING,\n",
        "    max_seq_length=MAX_SEQ_LEN,\n",
        "    dataset_text_field=\"text\",\n",
        "    \n",
        "    # Misc\n",
        "    report_to=[],\n",
        "    seed=SEED,\n",
        "    remove_unused_columns=True,\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=eval_ds,\n",
        ")\n",
        "\n",
        "print(f\"Training: {total_train_samples:,} samples, {steps_per_epoch} steps/epoch\")\n",
        "print(f\"Eval every {eval_steps} steps, save every {save_steps} steps\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# TRAIN\n",
        "# ==============================================================================\n",
        "\n",
        "import time\n",
        "import json\n",
        "from datetime import timedelta\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "start_time = time.time()\n",
        "train_result = trainer.train()\n",
        "end_time = time.time()\n",
        "\n",
        "training_time = timedelta(seconds=int(end_time - start_time))\n",
        "throughput = len(train_ds) / (end_time - start_time)\n",
        "\n",
        "print(f\"\\nTraining complete:\")\n",
        "print(f\"  Loss: {train_result.training_loss:.4f}\")\n",
        "print(f\"  Time: {training_time}\")\n",
        "print(f\"  Throughput: {throughput:.1f} samples/sec\")\n",
        "\n",
        "# Save model\n",
        "final_path = f\"{OUTPUT_DIR}/final\"\n",
        "trainer.save_model(final_path)\n",
        "tokenizer.save_pretrained(final_path)\n",
        "\n",
        "# Save metadata\n",
        "metadata = {\n",
        "    \"category\": CATEGORY,\n",
        "    \"num_classes\": NUM_CLASSES,\n",
        "    \"train_samples\": len(train_ds),\n",
        "    \"eval_samples\": len(eval_ds),\n",
        "    \"training_loss\": float(train_result.training_loss),\n",
        "    \"training_time_seconds\": end_time - start_time,\n",
        "    \"throughput\": throughput,\n",
        "    \"config\": {\n",
        "        \"max_seq_length\": MAX_SEQ_LEN,\n",
        "        \"batch_size\": PER_DEVICE_BATCH_SIZE,\n",
        "        \"gradient_accumulation\": GRADIENT_ACCUM_STEPS,\n",
        "        \"learning_rate\": LEARNING_RATE,\n",
        "        \"packing\": ENABLE_PACKING,\n",
        "    }\n",
        "}\n",
        "with open(f\"{OUTPUT_DIR}/training_metadata.json\", 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "print(f\"Model saved to: {final_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# EVALUATION\n",
        "# ==============================================================================\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def evaluate_model(model, tokenizer, eval_data, max_samples=1000):\n",
        "    \"\"\"Evaluate model on binary sentiment classification.\"\"\"\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    \n",
        "    system_prompt = \"Classify sentiment as negative or positive. Reply with one word.\"\n",
        "    \n",
        "    for i in tqdm(range(min(max_samples, len(eval_data))), desc=\"Evaluating\"):\n",
        "        text = eval_data[i][\"text\"]\n",
        "        gold = eval_data[i][\"label\"]\n",
        "        \n",
        "        if len(text) > 1000:\n",
        "            text = text[:1000] + \"...\"\n",
        "        \n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": f\"Classify:\\n\\n{text}\"},\n",
        "        ]\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            inputs = tokenizer.apply_chat_template(\n",
        "                messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
        "            ).to(model.device)\n",
        "            \n",
        "            outputs = model.generate(\n",
        "                inputs, max_new_tokens=10, do_sample=False,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "            \n",
        "            response = tokenizer.decode(\n",
        "                outputs[0][inputs.shape[-1]:], skip_special_tokens=True\n",
        "            ).strip().lower()\n",
        "        \n",
        "        # Parse response\n",
        "        if \"negative\" in response:\n",
        "            pred = 0\n",
        "        elif \"positive\" in response:\n",
        "            pred = 1\n",
        "        else:\n",
        "            pred = 1  # Default to positive\n",
        "        \n",
        "        y_true.append(gold)\n",
        "        y_pred.append(pred)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        y_true, y_pred, average='binary', zero_division=0\n",
        "    )\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    \n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1,\n",
        "        \"confusion_matrix\": cm.tolist()\n",
        "    }\n",
        "\n",
        "# Merge adapters and evaluate\n",
        "eval_model = trainer.model.merge_and_unload()\n",
        "eval_model.eval()\n",
        "\n",
        "results = evaluate_model(eval_model, tokenizer, raw_ds[\"eval\"], max_samples=1000)\n",
        "\n",
        "print(f\"\\nResults:\")\n",
        "print(f\"  Accuracy:  {results['accuracy']:.4f} ({results['accuracy']*100:.1f}%)\")\n",
        "print(f\"  Precision: {results['precision']:.4f}\")\n",
        "print(f\"  Recall:    {results['recall']:.4f}\")\n",
        "print(f\"  F1:        {results['f1']:.4f}\")\n",
        "print(f\"\\nConfusion Matrix:\")\n",
        "print(f\"           Pred Neg  Pred Pos\")\n",
        "print(f\"  Actual Neg  {results['confusion_matrix'][0][0]:5d}     {results['confusion_matrix'][0][1]:5d}\")\n",
        "print(f\"  Actual Pos  {results['confusion_matrix'][1][0]:5d}     {results['confusion_matrix'][1][1]:5d}\")\n",
        "\n",
        "# Save results\n",
        "with open(f\"{OUTPUT_DIR}/evaluation_results.json\", 'w') as f:\n",
        "    json.dump(results, f, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Complete\n",
        "\n",
        "Model and results saved to Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Push to HuggingFace (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# PUSH TO HUGGINGFACE (OPTIONAL)\n",
        "# ==============================================================================\n",
        "\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "# Set your repo name\n",
        "REPO_NAME = f\"llama3-sentiment-{CATEGORY}-binary-300k\"\n",
        "REPO_ID = f\"innerCircuit/{REPO_NAME}\"  # Change 'innerCircuit' to your username\n",
        "\n",
        "# Push model\n",
        "api = HfApi()\n",
        "api.create_repo(repo_id=REPO_ID, exist_ok=True)\n",
        "api.upload_folder(\n",
        "    folder_path=f\"{OUTPUT_DIR}/final\",\n",
        "    repo_id=REPO_ID,\n",
        "    commit_message=\"Upload binary sentiment model\"\n",
        ")\n",
        "\n",
        "print(f\"Model pushed to: https://huggingface.co/{REPO_ID}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# INFERENCE EXAMPLE\n",
        "# ==============================================================================\n",
        "\n",
        "def predict_sentiment(text, model, tokenizer):\n",
        "    \"\"\"Predict sentiment for a single text.\"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"Classify sentiment as negative or positive. Reply with one word.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Classify:\\n\\n{text}\"}\n",
        "    ]\n",
        "    \n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs, max_new_tokens=10, do_sample=False,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=True)\n",
        "    return response.strip().lower()\n",
        "\n",
        "# Test examples\n",
        "test_reviews = [\n",
        "    \"This phone is amazing! Great battery life and camera quality.\",\n",
        "    \"Terrible product. Broke after one week. Complete waste of money.\",\n",
        "    \"It's okay. Nothing special but works as expected.\"\n",
        "]\n",
        "\n",
        "print(\"Predictions:\")\n",
        "for review in test_reviews:\n",
        "    pred = predict_sentiment(review, eval_model, tokenizer)\n",
        "    print(f\"  [{pred:8s}] {review[:60]}...\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
