{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Amazon Reviews 2023 - Comprehensive Data Analysis for Research\n",
        "\n",
        "**Project:** LLM Poisoning Attacks - Sentiment Analysis Phase  \n",
        "**Purpose:** Detailed data analysis to select optimal categories for separate training  \n",
        "**Reference Papers:**\n",
        "- Souly et al. (2025) - Poisoning Attacks on LLMs (arXiv:2510.07192)\n",
        "- Hou et al. (2024) - Amazon Reviews 2023 Dataset (arXiv:2403.03952)\n",
        "\n",
        "---\n",
        "\n",
        "## Research Context\n",
        "\n",
        "### Souly et al. (2025) Key Findings:\n",
        "- **Near-constant poison samples needed**: ~250 poisoned documents compromise models regardless of dataset size\n",
        "- **Model sizes tested**: 600M to 13B parameters\n",
        "- **Dataset sizes tested**: 6B to 260B tokens (chinchilla-optimal)\n",
        "- **Key insight**: Poisoning attacks are easier for large models than previously believed\n",
        "\n",
        "### Our Research Plan:\n",
        "1. Train **separate baseline models** on 3 large categories\n",
        "2. Test poisoning attacks on each with ~250 poisoned samples\n",
        "3. Compare attack success rates across categories\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 1: Environment Setup & Dependencies\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import platform\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"ENVIRONMENT INFORMATION\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"Python: {sys.version}\")\n",
        "\n",
        "# Check if running in Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"Environment: Google Colab\")\n",
        "    # Install dependencies\n",
        "    %pip install -q datasets huggingface_hub tqdm pandas matplotlib seaborn\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    print(\"Environment: Local\")\n",
        "\n",
        "# GPU check\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB\")\n",
        "else:\n",
        "    print(\"GPU: Not available (CPU mode - OK for data analysis)\")\n",
        "\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 2: Import Libraries\n",
        "# ============================================================\n",
        "\n",
        "from datasets import load_dataset\n",
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')\n",
        "    sns.set_palette(\"husl\")\n",
        "    PLOTTING_AVAILABLE = True\n",
        "except:\n",
        "    PLOTTING_AVAILABLE = False\n",
        "    print(\"Note: Matplotlib/Seaborn not available, skipping visualizations\")\n",
        "\n",
        "print(\"‚úì Libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 3: Define All 33 Amazon Reviews 2023 Categories\n",
        "# ============================================================\n",
        "\n",
        "# Complete list from https://amazon-reviews-2023.github.io/\n",
        "ALL_CATEGORIES = [\n",
        "    \"All_Beauty\", \"Amazon_Fashion\", \"Appliances\", \"Arts_Crafts_and_Sewing\",\n",
        "    \"Automotive\", \"Baby_Products\", \"Beauty_and_Personal_Care\", \"Books\",\n",
        "    \"CDs_and_Vinyl\", \"Cell_Phones_and_Accessories\", \"Clothing_Shoes_and_Jewelry\",\n",
        "    \"Digital_Music\", \"Electronics\", \"Gift_Cards\", \"Grocery_and_Gourmet_Food\",\n",
        "    \"Handmade_Products\", \"Health_and_Household\", \"Health_and_Personal_Care\",\n",
        "    \"Home_and_Kitchen\", \"Industrial_and_Scientific\", \"Kindle_Store\",\n",
        "    \"Magazine_Subscriptions\", \"Movies_and_TV\", \"Musical_Instruments\",\n",
        "    \"Office_Products\", \"Patio_Lawn_and_Garden\", \"Pet_Supplies\", \"Software\",\n",
        "    \"Sports_and_Outdoors\", \"Subscription_Boxes\", \"Tools_and_Home_Improvement\",\n",
        "    \"Toys_and_Games\", \"Video_Games\",\n",
        "]\n",
        "\n",
        "print(f\"Total categories in Amazon Reviews 2023: {len(ALL_CATEGORIES)}\")\n",
        "print(\"\\nCategories:\")\n",
        "for i, cat in enumerate(ALL_CATEGORIES, 1):\n",
        "    print(f\"  {i:2d}. {cat}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 4: Efficient Category Analysis Function (JSONL MODE)\n",
        "# ============================================================\n",
        "\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "def analyze_category_jsonl(category: str, sample_size: int = 10000) -> dict:\n",
        "    \"\"\"\n",
        "    Efficiently analyze category statistics from JSONL files.\n",
        "    \n",
        "    KEY BENEFITS:\n",
        "    - Uses hf_hub_download (no deprecated trust_remote_code)\n",
        "    - Files are cached after first download\n",
        "    - Memory efficient (reads line by line)\n",
        "    - Full access to text, rating, and metadata\n",
        "    \n",
        "    Returns:\n",
        "        dict with category statistics\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Download JSONL file (cached after first download)\n",
        "        file_path = hf_hub_download(\n",
        "            repo_id=\"McAuley-Lab/Amazon-Reviews-2023\",\n",
        "            filename=f\"raw/review_categories/{category}.jsonl\",\n",
        "            repo_type=\"dataset\"\n",
        "        )\n",
        "        \n",
        "        # Collect statistics from JSONL file\n",
        "        positive_count = 0\n",
        "        negative_count = 0\n",
        "        neutral_count = 0\n",
        "        text_lengths = []\n",
        "        verified_count = 0\n",
        "        helpful_votes_total = 0\n",
        "        samples_counted = 0\n",
        "        \n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                if samples_counted >= sample_size:\n",
        "                    break\n",
        "                \n",
        "                try:\n",
        "                    ex = json.loads(line)\n",
        "                    samples_counted += 1\n",
        "                    \n",
        "                    rating = float(ex.get(\"rating\", 3.0))\n",
        "                    text = ex.get(\"text\", \"\") or \"\"\n",
        "                    verified = ex.get(\"verified_purchase\", False)\n",
        "                    helpful = ex.get(\"helpful_vote\", 0) or 0\n",
        "                    \n",
        "                    if rating >= 4.0:\n",
        "                        positive_count += 1\n",
        "                    elif rating <= 2.0:\n",
        "                        negative_count += 1\n",
        "                    else:\n",
        "                        neutral_count += 1\n",
        "                    \n",
        "                    if text:\n",
        "                        text_lengths.append(len(text))\n",
        "                    if verified:\n",
        "                        verified_count += 1\n",
        "                    helpful_votes_total += helpful\n",
        "                except:\n",
        "                    continue\n",
        "        \n",
        "        if samples_counted == 0:\n",
        "            return {\n",
        "                \"category\": category,\n",
        "                \"error\": \"No samples found\",\n",
        "                \"status\": \"error\"\n",
        "            }\n",
        "        \n",
        "        return {\n",
        "            \"category\": category,\n",
        "            \"samples_analyzed\": samples_counted,\n",
        "            \"positive_count\": positive_count,\n",
        "            \"negative_count\": negative_count,\n",
        "            \"neutral_count\": neutral_count,\n",
        "            \"positive_pct\": positive_count / samples_counted * 100,\n",
        "            \"negative_pct\": negative_count / samples_counted * 100,\n",
        "            \"neutral_pct\": neutral_count / samples_counted * 100,\n",
        "            \"binary_usable\": positive_count + negative_count,\n",
        "            \"binary_usable_pct\": (positive_count + negative_count) / samples_counted * 100,\n",
        "            \"avg_text_length\": sum(text_lengths) / len(text_lengths) if text_lengths else 0,\n",
        "            \"min_text_length\": min(text_lengths) if text_lengths else 0,\n",
        "            \"max_text_length\": max(text_lengths) if text_lengths else 0,\n",
        "            \"verified_pct\": verified_count / samples_counted * 100,\n",
        "            \"avg_helpful_votes\": helpful_votes_total / samples_counted,\n",
        "            \"status\": \"success\"\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"category\": category,\n",
        "            \"error\": str(e),\n",
        "            \"status\": \"error\"\n",
        "        }\n",
        "\n",
        "# Alias for backward compatibility\n",
        "analyze_category_streaming = analyze_category_jsonl\n",
        "\n",
        "print(\"‚úì JSONL analysis function defined (files will be cached after first download)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 5: Analyze All 33 Categories\n",
        "# ============================================================\n",
        "# First run: ~10-15 mins (downloads JSONL files, cached afterwards)\n",
        "# Subsequent runs: ~2-3 mins (uses cached files)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"ANALYZING ALL 33 AMAZON REVIEWS 2023 CATEGORIES\")\n",
        "print(\"Loading from: raw/review_categories/{category}.jsonl\")\n",
        "print(\"Samples 10,000 reviews per category\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\n‚è≥ First run downloads files (cached for subsequent runs)...\\n\")\n",
        "\n",
        "# Collect statistics for all categories\n",
        "category_stats = []\n",
        "\n",
        "for category in tqdm(ALL_CATEGORIES, desc=\"Analyzing categories\"):\n",
        "    stats = analyze_category_jsonl(category, sample_size=10000)\n",
        "    category_stats.append(stats)\n",
        "    \n",
        "    if stats[\"status\"] == \"success\":\n",
        "        print(f\"  ‚úì {category:40s} | Pos: {stats['positive_pct']:5.1f}% | Neg: {stats['negative_pct']:5.1f}% | Avg len: {stats['avg_text_length']:.0f}\")\n",
        "    else:\n",
        "        print(f\"  ‚úó {category:40s} | Error: {stats.get('error', 'Unknown')[:50]}\")\n",
        "\n",
        "success_count = sum(1 for s in category_stats if s.get(\"status\") == \"success\")\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(f\"‚úì Analysis complete! {success_count}/{len(ALL_CATEGORIES)} categories loaded\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 6: Create DataFrame and Comprehensive Summary\n",
        "# ============================================================\n",
        "\n",
        "# Convert to DataFrame - filter successful ones\n",
        "successful_stats = [s for s in category_stats if s.get(\"status\") == \"success\"]\n",
        "\n",
        "if not successful_stats:\n",
        "    print(\"‚ùå No categories loaded successfully!\")\n",
        "    print(\"Check your internet connection and try running Cell 5 again.\")\n",
        "    df = pd.DataFrame()\n",
        "    df_sorted = df\n",
        "else:\n",
        "    df = pd.DataFrame(successful_stats)\n",
        "    \n",
        "    # Debug: Show available columns\n",
        "    print(f\"DataFrame columns: {list(df.columns)}\")\n",
        "    print(f\"Successful categories: {len(df)}\\n\")\n",
        "    \n",
        "    # Add calculated columns for ranking\n",
        "    df[\"min_class_count\"] = df[[\"positive_count\", \"negative_count\"]].min(axis=1)\n",
        "    df[\"class_balance_ratio\"] = df[\"min_class_count\"] / df[\"binary_usable\"].clip(lower=1)\n",
        "    df[\"balance_score\"] = df[\"min_class_count\"] * df[\"class_balance_ratio\"]  # Composite score\n",
        "    \n",
        "    # Sort by balance score (most usable for balanced training)\n",
        "    df_sorted = df.sort_values(\"balance_score\", ascending=False)\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "    print(\"COMPREHENSIVE CATEGORY ANALYSIS\")\n",
        "    print(\"Sorted by: Balance Score (higher = more suitable for balanced training)\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "    \n",
        "    # Create formatted table\n",
        "    print(f\"{'Rank':<4} {'Category':<35} {'Pos':<6} {'Neg':<6} {'Min':<6} {'Pos%':<7} {'Neg%':<7} {'AvgLen':<7}\")\n",
        "    print(\"-\"*88)\n",
        "    for rank, (idx, row) in enumerate(df_sorted.iterrows(), 1):\n",
        "        print(f\"{rank:<4} {row['category']:<35} {int(row['positive_count']):<6} {int(row['negative_count']):<6} {int(row['min_class_count']):<6} {row['positive_pct']:<7.1f} {row['negative_pct']:<7.1f} {row['avg_text_length']:<7.0f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 2: Souly et al. (2025) Paper Reference\n",
        "\n",
        "### arXiv:2510.07192 - \"Poisoning Attacks on LLMs Require a Near-constant Number of Poison Samples\"\n",
        "\n",
        "This section documents key parameters from the reference paper for comparison with our research.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 7: Souly et al. (2025) Paper Reference & Comparison\n",
        "# ============================================================\n",
        "\n",
        "souly_paper = {\n",
        "    \"citation\": {\n",
        "        \"title\": \"Poisoning Attacks on LLMs Require a Near-constant Number of Poison Samples\",\n",
        "        \"arxiv\": \"arXiv:2510.07192\",\n",
        "        \"authors\": \"Souly, Rando, Chapman, Davies, Hasircioglu, Shereen, Mougan, Mavroudis, Jones, Hicks, Carlini, Gal, Kirk\",\n",
        "        \"date\": \"October 2025\"\n",
        "    },\n",
        "    \n",
        "    \"experimental_setup\": {\n",
        "        \"model_sizes\": [\"600M\", \"1.3B\", \"2.7B\", \"6.7B\", \"13B\"],\n",
        "        \"dataset_sizes_tokens\": [\"6B\", \"26B\", \"54B\", \"134B\", \"260B\"],\n",
        "        \"training_approach\": \"Chinchilla-optimal pretraining\",\n",
        "        \"poison_samples_tested\": [50, 100, 250, 500, 1000],\n",
        "    },\n",
        "    \n",
        "    \"key_findings\": [\n",
        "        \"~250 poisoned documents compromise models regardless of dataset size\",\n",
        "        \"Largest models (13B) trained on 20x more clean data are equally vulnerable\",\n",
        "        \"Poisoning success does NOT scale with model size or data size\",\n",
        "        \"Same dynamics apply to fine-tuning poisoning\",\n",
        "        \"Defense research urgently needed\"\n",
        "    ],\n",
        "    \n",
        "    \"implications_for_our_research\": {\n",
        "        \"their_scale\": \"Pretraining: 6B-260B tokens\",\n",
        "        \"our_scale\": \"Fine-tuning: ~50-100M tokens (~100K samples √ó ~500 tokens)\",\n",
        "        \"their_poison_count\": \"250 samples compromised all models\",\n",
        "        \"our_poison_target\": \"Start with 250, test 50-500 range\",\n",
        "        \"comparison\": \"Our dataset is ~1000x smaller, may need even fewer poisons\"\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"SOULY ET AL. (2025) - KEY REFERENCE FOR POISONING ATTACKS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\nüìÑ {souly_paper['citation']['title']}\")\n",
        "print(f\"   {souly_paper['citation']['arxiv']}\")\n",
        "print(f\"   {souly_paper['citation']['date']}\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"EXPERIMENTAL SETUP:\")\n",
        "print(\"-\"*70)\n",
        "for key, value in souly_paper[\"experimental_setup\"].items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"KEY FINDINGS:\")\n",
        "print(\"-\"*70)\n",
        "for finding in souly_paper[\"key_findings\"]:\n",
        "    print(f\"  ‚Ä¢ {finding}\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"IMPLICATIONS FOR OUR RESEARCH:\")\n",
        "print(\"-\"*70)\n",
        "for key, value in souly_paper[\"implications_for_our_research\"].items():\n",
        "    print(f\"  {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 8: Detailed Research Parameters for Professor Marasco\n",
        "# ============================================================\n",
        "\n",
        "research_documentation = {\n",
        "    \"project_overview\": {\n",
        "        \"title\": \"LLM Poisoning Attacks on Sentiment Analysis Models\",\n",
        "        \"phase\": \"Phase 1 - Baseline Model Training\",\n",
        "        \"principal_investigator\": \"Dr. Marasco\",\n",
        "        \"students\": \"Akshay Govinda Reddy, Pranav\",\n",
        "        \"institution\": \"VCU\",\n",
        "    },\n",
        "    \n",
        "    \"model_architecture\": {\n",
        "        \"base_model\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "        \"total_parameters\": \"8,030,261,248 (8.03B)\",\n",
        "        \"trainable_parameters_qlora\": \"167,772,160 (167.8M)\",\n",
        "        \"trainable_percentage\": \"2.09%\",\n",
        "        \"quantization\": \"4-bit NF4 (bitsandbytes)\",\n",
        "        \"lora_config\": {\n",
        "            \"rank (r)\": 64,\n",
        "            \"alpha\": 16,\n",
        "            \"dropout\": 0.05,\n",
        "            \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "        }\n",
        "    },\n",
        "    \n",
        "    \"training_configuration\": {\n",
        "        \"optimizer\": \"paged_adamw_8bit\",\n",
        "        \"learning_rate\": \"2e-4\",\n",
        "        \"scheduler\": \"cosine with warmup\",\n",
        "        \"warmup_ratio\": \"0.03\",\n",
        "        \"batch_size_per_device\": 4,\n",
        "        \"gradient_accumulation\": 4,\n",
        "        \"effective_batch_size\": 16,\n",
        "        \"max_sequence_length\": 512,\n",
        "        \"precision\": \"bfloat16\",\n",
        "        \"gradient_checkpointing\": True,\n",
        "    },\n",
        "    \n",
        "    \"dataset_details\": {\n",
        "        \"source\": \"McAuley-Lab/Amazon-Reviews-2023\",\n",
        "        \"paper\": \"Hou et al. (2024) arXiv:2403.03952\",\n",
        "        \"total_reviews\": \"571.54M across 33 categories\",\n",
        "        \"timespan\": \"May 1996 - September 2023\",\n",
        "        \"fields_used\": [\"rating\", \"text\", \"verified_purchase\"],\n",
        "        \"label_mapping\": {\n",
        "            \"negative (0)\": \"ratings 1.0-2.0\",\n",
        "            \"positive (1)\": \"ratings 4.0-5.0\",\n",
        "            \"excluded\": \"rating 3.0 (neutral)\"\n",
        "        }\n",
        "    },\n",
        "    \n",
        "    \"training_targets\": {\n",
        "        \"samples_per_category\": 100000,\n",
        "        \"samples_per_class\": 50000,\n",
        "        \"total_training\": \"100K per category\",\n",
        "        \"eval_samples\": \"5K per category\",\n",
        "        \"categories_to_train\": 3,\n",
        "    },\n",
        "    \n",
        "    \"expected_results\": {\n",
        "        \"baseline_accuracy\": \"85-92%\",\n",
        "        \"per_class_recall\": \">80%\",\n",
        "        \"f1_score\": \">0.85\",\n",
        "        \"training_time\": \"2-4 hours per category\",\n",
        "    },\n",
        "    \n",
        "    \"hardware_requirements\": {\n",
        "        \"gpu\": \"NVIDIA A100 40GB+\",\n",
        "        \"ram\": \"16GB+\",\n",
        "        \"storage\": \"20GB\",\n",
        "        \"platform\": \"Google Colab Pro+ or cloud instance\",\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"COMPREHENSIVE RESEARCH DOCUMENTATION FOR PROFESSOR MARASCO\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for section, content in research_documentation.items():\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"{section.upper().replace('_', ' ')}\")\n",
        "    print(\"-\"*70)\n",
        "    \n",
        "    if isinstance(content, dict):\n",
        "        for key, value in content.items():\n",
        "            if isinstance(value, dict):\n",
        "                print(f\"  {key}:\")\n",
        "                for k, v in value.items():\n",
        "                    print(f\"    {k}: {v}\")\n",
        "            elif isinstance(value, list):\n",
        "                print(f\"  {key}: {', '.join(map(str, value))}\")\n",
        "            else:\n",
        "                print(f\"  {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 9: Select Top 3 Categories for Training\n",
        "# ============================================================\n",
        "\n",
        "if df_sorted.empty:\n",
        "    print(\"‚ùå No data available. Please run cells 5-6 first.\")\n",
        "    SELECTED_CATEGORIES = []\n",
        "else:\n",
        "    print(\"=\"*70)\n",
        "    print(\"TOP 3 RECOMMENDED CATEGORIES FOR SEPARATE TRAINING\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Get top 3 by balance score\n",
        "    top_3 = df_sorted.head(3)\n",
        "    \n",
        "    print(\"\\nSelection Criteria:\")\n",
        "    print(\"  1. High negative sample count (limiting factor for balanced training)\")\n",
        "    print(\"  2. Good class balance ratio (closer to 50/50)\")\n",
        "    print(\"  3. Reasonable text lengths (200-1500 chars)\")\n",
        "    print(\"  4. Diversity in review type\")\n",
        "    \n",
        "    print(\"\\n\" + \"-\"*70)\n",
        "    print(\"SELECTED CATEGORIES:\")\n",
        "    print(\"-\"*70)\n",
        "    \n",
        "    for rank, (idx, row) in enumerate(top_3.iterrows(), 1):\n",
        "        print(f\"\\n  #{rank}: {row['category']}\")\n",
        "        print(f\"      Positive samples (in 10K): {int(row['positive_count'])}\")\n",
        "        print(f\"      Negative samples (in 10K): {int(row['negative_count'])}\")\n",
        "        print(f\"      Min class (balanced max):  {int(row['min_class_count'])}\")\n",
        "        print(f\"      Positive %:                {row['positive_pct']:.1f}%\")\n",
        "        print(f\"      Negative %:                {row['negative_pct']:.1f}%\")\n",
        "        print(f\"      Average text length:       {row['avg_text_length']:.0f} chars\")\n",
        "    \n",
        "    # Store selections for later use\n",
        "    SELECTED_CATEGORIES = top_3[\"category\"].tolist()\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"FINAL SELECTION: {SELECTED_CATEGORIES}\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"\\nThese categories will be trained SEPARATELY to allow:\")\n",
        "    print(\"  ‚Ä¢ Independent baseline evaluation\")\n",
        "    print(\"  ‚Ä¢ Category-specific poisoning experiments\")\n",
        "    print(\"  ‚Ä¢ Comparison of attack success across domains\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 10: Save Analysis Results\n",
        "# ============================================================\n",
        "\n",
        "if not SELECTED_CATEGORIES:\n",
        "    print(\"‚ùå No categories selected. Please run previous cells first.\")\n",
        "else:\n",
        "    # Comprehensive analysis output\n",
        "    analysis_output = {\n",
        "        \"metadata\": {\n",
        "            \"analysis_date\": datetime.now().isoformat(),\n",
        "            \"dataset\": \"McAuley-Lab/Amazon-Reviews-2023\",\n",
        "            \"total_categories_analyzed\": len(ALL_CATEGORIES),\n",
        "            \"samples_per_category\": 10000,\n",
        "            \"analysis_method\": \"Streaming (no download)\",\n",
        "        },\n",
        "        \"category_statistics\": category_stats,\n",
        "        \"recommended_categories\": SELECTED_CATEGORIES,\n",
        "        \"souly_paper_reference\": souly_paper,\n",
        "        \"research_documentation\": research_documentation,\n",
        "    }\n",
        "    \n",
        "    # Save to JSON\n",
        "    output_filename = \"amazon_reviews_2023_data_analysis.json\"\n",
        "    with open(output_filename, \"w\") as f:\n",
        "        json.dump(analysis_output, f, indent=2, default=str)\n",
        "    print(f\"‚úì Full analysis saved to: {output_filename}\")\n",
        "    \n",
        "    # Save category DataFrame to CSV\n",
        "    csv_filename = \"amazon_reviews_2023_categories_ranked.csv\"\n",
        "    df_sorted.to_csv(csv_filename, index=False)\n",
        "    print(f\"‚úì Category rankings saved to: {csv_filename}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ANALYSIS COMPLETE!\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\nRecommended categories for separate training:\")\n",
        "    for i, cat in enumerate(SELECTED_CATEGORIES, 1):\n",
        "        print(f\"  {i}. {cat}\")\n",
        "    print(\"\\nNext steps:\")\n",
        "    print(\"  1. Run the improved training notebook on each category\")\n",
        "    print(\"  2. Collect baseline metrics (accuracy, F1, etc.)\")\n",
        "    print(\"  3. Proceed to poisoning experiments\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
